{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf8dac",
   "metadata": {},
   "source": [
    "## Iteraci√≥n 6 - ENSEMBLE\n",
    "\n",
    "**Post-mortem V1-V5:**\n",
    "- V1: 0.8665 (baseline, overfitting)\n",
    "- V2: 0.8885 ‚Üê BEST single model\n",
    "- V3: 0.8733 (complementario a V2)\n",
    "- V4: 0.6456 (DistilBERT fracaso)\n",
    "- V5: 0.8593 (CNN+LSTM no mejora)\n",
    "\n",
    "**Estrategia V6:**\n",
    "1. **Entrenar V2 y V3** con misma seed para reproducibilidad\n",
    "2. **Weighted ensemble**: V2 (60%) + V3 (40%)\n",
    "3. **Threshold optimization**: Buscar umbral √≥ptimo > 0.5\n",
    "\n",
    "**Objetivo:** MCC > 0.90 (aprovechar complementariedad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5357bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared config\n",
    "MAX_WORDS = 50000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# V2 config (moderate regularization)\n",
    "V2_LSTM_UNITS = 96\n",
    "V2_DROPOUT = 0.4\n",
    "V2_L2_REG = 5e-5\n",
    "V2_EPOCHS = 10\n",
    "V2_LR = 1e-3\n",
    "\n",
    "# V3 config (extreme regularization)\n",
    "V3_LSTM_UNITS = 64\n",
    "V3_DROPOUT = 0.5\n",
    "V3_L2_REG = 1e-4\n",
    "V3_EPOCHS = 10\n",
    "V3_LR = 1e-3\n",
    "\n",
    "# Ensemble weights\n",
    "V2_WEIGHT = 0.6  # V2 es mejor individualmente\n",
    "V3_WEIGHT = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train['text'])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "# Train/val split\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, test_size=VALIDATION_SPLIT, random_state=seed, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train_final)}, Val: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3e87e",
   "metadata": {},
   "source": [
    "## Modelo V2 - Moderate Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1651ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_v2_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "        Bidirectional(LSTM(\n",
    "            V2_LSTM_UNITS,\n",
    "            kernel_regularizer=l2(V2_L2_REG),\n",
    "            recurrent_regularizer=l2(V2_L2_REG)\n",
    "        )),\n",
    "        Dropout(V2_DROPOUT),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='V2_LSTM_Moderate')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=V2_LR),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_v2 = build_v2_model()\n",
    "model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train V2\n",
    "callbacks_v2 = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING V2 (Moderate Regularization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "history_v2 = model_v2.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=V2_EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks_v2,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# V2 predictions\n",
    "y_pred_v2_proba = model_v2.predict(X_val, batch_size=BATCH_SIZE, verbose=0)\n",
    "y_pred_v2 = (y_pred_v2_proba > 0.5).astype(int).flatten()\n",
    "mcc_v2 = matthews_corrcoef(y_val, y_pred_v2)\n",
    "\n",
    "print(f\"\\nV2 Validation MCC: {mcc_v2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed46e15",
   "metadata": {},
   "source": [
    "## Modelo V3 - Extreme Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ec78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_v3_model():\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=MAX_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "        Bidirectional(LSTM(\n",
    "            V3_LSTM_UNITS,\n",
    "            kernel_regularizer=l2(V3_L2_REG),\n",
    "            recurrent_regularizer=l2(V3_L2_REG)\n",
    "        )),\n",
    "        Dropout(V3_DROPOUT),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ], name='V3_LSTM_Extreme')\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=V3_LR),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_v3 = build_v3_model()\n",
    "model_v3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4bd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train V3\n",
    "callbacks_v3 = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING V3 (Extreme Regularization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "history_v3 = model_v3.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=V3_EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks_v3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# V3 predictions\n",
    "y_pred_v3_proba = model_v3.predict(X_val, batch_size=BATCH_SIZE, verbose=0)\n",
    "y_pred_v3 = (y_pred_v3_proba > 0.5).astype(int).flatten()\n",
    "mcc_v3 = matthews_corrcoef(y_val, y_pred_v3)\n",
    "\n",
    "print(f\"\\nV3 Validation MCC: {mcc_v3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2618ffb",
   "metadata": {},
   "source": [
    "## Ensemble V2 + V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted average ensemble\n",
    "y_pred_ensemble_proba = (V2_WEIGHT * y_pred_v2_proba.flatten() + \n",
    "                          V3_WEIGHT * y_pred_v3_proba.flatten())\n",
    "\n",
    "# Test different thresholds\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_threshold = 0.5\n",
    "best_mcc = 0\n",
    "\n",
    "for threshold in np.arange(0.3, 0.7, 0.02):\n",
    "    y_pred_ensemble = (y_pred_ensemble_proba > threshold).astype(int)\n",
    "    mcc = matthews_corrcoef(y_val, y_pred_ensemble)\n",
    "    print(f\"Threshold {threshold:.2f}: MCC {mcc:.4f}\")\n",
    "    \n",
    "    if mcc > best_mcc:\n",
    "        best_mcc = mcc\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"BEST THRESHOLD: {best_threshold:.2f} ‚Üí MCC {best_mcc:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52488147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final ensemble prediction with best threshold\n",
    "y_pred_ensemble_final = (y_pred_ensemble_proba > best_threshold).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"V2 alone: {mcc_v2:.4f}\")\n",
    "print(f\"V3 alone: {mcc_v3:.4f}\")\n",
    "print(f\"Ensemble: {best_mcc:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_mcc > max(mcc_v2, mcc_v3):\n",
    "    improvement = best_mcc - max(mcc_v2, mcc_v3)\n",
    "    print(f\"‚úÖ MEJORA: +{improvement:.4f} sobre mejor modelo individual\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Ensemble no mejora modelos individuales\")\n",
    "\n",
    "print(\"\\n\" + classification_report(y_val, y_pred_ensemble_final, target_names=['Not SPAM', 'SPAM']))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred_ensemble_final)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')\n",
    "plt.title(f'Ensemble V2+V3 (MCC: {best_mcc:.4f})')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad17d26a",
   "metadata": {},
   "source": [
    "## Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14094673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# V2 test predictions\n",
    "y_test_v2_proba = model_v2.predict(X_test_pad, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "# V3 test predictions\n",
    "y_test_v3_proba = model_v3.predict(X_test_pad, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "# Ensemble test predictions\n",
    "y_test_ensemble_proba = (V2_WEIGHT * y_test_v2_proba.flatten() + \n",
    "                          V3_WEIGHT * y_test_v3_proba.flatten())\n",
    "\n",
    "y_test_ensemble = (y_test_ensemble_proba > best_threshold).astype(int)\n",
    "\n",
    "# Create submission\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_test_ensemble\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"Submission created with {len(y_test_ensemble)} predictions\")\n",
    "print(f\"Using ensemble threshold: {best_threshold:.2f}\")\n",
    "print(f\"Expected MCC: ~{best_mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8612334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVOLUCI√ìN COMPLETA - TODAS LAS ITERACIONES\")\n",
    "print(\"=\"*80)\n",
    "print(\"V1 LSTM baseline:        0.8665\")\n",
    "print(\"V2 LSTM + L2 moderate:   0.8885 ‚Üê Best individual\")\n",
    "print(\"V3 LSTM + L2 extreme:    0.8733\")\n",
    "print(\"V4 DistilBERT:           0.6456 ‚Üê Fracaso total\")\n",
    "print(\"V5 CNN+LSTM hybrid:      0.8593\")\n",
    "print(f\"V6 Ensemble V2+V3:       {best_mcc:.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if best_mcc > 0.90:\n",
    "    print(\"üéØ OBJETIVO ALCANZADO: MCC > 0.90\")\n",
    "elif best_mcc > 0.8885:\n",
    "    print(f\"‚úÖ MEJORA: +{(best_mcc-0.8885):.4f} sobre V2\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Usar V2 individual para submission\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
