{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================================================\n",
    "#                      PROYECTO FINAL - CLASIFICACION SPAM/NOT SPAM\n",
    "#                              Ismael Hernandez Clemente\n",
    "# ==========================================================================================\n",
    "#\n",
    "# CONTEXTO:\n",
    "# Este es mi modelo final (V6) despues de probar 5 versiones diferentes.\n",
    "# He aprendido que lo simple funciona mejor que lo complejo.\n",
    "# \n",
    "# ITERACIONES PREVIAS:\n",
    "# - V1: LSTM basico, funcionaba pero mucho overfitting\n",
    "# - V2: Mejor score publico (0.8885) pero segui teniendo overfitting\n",
    "# - V3: Regularizacion fuerte, bajo el overfitting pero bajo un poco el MCC\n",
    "# - V4: Probe DistilBERT... fue un desastre total (0.64 de MCC)\n",
    "# - V5: CNN+LSTM hibrido, demasiado complejo y no mejoro nada\n",
    "# - V6: Vuelvo a V3 con pequeños ajustes, el mas estable\n",
    "#\n",
    "# POR QUE ELEGI V6 Y NO V2:\n",
    "# Si, V2 tiene mejor score publico (0.8885 vs 0.87)\n",
    "# Pero V6 tiene mucho menos overfitting (0.09 vs 0.166)\n",
    "# Prefiero un modelo que generalice bien a uno que solo brille en el leaderboard publico\n",
    "# En el ranking privado creo que V6 sera mas estable\n",
    "#\n",
    "# ARQUITECTURA:\n",
    "# Simplemente un LSTM bidireccional con mucha regularizacion\n",
    "# Nada de fancy stuff, solo lo que funciona\n",
    "#\n",
    "# =========================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9adc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion del entorno para que TensorFlow no llene la consola de warnings\n",
    "# Estos warnings no aportan nada util, solo molestan\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'  # Evita warnings de protobuf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Solo errores, no warnings\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # Desactiva optimizaciones que dan warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports basicos para trabajar con datos\n",
    "import pandas as pd  # Para manejar los CSV\n",
    "import numpy as np  # Operaciones numericas\n",
    "import matplotlib.pyplot as plt  # Graficos\n",
    "import seaborn as sns  # Graficos bonitos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Quito todos los warnings molestos\n",
    "\n",
    "# Semilla para reproducibilidad - IMPORTANTE\n",
    "# Si no pongo esto, cada vez que ejecute tendre resultados diferentes\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Imports de TensorFlow/Keras para el modelo\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # Para convertir texto a numeros\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Para que todas las secuencias tengan el mismo tamaño\n",
    "from tensorflow.keras.models import Model  # Para construir el modelo\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,  # Capa de entrada\n",
    "    Embedding,  # Convierte palabras en vectores\n",
    "    LSTM,  # Red recurrente para secuencias\n",
    "    Bidirectional,  # Lee la secuencia en ambas direcciones\n",
    "    Dense,  # Capa densa normal\n",
    "    Dropout,  # Apaga neuronas aleatoriamente (regularizacion)\n",
    "    GlobalMaxPooling1D,  # No lo uso al final\n",
    "    SpatialDropout1D,  # Dropout especial para embeddings\n",
    "    Attention,  # No lo uso, causaba overfitting\n",
    "    Permute,  # No lo uso\n",
    "    Multiply  # No lo uso\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # Callbacks para el entrenamiento\n",
    "from tensorflow.keras.regularizers import l2  # Regularizacion L2\n",
    "\n",
    "# Mas semillas para TensorFlow (para que sea 100% reproducible)\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# Imports de sklearn para metricas\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix  # Metricas de evaluacion\n",
    "from sklearn.model_selection import train_test_split  # Para dividir train/val\n",
    "\n",
    "# Configuracion de pandas para ver mejor los datos\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5654352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIPERPARAMETROS - Estos son los valores que funcionaron mejor despues de 6 iteraciones\n",
    "\n",
    "# Preprocesamiento de texto\n",
    "MAX_WORDS = 10000  # Uso solo las 10k palabras mas frecuentes (el dataset tiene 46k)\n",
    "                   # Probe con 20k y no mejoro, las palabras raras solo añaden ruido\n",
    "MAX_LEN = 200  # Corto/relleno los textos a 200 tokens\n",
    "               # Probe con 250 y causaba overfitting\n",
    "EMBEDDING_DIM = 100  # Dimension de los vectores de palabras\n",
    "                     # 100 es suficiente, 128 causaba overfitting\n",
    "\n",
    "# Arquitectura del modelo\n",
    "LSTM_UNITS = 64  # Unidades LSTM (64*2=128 por ser bidireccional)\n",
    "                 # V1 tenia 128 (overfitting), V2 tenia 96, 64 es el optimo\n",
    "DENSE_UNITS = 32  # Neuronas en la capa densa\n",
    "                  # Lo justo para combinar las features sin sobreajustar\n",
    "\n",
    "# Regularizacion - CLAVE para controlar overfitting\n",
    "SPATIAL_DROPOUT = 0.4  # Dropout agresivo en embeddings\n",
    "                       # Apaga mapas completos de features\n",
    "DROPOUT_RATE = 0.7  # Dropout MUY agresivo en capa densa\n",
    "                    # Apaga el 70% de las conexiones - suena brutal pero es necesario\n",
    "L2_REG = 6e-4  # Regularizacion L2 en todas las capas con pesos\n",
    "               # V3 usaba 5e-4, subi a 6e-4 para mejor control\n",
    "               # Penaliza pesos grandes, fuerza al modelo a distribuir importancia\n",
    "\n",
    "# Entrenamiento\n",
    "BATCH_SIZE = 32  # Tamaño de lote estandar\n",
    "EPOCHS = 50  # Maximo de epochs (pero EarlyStopping para antes)\n",
    "VALIDATION_SPLIT = 0.2  # 80% train, 20% validacion\n",
    "LEARNING_RATE = 5e-4  # Learning rate conservador para convergencia estable\n",
    "                      # V1 usaba 1e-3 (demasiado agresivo)\n",
    "CLIPNORM = 1.0  # Gradient clipping para evitar que exploten los gradientes\n",
    "                # Importante en LSTMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de321008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos de entrenamiento\n",
    "# index_col=\"row_id\" para usar esa columna como indice\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9015a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESAMIENTO DEL TEXTO\n",
    "# Deliberadamente simple - no quito stopwords ni hago stemming porque pueden ser importantes para detectar spam\n",
    "\n",
    "# Creo el tokenizer que convertira texto a numeros\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')  # oov_token para palabras desconocidas\n",
    "tokenizer.fit_on_texts(train['text'])  # Aprende el vocabulario del texto de entrenamiento\n",
    "\n",
    "# Convierto los textos a secuencias de numeros\n",
    "X_train_seq = tokenizer.texts_to_sequences(train['text'])\n",
    "\n",
    "# Padding: ajusto todas las secuencias a la misma longitud\n",
    "# padding='post' -> relleno al final (preservo el inicio del mensaje)\n",
    "# truncating='post' -> corto por el final si es muy largo (preservo el inicio)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Etiquetas (0 = NOT SPAM, 1 = SPAM)\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "# Divido en train/validacion (80/20)\n",
    "# stratify=y_train para mantener la proporcion de clases (75% NOT SPAM, 25% SPAM)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, \n",
    "    test_size=VALIDATION_SPLIT, \n",
    "    random_state=seed, \n",
    "    stratify=y_train  # Importante: mantiene el balance de clases\n",
    ")\n",
    "\n",
    "# Muestro info basica\n",
    "print(f\"Muestras entrenamiento: {len(X_train_final)}\")\n",
    "print(f\"Muestras validacion: {len(X_val)}\")\n",
    "print(f\"Proporcion SPAM: {y_train.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7905cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_v6_model():\n",
    "    \"\"\"\n",
    "    Construyo mi modelo V6 - arquitectura simple pero efectiva\n",
    "    \n",
    "    Flujo:\n",
    "    Input -> Embedding -> Spatial Dropout -> Bi-LSTM -> Dense -> Dropout -> Output\n",
    "    \n",
    "    La clave esta en la regularizacion agresiva (dropout 0.7 + L2 6e-4)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Capa de entrada: secuencias de 200 tokens\n",
    "    inputs = Input(shape=(MAX_LEN,), name='input_sequences')\n",
    "    \n",
    "    # Embedding: convierte tokens a vectores de 100 dimensiones\n",
    "    # Entreno los embeddings desde cero (no uso preentrenados)\n",
    "    x = Embedding(\n",
    "        input_dim=MAX_WORDS,  # Vocabulario de 10k palabras\n",
    "        output_dim=EMBEDDING_DIM,  # Vectores de 100 dimensiones\n",
    "        input_length=MAX_LEN,\n",
    "        name='embedding'\n",
    "    )(inputs)\n",
    "    \n",
    "    # Spatial Dropout: apaga mapas completos de features (no elementos individuales)\n",
    "    # Esto previene que el modelo dependa de embeddings especificos\n",
    "    x = SpatialDropout1D(SPATIAL_DROPOUT, name='spatial_dropout')(x)\n",
    "    \n",
    "    # LSTM Bidireccional: lee el texto en ambas direcciones\n",
    "    # 64 units * 2 directions = 128 dimensiones efectivas\n",
    "    # L2 en kernel, recurrent y bias: regularizacion total\n",
    "    lstm_out = Bidirectional(\n",
    "        LSTM(\n",
    "            LSTM_UNITS,\n",
    "            kernel_regularizer=l2(L2_REG),  # Regulariza pesos input->hidden\n",
    "            recurrent_regularizer=l2(L2_REG),  # Regulariza pesos hidden->hidden\n",
    "            bias_regularizer=l2(L2_REG),  # Regulariza bias\n",
    "            return_sequences=False  # Solo quiero el output final, no toda la secuencia\n",
    "        ),\n",
    "        name='bidirectional_lstm'\n",
    "    )(x)\n",
    "    \n",
    "    # Capa densa: combina las features del LSTM\n",
    "    # ReLU como activacion (estandar)\n",
    "    dense = Dense(\n",
    "        DENSE_UNITS,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(L2_REG),  # Mas regularizacion L2\n",
    "        bias_regularizer=l2(L2_REG),\n",
    "        name='dense_classifier'\n",
    "    )(lstm_out)\n",
    "    \n",
    "    # Dropout brutal: 0.7 = apaga el 70% de las conexiones\n",
    "    # Suena excesivo pero es necesario para evitar overfitting\n",
    "    dense = Dropout(DROPOUT_RATE, name='dropout')(dense)\n",
    "    \n",
    "    # Output: sigmoid para probabilidad entre 0 y 1\n",
    "    # >0.5 = SPAM, <0.5 = NOT SPAM\n",
    "    outputs = Dense(1, activation='sigmoid', name='output')(dense)\n",
    "    \n",
    "    # Construyo el modelo\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='V6_LSTM_Final')\n",
    "    return model\n",
    "\n",
    "# Creo el modelo\n",
    "model = build_v6_model()\n",
    "\n",
    "# Optimizador AdamW con gradient clipping\n",
    "# AdamW es Adam mejorado con weight decay\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,  # 5e-4, conservador pero estable\n",
    "    weight_decay=1e-4,  # Weight decay adicional\n",
    "    clipnorm=CLIPNORM  # Limita la norma del gradiente a 1.0 (evita explosiones)\n",
    ")\n",
    "\n",
    "# Compilo el modelo\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',  # Loss estandar para clasificacion binaria\n",
    "    metrics=[\n",
    "        'accuracy',  # Porcentaje de aciertos\n",
    "        keras.metrics.Precision(name='precision'),  # De los que predigo SPAM, cuantos son SPAM\n",
    "        keras.metrics.Recall(name='recall'),  # De los SPAM reales, cuantos detecto\n",
    "        keras.metrics.AUC(name='auc')  # Area bajo la curva ROC\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Muestro resumen del modelo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODELO V6 CONSTRUIDO\")\n",
    "print(\"=\"*80)\n",
    "model.summary()\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91280f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks = [\n",
    "    # EarlyStopping: para si val_loss no mejora\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',  # Monitoreo val_loss\n",
    "        patience=2,  # Si no mejora en 2 epochs, paro\n",
    "        restore_best_weights=True,  # Al final, uso los pesos de la mejor epoch\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # ModelCheckpoint: guarda el mejor modelo\n",
    "    ModelCheckpoint(\n",
    "        'best_spam_model_v6.keras',  # Nombre del archivo\n",
    "        monitor='val_loss',  # Guardo cuando val_loss mejora\n",
    "        save_best_only=True,  # Solo guardo si mejora\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # ReduceLROnPlateau: reduce learning rate si val_loss se estanca\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,  # Divide LR por 2\n",
    "        patience=1,  # Espera solo 1 epoch antes de reducir\n",
    "        min_lr=1e-6,  # LR minimo\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# ENTRENO EL MODELO\n",
    "print(\"\\nIniciando entrenamiento...\")\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,  # Datos de entrenamiento\n",
    "    batch_size=BATCH_SIZE,  # Lotes de 32\n",
    "    epochs=EPOCHS,  # Maximo 50 epochs (pero EarlyStopping parara antes)\n",
    "    validation_data=(X_val, y_val),  # Datos de validacion\n",
    "    callbacks=callbacks,  # Uso los callbacks definidos arriba\n",
    "    verbose=1  # Muestro progreso\n",
    ")\n",
    "print(\"Entrenamiento completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUACION DEL MODELO EN VALIDACION\n",
    "\n",
    "# Obtengo probabilidades de prediccion\n",
    "y_pred_proba = model.predict(X_val, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "\n",
    "# Convierto probabilidades a clases (threshold 0.5)\n",
    "best_threshold = 0.5  # Estandar: >0.5 = SPAM, <0.5 = NOT SPAM\n",
    "y_pred = (y_pred_proba > best_threshold).astype(int)\n",
    "\n",
    "# Calculo MCC (metrica principal de la competicion)\n",
    "mcc_val = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUACION - METRICAS DE VALIDACION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"MCC en validacion (threshold 0.5): {mcc_val:.4f}\")\n",
    "\n",
    "# Analizo el entrenamiento para detectar overfitting\n",
    "final_epoch = len(history.history['loss'])  # Cuantas epochs se entrenaron\n",
    "train_loss_final = history.history['loss'][-1]  # Loss final de train\n",
    "val_loss_final = history.history['val_loss'][-1]  # Loss final de validacion\n",
    "train_acc_final = history.history['accuracy'][-1]  # Accuracy final de train\n",
    "val_acc_final = history.history['val_accuracy'][-1]  # Accuracy final de validacion\n",
    "\n",
    "# Delta de overfitting: diferencia entre val_loss y train_loss\n",
    "# Si es muy grande, hay overfitting\n",
    "overfitting_delta = val_loss_final - train_loss_final\n",
    "\n",
    "print(f\"\\nEntrenamiento detenido en epoch: {final_epoch}\")\n",
    "print(f\"Train Loss: {train_loss_final:.4f} | Val Loss: {val_loss_final:.4f}\")\n",
    "print(f\"Train Acc:  {train_acc_final:.4f} | Val Acc:  {val_acc_final:.4f}\")\n",
    "print(f\"Delta Overfitting: {overfitting_delta:.4f}\")\n",
    "\n",
    "# Interpreto el overfitting\n",
    "if overfitting_delta > 0.15:\n",
    "    print(\"\\nOVERFITTING DETECTADO (delta > 0.15)\")\n",
    "    print(\"-> Necesito MAS regularizacion\")\n",
    "elif overfitting_delta < 0.05:\n",
    "    print(\"\\nSin overfitting significativo (delta < 0.05)\")\n",
    "    print(\"-> Podria permitirme MENOS regularizacion\")\n",
    "else:\n",
    "    print(\"\\nOverfitting CONTROLADO (delta entre 0.05-0.15)\")\n",
    "    print(\"-> Balance perfecto!\")\n",
    "\n",
    "# Analizo la distribucion de probabilidades\n",
    "print(f\"\\nDistribucion de probabilidades:\")\n",
    "print(f\"  Media: {y_pred_proba.mean():.4f}\")\n",
    "print(f\"  Desv std: {y_pred_proba.std():.4f}\")\n",
    "print(f\"  Min: {y_pred_proba.min():.4f}\")\n",
    "print(f\"  Max: {y_pred_proba.max():.4f}\")\n",
    "\n",
    "# Analizo por clase\n",
    "spam_probs = y_pred_proba[y_val == 1]  # Probabilidades de mensajes SPAM reales\n",
    "notspam_probs = y_pred_proba[y_val == 0]  # Probabilidades de mensajes NOT SPAM reales\n",
    "\n",
    "print(f\"\\nProbabilidad media clase SPAM: {spam_probs.mean():.4f} (deberia ser > 0.5)\")\n",
    "print(f\"Probabilidad media clase NOT SPAM: {notspam_probs.mean():.4f} (deberia ser < 0.5)\")\n",
    "\n",
    "# Separacion de clases: cuanto separa el modelo las dos clases\n",
    "separation = spam_probs.mean() - notspam_probs.mean()\n",
    "print(f\"Separacion de clases: {separation:.4f} (cuanto mayor, mejor)\")\n",
    "\n",
    "if separation < 0.3:\n",
    "    print(\"-> BAJA separacion, el modelo no distingue bien\")\n",
    "elif separation > 0.6:\n",
    "    print(\"-> BUENA separacion, el modelo distingue claramente\")\n",
    "\n",
    "# Reporte de clasificacion completo\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSIFICATION REPORT:\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f97458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICCIONES EN TEST PARA SUBMISSION\n",
    "\n",
    "# Cargo datos de test\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "# Preproceso igual que train\n",
    "X_test_seq = tokenizer.texts_to_sequences(test['text'])  # Texto a numeros\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')  # Padding\n",
    "\n",
    "# Predigo\n",
    "y_test_proba = model.predict(X_test_pad, batch_size=BATCH_SIZE, verbose=0).flatten()  # Probabilidades\n",
    "y_test_pred = (y_test_proba > best_threshold).astype(int)  # Convierto a clases\n",
    "\n",
    "# Creo el submission\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_test_pred  # Reemplazo con mis predicciones\n",
    "submission.to_csv('submission.csv', index=False)  # Guardo\n",
    "\n",
    "print(f\"\\nSubmission creado: {len(y_test_pred)} predicciones\")\n",
    "print(f\"Threshold usado: {best_threshold:.2f}\")\n",
    "print(f\"Predicciones SPAM: {y_test_pred.sum()} ({y_test_pred.mean():.2%})\")\n",
    "print(f\"Predicciones NOT SPAM: {len(y_test_pred) - y_test_pred.sum()} ({(1-y_test_pred.mean()):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESUMEN Y DIAGNOSTICO AUTOMATICO\n",
    "# Este codigo analiza las metricas y me dice que hacer en la proxima iteracion\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Val MCC: {mcc_val:.4f} | Overfitting Delta: {overfitting_delta:.4f} | Separacion: {separation:.4f}\")\n",
    "print(\"\")\n",
    "\n",
    "# Arbol de decision basado en las metricas\n",
    "if overfitting_delta > 0.15:\n",
    "    print(\"DIAGNOSTICO: OVERFITTING SEVERO\")\n",
    "    print(\"ACCION:\")\n",
    "    print(\"   1. Aumentar L2: 6e-4 -> 8e-4\")\n",
    "    print(\"   2. Aumentar Dropout: 0.7 -> 0.75\")\n",
    "    print(\"   3. Reducir LSTM units: 64 -> 56\")\n",
    "    \n",
    "elif overfitting_delta < 0.05 and mcc_val < 0.88:\n",
    "    print(\"DIAGNOSTICO: UNDERFIT - Demasiada regularizacion\")\n",
    "    print(\"ACCION:\")\n",
    "    print(\"   1. Reducir L2: 6e-4 -> 4e-4\")\n",
    "    print(\"   2. Reducir Dropout: 0.7 -> 0.6\")\n",
    "    print(\"   3. Aumentar LSTM units: 64 -> 80\")\n",
    "    \n",
    "elif separation < 0.3:\n",
    "    print(\"DIAGNOSTICO: BAJA CAPACIDAD DISCRIMINATIVA\")\n",
    "    print(\"ACCION:\")\n",
    "    print(\"   1. Aumentar MAX_LEN: 200 -> 220\")\n",
    "    print(\"   2. Aumentar EMBEDDING_DIM: 100 -> 128\")\n",
    "    print(\"   3. Mantener regularizacion actual\")\n",
    "    \n",
    "elif mcc_val >= 0.87 and overfitting_delta <= 0.12:\n",
    "    print(\"DIAGNOSTICO: MODELO BALANCEADO\")\n",
    "    print(\"ACCION:\")\n",
    "    print(\"   1. Este es probablemente el mejor resultado alcanzable\")\n",
    "    print(\"   2. Usar ESTE modelo para submission\")\n",
    "    print(\"   3. Si el score de test es peor, el problema es el dataset, no el modelo\")\n",
    "\n",
    "else:\n",
    "    print(\"DIAGNOSTICO: AJUSTE FINO NECESARIO\")\n",
    "    print(\"ACCION:\")\n",
    "    print(\"   1. Probar L2 entre 5e-4 y 7e-4\")\n",
    "    print(\"   2. Considerar LSTM 68-72 units\")\n",
    "    print(\"   3. Mantener threshold 0.5\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCOMPARACION CON ITERACIONES PREVIAS:\")\n",
    "print(\"V2: MCC 0.8885 (overfitting Delta ~0.16) <- Mejor score publico pero mas overfitting\")\n",
    "print(\"V3: MCC 0.8733 (overfitting Delta <0.08) <- Base de V6\")\n",
    "print(f\"V6: MCC {mcc_val:.4f} (overfitting Delta {overfitting_delta:.4f}) <- MODELO ACTUAL\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7ee1d",
   "metadata": {},
   "source": [
    "# ANÁLISIS EXHAUSTIVO Y CONCLUSIONES FINALES\n",
    "\n",
    "## 1. Análisis de Métricas del Modelo Final\n",
    "\n",
    "Este apartado presenta el análisis completo de las métricas obtenidas por el modelo V6,\n",
    "incluyendo interpretación de resultados y comparación con iteraciones previas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# ANALISIS DETALLADO DE METRICAS FINALES\n",
    "# ==================================================================================\n",
    "# Aqui conecto los resultados con todo el proceso experimental de las 6 iteraciones\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\" \"*25 + \"ANALISIS DE METRICAS FINALES - MODELO V6\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Recopilo todas las metricas importantes\n",
    "final_metrics = {\n",
    "    'MCC Validacion': mcc_val,\n",
    "    'Accuracy Validacion': val_acc_final,\n",
    "    'Train Loss': train_loss_final,\n",
    "    'Validation Loss': val_loss_final,\n",
    "    'Overfitting Delta': overfitting_delta,\n",
    "    'Epochs Entrenados': final_epoch,\n",
    "    'Separacion de Clases': separation\n",
    "}\n",
    "\n",
    "print(\"\\nMETRICAS PRINCIPALES:\")\n",
    "print(\"-\" * 90)\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric:.<50} {value:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"INTERPRETACION:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# 1. Analizo MCC\n",
    "print(f\"\\n1. MCC (Matthews Correlation Coefficient): {mcc_val:.4f}\")\n",
    "print(\"-\" * 90)\n",
    "print(\"   MCC es la metrica oficial de la competicion\")\n",
    "print(\"   Es mejor que accuracy para datasets desbalanceados\")\n",
    "print(f\"\\n   Mi MCC de {mcc_val:.4f} significa:\")\n",
    "if mcc_val >= 0.87:\n",
    "    print(\"   -> EXCELENTE: Correlacion muy fuerte\")\n",
    "    print(\"   -> El modelo distingue claramente SPAM de NOT SPAM\")\n",
    "    print(\"   -> Estoy en el rango competitivo\")\n",
    "elif mcc_val >= 0.80:\n",
    "    print(\"   -> BUENO: Correlacion fuerte, funciona bien\")\n",
    "    print(\"   -> Hay margen de mejora pero es solido\")\n",
    "else:\n",
    "    print(\"   -> MEJORABLE: Deberia revisar la arquitectura\")\n",
    "\n",
    "# 2. Analizo Overfitting\n",
    "print(f\"\\n2. OVERFITTING (Delta: {overfitting_delta:.4f})\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"   Train Loss: {train_loss_final:.4f}\")\n",
    "print(f\"   Val Loss: {val_loss_final:.4f}\")\n",
    "print(f\"   Diferencia (Delta): {overfitting_delta:.4f}\")\n",
    "print(\"\\n   Que significa:\")\n",
    "if overfitting_delta < 0.05:\n",
    "    print(\"   -> EXCELENTE: Casi sin overfitting\")\n",
    "    print(\"   -> El modelo generaliza perfectamente\")\n",
    "    print(\"   -> Incluso podria reducir un poco la regularizacion\")\n",
    "elif overfitting_delta < 0.10:\n",
    "    print(\"   -> OPTIMO: Overfitting controlado\")\n",
    "    print(\"   -> Este es el balance perfecto\")\n",
    "    print(\"   -> No tocar la regularizacion, esta bien asi\")\n",
    "elif overfitting_delta < 0.15:\n",
    "    print(\"   -> ACEPTABLE: Ligero overfitting\")\n",
    "    print(\"   -> Funciona bien pero hay margen de mejora\")\n",
    "    print(\"   -> Considerar aumentar regularizacion\")\n",
    "else:\n",
    "    print(\"   -> PROBLEMATICO: Overfitting severo\")\n",
    "    print(\"   -> El modelo memoriza en lugar de aprender\")\n",
    "    print(\"   -> URGENTE: Aumentar regularizacion\")\n",
    "\n",
    "# 3. Analizo Separacion de Clases\n",
    "print(f\"\\n3. SEPARACION DE CLASES: {separation:.4f}\")\n",
    "print(\"-\" * 90)\n",
    "print(\"   Es la diferencia entre prob media de SPAM vs NOT SPAM\")\n",
    "print(\"   Mide que tan bien distingue el modelo ambas clases\")\n",
    "print(\"\\n   Mi separacion de {separation:.4f} significa:\")\n",
    "if separation > 0.6:\n",
    "    print(\"   -> EXCELENTE: El modelo separa muy claramente\")\n",
    "    print(\"   -> Alta confianza en las predicciones\")\n",
    "elif separation > 0.4:\n",
    "    print(\"   -> BUENO: Separacion clara\")\n",
    "    print(\"   -> El modelo tiene buena capacidad discriminativa\")\n",
    "elif separation > 0.3:\n",
    "    print(\"   -> ACEPTABLE: Separacion moderada\")\n",
    "    print(\"   -> Funciona pero podria mejorar\")\n",
    "else:\n",
    "    print(\"   -> INSUFICIENTE: Baja separacion\")\n",
    "    print(\"   -> El modelo no distingue bien las clases\")\n",
    "\n",
    "# Comparacion con otras iteraciones\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPARACION CON MIS ITERACIONES PREVIAS:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Data de todas mis iteraciones\n",
    "comparison_data = {\n",
    "    'V1 (Baseline)': {'MCC': 0.8665, 'Delta': 0.184, 'Status': 'Overfitting severo'},\n",
    "    'V2 (Mejor Publico)': {'MCC': 0.8885, 'Delta': 0.166, 'Status': 'Overfitting moderado'},\n",
    "    'V3 (Regularizacion)': {'MCC': 0.8733, 'Delta': 0.08, 'Status': 'Controlado'},\n",
    "    'V4 (DistilBERT)': {'MCC': 0.6456, 'Delta': 'N/A', 'Status': 'FRACASO TOTAL'},\n",
    "    'V5 (CNN+LSTM)': {'MCC': 0.83, 'Delta': 0.242, 'Status': 'Overfitting severo'},\n",
    "    'V6 (ESTE MODELO)': {'MCC': mcc_val, 'Delta': overfitting_delta, 'Status': 'SELECCIONADO'}\n",
    "}\n",
    "\n",
    "print(\"\\n{:<25} {:<12} {:<12} {:<25}\".format(\"Iteracion\", \"MCC\", \"Delta\", \"Estado\"))\n",
    "print(\"-\" * 90)\n",
    "for iteration, metrics in comparison_data.items():\n",
    "    delta_str = f\"{metrics['Delta']:.3f}\" if isinstance(metrics['Delta'], float) else metrics['Delta']\n",
    "    marker = \" <- FINAL\" if \"ESTE\" in iteration else \"\"\n",
    "    print(f\"{iteration:<25} {metrics['MCC']:.4f}      {delta_str:<12} {metrics['Status']:<25}{marker}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"CONCLUSION:\")\n",
    "print(\"V6 no es el que tiene mejor MCC (ese es V2 con 0.8885)\")\n",
    "print(\"PERO V6 tiene mucho mejor overfitting (0.09 vs 0.166)\")\n",
    "print(\"Prefiero estabilidad a score publico alto\")\n",
    "print(\"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad617ca5",
   "metadata": {},
   "source": [
    "## 2. Curvas de Aprendizaje y Diagnóstico de Overfitting/Underfitting\n",
    "\n",
    "Las curvas de aprendizaje son fundamentales para diagnosticar el comportamiento del modelo\n",
    "durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351e33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================================\n",
    "# CURVAS DE APRENDIZAJE\n",
    "# ==================================================================================\n",
    "# Visualizo como evoluciono el modelo durante el entrenamiento\n",
    "\n",
    "# Config de estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Curvas de Aprendizaje - Modelo V6 Final', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Rango de epochs para los graficos\n",
    "epochs_range = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "# --- GRAFICO 1: Loss ---\n",
    "ax1 = axes[0, 0]\n",
    "# Ploteo train loss y val loss\n",
    "ax1.plot(epochs_range, history.history['loss'], 'b-o', label='Train Loss', linewidth=2, markersize=6)\n",
    "ax1.plot(epochs_range, history.history['val_loss'], 'r-s', label='Validation Loss', linewidth=2, markersize=6)\n",
    "# Lineas horizontales para valores finales\n",
    "ax1.axhline(y=train_loss_final, color='b', linestyle='--', alpha=0.3, label=f'Final Train: {train_loss_final:.4f}')\n",
    "ax1.axhline(y=val_loss_final, color='r', linestyle='--', alpha=0.3, label=f'Final Val: {val_loss_final:.4f}')\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Evolucion del Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='best', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Añado etiqueta de diagnostico segun overfitting\n",
    "if overfitting_delta < 0.10:\n",
    "    ax1.text(0.5, 0.95, 'Overfitting: CONTROLADO', \n",
    "             transform=ax1.transAxes, ha='center', va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7),\n",
    "             fontsize=11, fontweight='bold')\n",
    "elif overfitting_delta < 0.15:\n",
    "    ax1.text(0.5, 0.95, 'Overfitting: LEVE', \n",
    "             transform=ax1.transAxes, ha='center', va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7),\n",
    "             fontsize=11, fontweight='bold')\n",
    "else:\n",
    "    ax1.text(0.5, 0.95, 'Overfitting: SEVERO', \n",
    "             transform=ax1.transAxes, ha='center', va='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.7),\n",
    "             fontsize=11, fontweight='bold')\n",
    "\n",
    "# --- GRAFICO 2: Accuracy ---\n",
    "ax2 = axes[0, 1]\n",
    "# Ploteo train accuracy y val accuracy\n",
    "ax2.plot(epochs_range, history.history['accuracy'], 'b-o', label='Train Accuracy', linewidth=2, markersize=6)\n",
    "ax2.plot(epochs_range, history.history['val_accuracy'], 'r-s', label='Validation Accuracy', linewidth=2, markersize=6)\n",
    "# Lineas finales\n",
    "ax2.axhline(y=train_acc_final, color='b', linestyle='--', alpha=0.3)\n",
    "ax2.axhline(y=val_acc_final, color='r', linestyle='--', alpha=0.3)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Evolucion de Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='best', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim([0.85, 1.0])  # Zoom para ver mejor\n",
    "\n",
    "# --- GRAFICO 3: Learning Rate ---\n",
    "ax3 = axes[1, 0]\n",
    "# Si hay historial de LR, lo ploteo\n",
    "if 'lr' in history.history:\n",
    "    ax3.plot(epochs_range, history.history['lr'], 'g-o', linewidth=2, markersize=6)\n",
    "    ax3.set_xlabel('Epoch', fontsize=12)\n",
    "    ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax3.set_title('Evolucion del Learning Rate', fontsize=14, fontweight='bold')\n",
    "    ax3.set_yscale('log')  # Escala logaritmica\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "else:\n",
    "    # Si no hay historial, muestro mensaje\n",
    "    ax3.text(0.5, 0.5, 'Learning Rate history no disponible\\n(ReduceLROnPlateau aplicado)', \n",
    "             ha='center', va='center', fontsize=12, transform=ax3.transAxes)\n",
    "    ax3.set_title('Evolucion del Learning Rate', fontsize=14, fontweight='bold')\n",
    "\n",
    "# --- GRAFICO 4: Overfitting Delta ---\n",
    "ax4 = axes[1, 1]\n",
    "# Calculo delta en cada epoch\n",
    "delta_history = [val - train for val, train in zip(history.history['val_loss'], history.history['loss'])]\n",
    "# Ploteo\n",
    "ax4.plot(epochs_range, delta_history, 'purple', linewidth=2, marker='o', markersize=6)\n",
    "# Lineas de referencia\n",
    "ax4.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.3)\n",
    "ax4.axhline(y=0.10, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='Threshold 0.10')\n",
    "ax4.axhline(y=0.15, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Threshold 0.15')\n",
    "# Area bajo la curva\n",
    "ax4.fill_between(epochs_range, 0, delta_history, alpha=0.3, color='purple')\n",
    "ax4.set_xlabel('Epoch', fontsize=12)\n",
    "ax4.set_ylabel('Overfitting Delta (Val Loss - Train Loss)', fontsize=12)\n",
    "ax4.set_title('Evolucion del Overfitting', fontsize=14, fontweight='bold')\n",
    "ax4.legend(loc='best', fontsize=10)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# DIAGNOSTICO BASADO EN LAS CURVAS\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"DIAGNOSTICO BASADO EN CURVAS:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Analizo velocidad de convergencia\n",
    "num_epochs = len(history.history['loss'])\n",
    "if num_epochs < 5:\n",
    "    print(\"\\nCONVERGENCIA PREMATURA:\")\n",
    "    print(\"   Se detuvo muy rapido. Considerar:\")\n",
    "    print(\"   - Aumentar patience de EarlyStopping\")\n",
    "    print(\"   - Reducir learning rate inicial\")\n",
    "elif num_epochs > 30:\n",
    "    print(\"\\nCONVERGENCIA LENTA:\")\n",
    "    print(\"   Necesito muchas epochs. Considerar:\")\n",
    "    print(\"   - Aumentar learning rate inicial\")\n",
    "    print(\"   - Ajustar ReduceLROnPlateau\")\n",
    "else:\n",
    "    print(f\"\\nCONVERGENCIA ADECUADA: {num_epochs} epochs\")\n",
    "    print(\"   El modelo converge en tiempo razonable\")\n",
    "\n",
    "# Analizo tendencias de las ultimas 3 epochs\n",
    "train_loss_trend = history.history['loss'][-3:] if num_epochs >= 3 else history.history['loss']\n",
    "val_loss_trend = history.history['val_loss'][-3:] if num_epochs >= 3 else history.history['val_loss']\n",
    "\n",
    "# Train loss debe ir bajando\n",
    "if all(train_loss_trend[i] > train_loss_trend[i+1] for i in range(len(train_loss_trend)-1)):\n",
    "    print(\"\\nTRAIN LOSS: Descendente (BIEN)\")\n",
    "else:\n",
    "    print(\"\\nTRAIN LOSS: Con oscilaciones\")\n",
    "\n",
    "# Val loss idealmente baja o se estanca\n",
    "if all(val_loss_trend[i] > val_loss_trend[i+1] for i in range(len(val_loss_trend)-1)):\n",
    "    print(\"VAL LOSS: Descendente (BIEN)\")\n",
    "elif len(set(val_loss_trend)) == 1:\n",
    "    print(\"VAL LOSS: Estancada (convergencia prematura?)\")\n",
    "else:\n",
    "    print(\"VAL LOSS: Con oscilaciones (normal con ReduceLROnPlateau)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
