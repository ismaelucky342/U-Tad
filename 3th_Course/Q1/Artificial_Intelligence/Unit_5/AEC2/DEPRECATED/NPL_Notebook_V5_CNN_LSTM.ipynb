{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteracion 5 - Mi intento con arquitectura hibrida CNN + LSTM\n",
    "# Despues del desastre de DistilBERT (V4), volvi a LSTM pero con un twist\n",
    "# 多Y si combino CNN (para capturar patrones locales) con LSTM (para el contexto)?\n",
    "\n",
    "# La idea:\n",
    "# - CNN: Captura n-gramas tipo \"click here\", \"free money\", \"urgent!!!\" (patrones tipicos de spam)\n",
    "# - LSTM: Captura el contexto general del mensaje\n",
    "# - Junto las features de ambos y clasifico\n",
    "\n",
    "# Spoiler: Funciono mejor que V4 pero no supero a V2 (0.81-0.86 vs 0.8885)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fc328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de entorno para silenciar warnings molestos\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports basicos\n",
    "import pandas as pd  # CSV\n",
    "import numpy as np  # Operaciones numericas\n",
    "import matplotlib.pyplot as plt  # Graficos\n",
    "import seaborn as sns  # Graficos bonitos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Nada de warnings molestos\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, Dense, Dropout, LSTM, Bidirectional, \n",
    "    Conv1D, GlobalMaxPooling1D, Concatenate, Input, SpatialDropout1D  # NUEVO: CNN layers\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ae89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametros - V5 Arquitectura Hibrida CNN+LSTM\n",
    "# Probe MUCHAS combinaciones diferentes (esto fue un experimento largo)\n",
    "\n",
    "# Text processing\n",
    "MAX_WORDS = 50000  # Aumente el vocabulario para capturar mas patrones\n",
    "MAX_LEN = 200  # Vuelvo a 200 (250 era demasiado)\n",
    "\n",
    "# Embedding\n",
    "EMBEDDING_DIM = 128  # 150 sobreajustaba, bajo a 128\n",
    "\n",
    "# CNN branch - Para capturar n-gramas\n",
    "CNN_FILTERS = 128  # Numero de filtros CNN\n",
    "CNN_KERNEL_SIZES = [2, 3, 4, 5]  # Capturo bigramas, trigramas, 4-gramas y 5-gramas\n",
    "\n",
    "# LSTM branch - Para capturar contexto\n",
    "LSTM_UNITS = 96  # Igual que V2 (funcionaba bien)\n",
    "\n",
    "# Dense classifier\n",
    "DENSE_UNITS = 64  # Reducido para evitar overfitting\n",
    "\n",
    "# Regularization - Fuerte para controlar overfitting\n",
    "SPATIAL_DROPOUT = 0.3  # Dropout espacial en embeddings\n",
    "DROPOUT = 0.5  # Dropout en clasificador\n",
    "L2_REG = 1e-4  # Regularizacion L2\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-3  # Learning rate bajito\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50924ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos y preparo las secuencias\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "\n",
    "# Tokenizo con vocabulario ampliado\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train['text'])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "# Divido train/validation\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, test_size=VALIDATION_SPLIT, random_state=seed, stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_lstm_model():\n",
    "    \"\"\"\n",
    "    Modelo hibrido CNN + LSTM\n",
    "    \n",
    "    La arquitectura:\n",
    "    1. Input + Embedding + SpatialDropout\n",
    "    2. RAMA CNN: Varios Conv1D con diferentes kernel_sizes (para capturar n-gramas de diferentes tama単os)\n",
    "    3. RAMA LSTM: Bi-LSTM para capturar contexto secuencial\n",
    "    4. Concateno las features de CNN y LSTM\n",
    "    5. Clasificador Dense con dropout\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = Input(shape=(MAX_LEN,))\n",
    "    \n",
    "    # Capa embedding compartida por CNN y LSTM\n",
    "    x = Embedding(\n",
    "        input_dim=MAX_WORDS,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_LEN\n",
    "    )(inputs)\n",
    "    x = SpatialDropout1D(SPATIAL_DROPOUT)(x)  # Dropout espacial para regularizar embeddings\n",
    "    \n",
    "    # RAMA CNN: Creo una Conv1D por cada kernel_size\n",
    "    cnn_outputs = []\n",
    "    for kernel_size in CNN_KERNEL_SIZES:\n",
    "        # Conv1D para capturar n-gramas de tama単o kernel_size\n",
    "        conv = Conv1D(\n",
    "            filters=CNN_FILTERS,\n",
    "            kernel_size=kernel_size,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            padding='same'\n",
    "        )(x)\n",
    "        # GlobalMaxPooling para quedarse con la feature mas importante\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        cnn_outputs.append(pool)\n",
    "    \n",
    "    # RAMA LSTM: Bi-LSTM para capturar dependencias largas\n",
    "    lstm_out = Bidirectional(\n",
    "        LSTM(\n",
    "            LSTM_UNITS,\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            recurrent_regularizer=l2(L2_REG),\n",
    "            dropout=0.3,\n",
    "            recurrent_dropout=0.3\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    # Junto todas las features (CNN + LSTM)\n",
    "    concatenated = Concatenate()(cnn_outputs + [lstm_out])\n",
    "    \n",
    "    # Clasificador final\n",
    "    dense = Dense(\n",
    "        DENSE_UNITS,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(L2_REG)\n",
    "    )(concatenated)\n",
    "    dense = Dropout(DROPOUT)(dense)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs, name='CNN_LSTM_Hybrid_V5')\n",
    "    return model\n",
    "\n",
    "# Construyo el modelo\n",
    "model = build_cnn_lstm_model()\n",
    "\n",
    "# Compilo con Adam\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1d9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno con early stopping mas agresivo (para evitar overfitting)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint('best_cnn_lstm_model.keras', monitor='val_loss', save_best_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluo en validation\n",
    "y_pred_proba = model.predict(X_val, batch_size=BATCH_SIZE, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(f\"MCC: {mcc_score:.4f}\")\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "\n",
    "# Matriz de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'CNN+LSTM Hybrid (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predigo en test y creo submission\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "y_pred_test = model.predict(X_test_pad, batch_size=BATCH_SIZE, verbose=0)\n",
    "y_pred_test = (y_pred_test > 0.5).astype(int).flatten()\n",
    "\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"Submission created. Test predictions: {len(y_pred_test)}\")\n",
    "print(\"\\nResultado V5:\")\n",
    "print(\"MCC publico: Entre 0.81-0.86 (vario segun los hyperparametros)\")\n",
    "print(\"No supero a V2 (0.8885) pero fue mejor que V4 (0.6456)\")\n",
    "print(\"\\nLecciones:\")\n",
    "print(\"  - La arquitectura hibrida funciona pero a単ade complejidad\")\n",
    "print(\"  - Dificil balancear CNN y LSTM para no overfittear\")\n",
    "print(\"  - A veces mas simple (solo LSTM) es mejor\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
