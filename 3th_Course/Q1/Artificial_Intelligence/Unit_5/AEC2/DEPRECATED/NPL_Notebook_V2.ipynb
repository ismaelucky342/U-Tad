{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee074d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR AEC2                           ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        04/12/2025  -  16:45:12           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    04/12/2025  -  19:20:30           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d6971",
   "metadata": {},
   "source": [
    "# Iteración 2: Mejora en Regularización\n",
    "**Estado: DEPRECATED**\n",
    "\n",
    "En esta iteración, mi objetivo principal es reducir el sobreajuste observado en la V1. Para ello, he introducido regularización L2 en las capas LSTM y densas, además de aumentar las tasas de Dropout y añadir SpatialDropout1D tras el embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fe618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuro variables de entorno para optimizar TensorFlow y protobuf.\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo las bibliotecas necesarias, incluyendo regularizadores L2.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Bidirectional, Dense, Dropout, \n",
    "    GlobalMaxPooling1D, Conv1D, SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2  \n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    for gpu in gpu_devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10985c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino los hiperparámetros ajustados para reducir sobreajuste, incluyendo L2 regularization.\n",
    "MAX_WORDS = 10000  \n",
    "MAX_LEN = 200  \n",
    "EMBEDDING_DIM = 100  \n",
    "LSTM_UNITS = 96           \n",
    "DENSE_UNITS = 48          \n",
    "DROPOUT_RATE = 0.6        \n",
    "SPATIAL_DROPOUT = 0.3     \n",
    "L2_REG = 1e-4             \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 1e-3\n",
    "estimated_params = (\n",
    "    MAX_WORDS * EMBEDDING_DIM +  \n",
    "    4 * LSTM_UNITS * (EMBEDDING_DIM + LSTM_UNITS + 1) * 2 +  \n",
    "    (LSTM_UNITS * 2) * DENSE_UNITS + DENSE_UNITS +  \n",
    "    DENSE_UNITS + 1  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca78f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos de entrenamiento.\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb096117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizo la distribución de la longitud de los textos.\n",
    "train['text_length'] = train['text'].apply(lambda x: len(str(x).split()))\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(train['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribución de Longitud de Textos')\n",
    "plt.xlabel('Número de palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.axvline(train['text_length'].mean(), color='red', linestyle='--', label=f'Media: {train[\"text_length\"].mean():.1f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 3, 2)\n",
    "train.groupby('spam_label')['text_length'].hist(bins=30, alpha=0.7, label=['Not SPAM', 'SPAM'])\n",
    "plt.title('Longitud por Clase')\n",
    "plt.xlabel('Número de palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=train, x='spam_label', y='text_length')\n",
    "plt.title('Boxplot Longitud por Clase')\n",
    "plt.xlabel('Clase (0=Not SPAM, 1=SPAM)')\n",
    "plt.ylabel('Número de palabras')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac080af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizo los textos y divido en train/val.\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, \n",
    "    test_size=VALIDATION_SPLIT, \n",
    "    random_state=seed,\n",
    "    stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c575250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construyo el modelo V2 con regularización L2 y compilo.\n",
    "def build_model_v2():\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=MAX_WORDS,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_LEN,\n",
    "            name='embedding'\n",
    "        ),\n",
    "        SpatialDropout1D(SPATIAL_DROPOUT),\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                LSTM_UNITS, \n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(L2_REG),      \n",
    "                recurrent_regularizer=l2(L2_REG)    \n",
    "            ), \n",
    "            name='bidirectional_lstm'\n",
    "        ),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(\n",
    "            DENSE_UNITS, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(L2_REG),          \n",
    "            name='dense_1'\n",
    "        ),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ], name='spam_classifier_v2')\n",
    "    return model\n",
    "model = build_model_v2()\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb5ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuro callbacks con paciencia reducida y entreno.\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,  \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_spam_model_v2.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,  \n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c10bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predigo en validación y genero la matriz de confusión V2.\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'Confusion Matrix V2 (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786ca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero las curvas de aprendizaje para evaluar el impacto de la regularización.\n",
    "def plot_learning_curves(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos de prueba.\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizo y predigo en el conjunto de prueba.\n",
    "X_test_text = test['text'].values\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y_pred_proba_test = model.predict(X_test_pad, batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el archivo de submission para V2.\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)  \n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9147f52d",
   "metadata": {},
   "source": [
    "## Análisis de la Iteración 2\n",
    "La introducción de regularización L2 y el aumento de Dropout han dado sus frutos: el MCC ha subido a 0.8685 y el gap entre entrenamiento y validación se ha reducido notablemente. El modelo es ahora mucho más robusto.\n",
    "\n",
    "**Conclusión:** Aunque la regularización ha funcionado, creo que el modelo aún puede mejorar si capturamos patrones locales más específicos. En la siguiente iteración exploraré una arquitectura híbrida."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
