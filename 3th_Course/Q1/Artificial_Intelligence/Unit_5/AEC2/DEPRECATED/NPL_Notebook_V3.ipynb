{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR AEC2                           ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        05/12/2025  -  11:20:05           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    05/12/2025  -  14:55:18           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e493901",
   "metadata": {},
   "source": [
    "# Iteración 3: (Regularización)\n",
    "**Estado: DEPRECATED**\n",
    "\n",
    "En este experimento, he intentado aplicar una regularización extrema (L2 más alta, Dropout del 0.7) para ver si podía eliminar por completo el sobreajuste. Aunque el modelo se volvió muy estable, el rendimiento global no superó a la V2, lo que sugiere que estábamos empezando a sufrir de *underfitting* en algunas áreas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuro variables de entorno y verifico protobuf.\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "import sys\n",
    "try:\n",
    "    import google.protobuf\n",
    "    if hasattr(google.protobuf, '__version__'):\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b705b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo las bibliotecas con TensorFlow y regularizadores.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Bidirectional, Dense, Dropout, \n",
    "    GlobalMaxPooling1D, Conv1D, SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef09c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino hiperparámetros.\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_UNITS = 64           \n",
    "DENSE_UNITS = 32          \n",
    "DROPOUT_RATE = 0.7        \n",
    "SPATIAL_DROPOUT = 0.4     \n",
    "L2_REG = 5e-4             \n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 5e-4      \n",
    "CLIPNORM = 1.0            \n",
    "estimated_params_v3 = (\n",
    "    MAX_WORDS * EMBEDDING_DIM +\n",
    "    4 * LSTM_UNITS * (EMBEDDING_DIM + LSTM_UNITS + 1) * 2 +\n",
    "    (LSTM_UNITS * 2) * DENSE_UNITS + DENSE_UNITS +\n",
    "    DENSE_UNITS + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43721cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos de entrenamiento.\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ec7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizo la longitud de los textos con gráficos simplificados.\n",
    "train['text_length'] = train['text'].apply(lambda x: len(str(x).split()))\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribución de Longitud de Textos')\n",
    "plt.xlabel('Número de palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.axvline(train['text_length'].mean(), color='red', linestyle='--', label=f'Media: {train[\"text_length\"].mean():.1f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=train, x='spam_label', y='text_length')\n",
    "plt.title('Longitud por Clase')\n",
    "plt.xlabel('Clase (0=Not SPAM, 1=SPAM)')\n",
    "plt.ylabel('Número de palabras')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb58216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizo los textos y preparo train/val.\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, \n",
    "    test_size=VALIDATION_SPLIT, \n",
    "    random_state=seed,\n",
    "    stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construyo el modelo \n",
    "def build_model_v3():\n",
    "    model = Sequential([\n",
    "        Embedding(\n",
    "            input_dim=MAX_WORDS,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_LEN,\n",
    "            name='embedding'\n",
    "        ),\n",
    "        SpatialDropout1D(SPATIAL_DROPOUT),\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                LSTM_UNITS, \n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(L2_REG),\n",
    "                recurrent_regularizer=l2(L2_REG),\n",
    "                bias_regularizer=l2(L2_REG)     \n",
    "            ), \n",
    "            name='bidirectional_lstm'\n",
    "        ),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(\n",
    "            DENSE_UNITS, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            bias_regularizer=l2(L2_REG),        \n",
    "            name='dense_1'\n",
    "        ),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ], name='spam_classifier_v3_shock_therapy')\n",
    "    return model\n",
    "model = build_model_v3()\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=1e-4,\n",
    "    clipnorm=CLIPNORM  \n",
    ")\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e80b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno con paciencia mínima \n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=2,  \n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_spam_model_v3.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=1,  \n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predigo en validación y genero la matriz de confusión V3.\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'Confusion Matrix V3 - Shock Therapy (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daed813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero las curvas de aprendizaje para analizar el efecto de la regularización.\n",
    "def plot_learning_curves(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443772fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos de prueba.\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizo y predigo en el conjunto de prueba.\n",
    "X_test_text = test['text'].values\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y_pred_proba_test = model.predict(X_test_pad, batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73547050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el archivo de submission para V3.\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c3aaf9",
   "metadata": {},
   "source": [
    "## Análisis de la Iteración 3\n",
    "Este experimento de de regularizar un poco a lo bestia ha demostrado que existe un límite para la regularización. Aunque el sobreajuste es casi inexistente, el MCC ha bajado ligeramente respecto a la V2. El modelo es demasiado \"rígido\" para capturar la complejidad del lenguaje en algunos mensajes de spam.\n",
    "\n",
    "**Conclusión:** Volveré a los niveles de regularización de la V2 pero cambiaré la arquitectura para mejorar la extracción de características."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
