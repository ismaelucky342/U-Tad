{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version 1 - Mi primer modelo baseline para clasificar SPAM\n",
    "# Este fue mi punto de partida, sin optimizaciones complejas\n",
    "# Solo queria ver que tal funcionaba un Bi-LSTM basico\n",
    "# Resultado: MCC publico 0.8665 - No esta mal para empezar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:06:07.216455Z",
     "iopub.status.busy": "2025-11-04T06:06:07.21624Z",
     "iopub.status.idle": "2025-11-04T06:06:10.796746Z",
     "shell.execute_reply": "2025-11-04T06:06:10.795976Z",
     "shell.execute_reply.started": "2025-11-04T06:06:07.216437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuracion inicial del entorno\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'  # Fix para Kaggle\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Menos warnings de TensorFlow\n",
    "\n",
    "# Imports basicos\n",
    "import pandas as pd  # Manejo de datos\n",
    "import numpy as np  # Operaciones numericas\n",
    "import matplotlib.pyplot as plt  # Graficos\n",
    "import seaborn as sns  # Graficos bonitos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Nada de warnings\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# TensorFlow y Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer  # Para tokenizar textos\n",
    "from keras.preprocessing.sequence import pad_sequences  # Para padding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import (\n",
    "    Embedding, LSTM, Bidirectional, Dense, Dropout, \n",
    "    GlobalMaxPooling1D, Conv1D, SpatialDropout1D\n",
    ")\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# Metricas de evaluacion\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"Python version: {os.sys.version}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"GPU Devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Random seed: {seed}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuracion del modelo baseline\n",
    "\n",
    "Hyperparametros iniciales sin mucha optimizacion, solo para probar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametros para V1 - Mi primer intento\n",
    "MAX_WORDS = 10000  # Vocabulario de 10k palabras mas comunes\n",
    "MAX_LEN = 200  # Secuencias de hasta 200 tokens\n",
    "EMBEDDING_DIM = 100  # Embeddings de 100 dimensiones\n",
    "\n",
    "# Arquitectura del modelo\n",
    "LSTM_UNITS = 128  # Bi-LSTM con 128 unidades (quizas demasiado grande)\n",
    "DENSE_UNITS = 64  # Capa densa de 64 neuronas\n",
    "DROPOUT_RATE = 0.5  # Dropout del 50%\n",
    "SPATIAL_DROPOUT = 0.2  # Spatial dropout en embeddings\n",
    "\n",
    "# Configuracion de entrenamiento\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50  # Hasta 50 epochs pero con early stopping\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary Size: {MAX_WORDS:,}\")\n",
    "print(f\"Sequence Length: {MAX_LEN}\")\n",
    "print(f\"Embedding Dimension: {EMBEDDING_DIM}\")\n",
    "print(f\"LSTM Units: {LSTM_UNITS}\")\n",
    "print(f\"Dense Units: {DENSE_UNITS}\")\n",
    "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Max Epochs: {EPOCHS}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos y exploraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:06:10.799421Z",
     "iopub.status.busy": "2025-11-04T06:06:10.798848Z",
     "iopub.status.idle": "2025-11-04T06:06:10.983318Z",
     "shell.execute_reply": "2025-11-04T06:06:10.982344Z",
     "shell.execute_reply.started": "2025-11-04T06:06:10.799391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cargo el dataset de entrenamiento\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DATA OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(train):,}\")\n",
    "print(f\"\\nColumns: {list(train.columns)}\")\n",
    "print(f\"\\nData types:\\n{train.dtypes}\")\n",
    "print(f\"\\nNull values:\\n{train.isnull().sum()}\")\n",
    "print(f\"\\nClass distribution:\\n{train['spam_label'].value_counts()}\")\n",
    "print(f\"\\nClass balance:\\n{train['spam_label'].value_counts(normalize=True)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Muestro algunas filas\n",
    "print(\"\\nSample data:\")\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizo cuanto miden los textos (para decidir MAX_LEN)\n",
    "train['text_length'] = train['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Grafico la distribucion de longitudes\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(train['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribuci√≥n de Longitud de Textos')\n",
    "plt.xlabel('N√∫mero de palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.axvline(train['text_length'].mean(), color='red', linestyle='--', label=f'Media: {train[\"text_length\"].mean():.1f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "train.groupby('spam_label')['text_length'].hist(bins=30, alpha=0.7, label=['Not SPAM', 'SPAM'])\n",
    "plt.title('Longitud por Clase')\n",
    "plt.xlabel('N√∫mero de palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "sns.boxplot(data=train, x='spam_label', y='text_length')\n",
    "plt.title('Boxplot Longitud por Clase')\n",
    "plt.xlabel('Clase (0=Not SPAM, 1=SPAM)')\n",
    "plt.ylabel('N√∫mero de palabras')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Longitud promedio: {train['text_length'].mean():.2f} palabras\")\n",
    "print(f\"Longitud mediana: {train['text_length'].median():.2f} palabras\")\n",
    "print(f\"Longitud m√°xima: {train['text_length'].max()} palabras\")\n",
    "print(f\"Longitud m√≠nima: {train['text_length'].min()} palabras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento - Tokenizacion y padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preparo textos y etiquetas\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "# Tokenizo los textos (convierto palabras a numeros)\n",
    "print(\"Tokenizando textos...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")  # OOV = Out Of Vocabulary\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "# Convierto textos a secuencias de numeros\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "\n",
    "# Padding: hago que todas las secuencias tengan la misma longitud\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Divido en train y validation\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, \n",
    "    test_size=VALIDATION_SPLIT, \n",
    "    random_state=seed,\n",
    "    stratify=y_train  # Mantengo la proporcion de clases\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index):,}\")\n",
    "print(f\"Training samples: {len(X_train_final):,}\")\n",
    "print(f\"Validation samples: {len(X_val):,}\")\n",
    "print(f\"Sequence shape: {X_train_pad.shape}\")\n",
    "print(f\"Train class distribution: {np.bincount(y_train_final)}\")\n",
    "print(f\"Val class distribution: {np.bincount(y_val)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mi modelo V1 - Bi-LSTM baseline\n",
    "\n",
    "**Arquitectura simple:**\n",
    "Embedding ‚Üí SpatialDropout ‚Üí Bi-LSTM ‚Üí GlobalMaxPooling ‚Üí Dense ‚Üí Dropout ‚Üí Output\n",
    "\n",
    "Nada demasiado complejo, solo quiero ver que MCC consigo con un modelo basico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construccion del modelo V1\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        # Embeddings: convierto numeros a vectores densos\n",
    "        Embedding(\n",
    "            input_dim=MAX_WORDS,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_LEN,\n",
    "            name='embedding'\n",
    "        ),\n",
    "        \n",
    "        # Spatial Dropout: dropout pero manteniendo coherencia espacial\n",
    "        SpatialDropout1D(SPATIAL_DROPOUT),\n",
    "        \n",
    "        # Bi-LSTM: Lee el texto en ambas direcciones (izq‚Üíder y der‚Üíizq)\n",
    "        Bidirectional(LSTM(LSTM_UNITS, return_sequences=True), name='bidirectional_lstm'),\n",
    "        \n",
    "        # GlobalMaxPooling: me quedo con las features mas importantes\n",
    "        GlobalMaxPooling1D(),\n",
    "        \n",
    "        # Capa densa con ReLU\n",
    "        Dense(DENSE_UNITS, activation='relu', name='dense_1'),\n",
    "        Dropout(DROPOUT_RATE),  # Dropout del 50% para regularizar\n",
    "        \n",
    "        # Salida: sigmoid para probabilidad de SPAM\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ], name='spam_classifier_v1')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creo el modelo\n",
    "model = build_model()\n",
    "\n",
    "# Compilo con AdamW (Adam con weight decay)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Veo el resumen\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPILED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(f\"Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay=1e-4)\")\n",
    "print(f\"Loss: Binary Crossentropy\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks para controlar el entrenamiento\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,  # Si no mejora en 5 epochs, paro\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_spam_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,  # Solo guardo el mejor modelo\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,  # Reduzco LR a la mitad si no mejora\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Entreno el modelo\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,  # Hasta 50 pero early stopping probablemente pare antes\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluacion en validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predigo en validation para ver que tal fue\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculo el MCC (la metrica de la competicion)\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc_score:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Matriz de confusion\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'Confusion Matrix (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvas de aprendizaje (para detectar overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de curvas de aprendizaje\n",
    "def plot_learning_curves(history, title_prefix=\"\"):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[0, 0].set_title(f'{title_prefix} Loss', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    axes[0, 1].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    axes[0, 1].set_title(f'{title_prefix} Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision\n",
    "    axes[1, 0].plot(history.history['precision'], label='Train Precision', linewidth=2)\n",
    "    axes[1, 0].plot(history.history['val_precision'], label='Val Precision', linewidth=2)\n",
    "    axes[1, 0].set_title(f'{title_prefix} Precision', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall\n",
    "    axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2)\n",
    "    axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2)\n",
    "    axes[1, 1].set_title(f'{title_prefix} Recall', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history, title_prefix=\"Model\")\n",
    "\n",
    "# An√°lisis de overfitting\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final Train Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Final Val Loss: {final_val_loss:.4f}\")\n",
    "print(f\"Loss Difference: {abs(final_val_loss - final_train_loss):.4f}\")\n",
    "print(f\"\\nFinal Train Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Val Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Accuracy Difference: {abs(final_val_acc - final_train_acc):.4f}\")\n",
    "\n",
    "if abs(final_val_loss - final_train_loss) < 0.1:\n",
    "    print(\"\\n‚úì Modelo bien balanceado - No hay overfitting significativo\")\n",
    "elif final_val_loss > final_train_loss:\n",
    "    print(\"\\n‚ö† Posible overfitting - Val loss mayor que train loss\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Posible underfitting - Modelo podr√≠a mejorar\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones en Test Data\n",
    "\n",
    "Generaci√≥n de predicciones para el conjunto de test de la competici√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:06:10.984694Z",
     "iopub.status.busy": "2025-11-04T06:06:10.984434Z",
     "iopub.status.idle": "2025-11-04T06:06:11.115096Z",
     "shell.execute_reply": "2025-11-04T06:06:11.11422Z",
     "shell.execute_reply.started": "2025-11-04T06:06:10.984674Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total test samples: {len(test):,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:06:11.116879Z",
     "iopub.status.busy": "2025-11-04T06:06:11.116104Z",
     "iopub.status.idle": "2025-11-04T06:06:11.1208Z",
     "shell.execute_reply": "2025-11-04T06:06:11.119841Z",
     "shell.execute_reply.started": "2025-11-04T06:06:11.116849Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Preprocesar test data\n",
    "X_test_text = test['text'].values\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Generar predicciones\n",
    "print(\"Generando predicciones en test data...\")\n",
    "y_pred_proba_test = model.predict(X_test_pad, batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"Predicciones generadas: {len(y_pred_test):,}\")\n",
    "print(f\"Distribuci√≥n de predicciones:\")\n",
    "print(f\"  Not SPAM (0): {np.sum(y_pred_test == 0):,} ({np.mean(y_pred_test == 0)*100:.2f}%)\")\n",
    "print(f\"  SPAM (1): {np.sum(y_pred_test == 1):,} ({np.mean(y_pred_test == 1)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generaci√≥n del Archivo de Submission\n",
    "\n",
    "Creaci√≥n del archivo `submission.csv` para env√≠o a Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:06:11.122392Z",
     "iopub.status.busy": "2025-11-04T06:06:11.121642Z",
     "iopub.status.idle": "2025-11-04T06:06:11.156547Z",
     "shell.execute_reply": "2025-11-04T06:06:11.155676Z",
     "shell.execute_reply.started": "2025-11-04T06:06:11.122365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Crear submission file\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUBMISSION FILE CREATED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(submission):,}\")\n",
    "print(f\"File: submission.csv\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T06:06:11.157716Z",
     "iopub.status.busy": "2025-11-04T06:06:11.15742Z",
     "iopub.status.idle": "2025-11-04T06:06:11.164994Z",
     "shell.execute_reply": "2025-11-04T06:06:11.164202Z",
     "shell.execute_reply.started": "2025-11-04T06:06:11.157696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Verificar submission\n",
    "print(\"Primeras predicciones:\")\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# An√°lisis Final y Conclusiones\n",
    "\n",
    "En esta secci√≥n se presentan las m√©tricas finales y una reflexi√≥n sobre el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de M√©tricas Finales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen de m√©tricas finales\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'M√©trica': ['MCC', 'Accuracy', 'Precision', 'Recall', 'Loss'],\n",
    "    'Training': [\n",
    "        'N/A',  # MCC no se calcula durante entrenamiento\n",
    "        f\"{history.history['accuracy'][-1]:.4f}\",\n",
    "        f\"{history.history['precision'][-1]:.4f}\",\n",
    "        f\"{history.history['recall'][-1]:.4f}\",\n",
    "        f\"{history.history['loss'][-1]:.4f}\"\n",
    "    ],\n",
    "    'Validation': [\n",
    "        f\"{mcc_score:.4f}\",\n",
    "        f\"{history.history['val_accuracy'][-1]:.4f}\",\n",
    "        f\"{history.history['val_precision'][-1]:.4f}\",\n",
    "        f\"{history.history['val_recall'][-1]:.4f}\",\n",
    "        f\"{history.history['val_loss'][-1]:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESUMEN DE M√âTRICAS FINALES - ITERACI√ìN 1\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_summary.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Matthews Correlation Coefficient (MCC): {mcc_score:.4f}\")\n",
    "print(f\"   ‚Üí Este es el score que se usar√° en Kaggle\")\n",
    "print(f\"\\nüéØ Score Esperado en Kaggle: {mcc_score:.4f} ¬± 0.02\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Visualizaci√≥n comparativa\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(['Accuracy', 'Precision', 'Recall']))\n",
    "width = 0.35\n",
    "\n",
    "train_vals = [\n",
    "    history.history['accuracy'][-1],\n",
    "    history.history['precision'][-1],\n",
    "    history.history['recall'][-1]\n",
    "]\n",
    "val_vals = [\n",
    "    history.history['val_accuracy'][-1],\n",
    "    history.history['val_precision'][-1],\n",
    "    history.history['val_recall'][-1]\n",
    "]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_vals, width, label='Training', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, val_vals, width, label='Validation', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('M√©trica', fontweight='bold')\n",
    "ax.set_ylabel('Score', fontweight='bold')\n",
    "ax.set_title(f'Comparaci√≥n M√©tricas - Train vs Validation\\nMCC Validation: {mcc_score:.4f}', \n",
    "             fontweight='bold', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Accuracy', 'Precision', 'Recall'])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1.1])\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir valores sobre las barras\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen de m√©tricas\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'M√©trica': [\n",
    "        'Matthews Correlation Coefficient',\n",
    "        'Validation Accuracy',\n",
    "        'Validation Precision',\n",
    "        'Validation Recall',\n",
    "        'Training Loss',\n",
    "        'Validation Loss',\n",
    "        'Total Parameters',\n",
    "        'Training Samples',\n",
    "        'Validation Samples',\n",
    "        'Test Samples'\n",
    "    ],\n",
    "    'Valor': [\n",
    "        f\"{mcc_score:.4f}\",\n",
    "        f\"{final_val_acc:.4f}\",\n",
    "        f\"{history.history['val_precision'][-1]:.4f}\",\n",
    "        f\"{history.history['val_recall'][-1]:.4f}\",\n",
    "        f\"{final_train_loss:.4f}\",\n",
    "        f\"{final_val_loss:.4f}\",\n",
    "        f\"{model.count_params():,}\",\n",
    "        f\"{len(X_train_final):,}\",\n",
    "        f\"{len(X_val):,}\",\n",
    "        f\"{len(test):,}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"M√âTRICAS FINALES DEL MODELO\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_summary.to_string(index=False))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflexi√≥n Final sobre el Modelo\n",
    "\n",
    "### Arquitectura Elegida\n",
    "\n",
    "El modelo baseline implementado utiliza una arquitectura **LSTM Bidireccional** con las siguientes caracter√≠sticas clave:\n",
    "\n",
    "1. **Embeddings**: Capa de embeddings de 100 dimensiones para representar palabras como vectores densos\n",
    "2. **Spatial Dropout**: Regularizaci√≥n espec√≠fica para embeddings (20%) que reduce overfitting en la capa de entrada\n",
    "3. **Bidirectional LSTM**: 128 unidades que capturan contexto tanto hacia adelante como hacia atr√°s en el texto\n",
    "4. **Global Max Pooling**: Extrae las caracter√≠sticas m√°s relevantes de la secuencia completa\n",
    "5. **Dense Layer**: 64 unidades con activaci√≥n ReLU y dropout del 50%\n",
    "6. **Output Layer**: Clasificaci√≥n binaria con sigmoid\n",
    "\n",
    "### Justificaci√≥n de las Decisiones\n",
    "\n",
    "- **LSTM Bidireccional**: El contexto en ambas direcciones es crucial para detectar SPAM, ya que palabras clave pueden aparecer al principio o al final del mensaje\n",
    "- **Dropout Alto (50%)**: Necesario para prevenir overfitting dado el tama√±o relativamente peque√±o del dataset\n",
    "- **Global Max Pooling**: M√°s efectivo que Average Pooling para detectar palabras clave espec√≠ficas de SPAM\n",
    "- **AdamW Optimizer**: Mejor generalizaci√≥n que Adam est√°ndar gracias al weight decay\n",
    "\n",
    "### An√°lisis de Overfitting/Underfitting\n",
    "\n",
    "Bas√°ndose en las curvas de aprendizaje:\n",
    "- Si la diferencia entre train loss y val loss es **< 0.1**: Modelo bien balanceado\n",
    "- Si val loss > train loss significativamente: Posible overfitting ‚Üí Aumentar dropout o reducir complejidad\n",
    "- Si ambas losses son altas: Underfitting ‚Üí Aumentar capacidad del modelo\n",
    "\n",
    "### Pr√≥ximas Iteraciones Sugeridas\n",
    "\n",
    "1. **Iteraci√≥n 2**: Probar embeddings pre-entrenados (GloVe o Word2Vec)\n",
    "2. **Iteraci√≥n 3**: Arquitectura h√≠brida CNN + LSTM para capturar n-gramas y secuencias\n",
    "3. **Iteraci√≥n 4**: Transformers ligeros (DistilBERT) para mejor comprensi√≥n contextual\n",
    "4. **Iteraci√≥n 5**: Ensemble de m√∫ltiples modelos para mejorar robustez\n",
    "\n",
    "### Expectativas de Score\n",
    "\n",
    "- **Validation MCC**: Indicador directo del score esperado en Kaggle\n",
    "- **Target**: > 0.85 MCC para estar en el top 25% de la competici√≥n\n",
    "- **Mejoras esperadas**: +0.05-0.10 MCC con embeddings pre-entrenados y optimizaci√≥n de hiperpar√°metros\n",
    "\n",
    "### Referencias\n",
    "\n",
    "- Keras LSTM Documentation: https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "- Understanding LSTM Networks: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Bidirectional RNN: https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "- Matthews Correlation Coefficient: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14338739,
     "sourceId": 119855,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
