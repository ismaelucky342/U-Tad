{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR AEC2                           ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        06/12/2025  -  14:10:30           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    06/12/2025  -  17:45:00           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e9e7e",
   "metadata": {},
   "source": [
    "# Iteración 4: Transfer Learning con DistilBERT\n",
    "**Estado: DEPRECATED**\n",
    "\n",
    "En esta iteración, he intentado utilizar un modelo pre-entrenado de última generación (DistilBERT) para ver si el conocimiento contextual profundo podía superar a mis modelos personalizados. He realizado un entrenamiento en dos fases: primero congelando la base y luego haciendo fine-tuning de las últimas capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f065e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo las bibliotecas necesarias para la manipulación de datos, visualización, aprendizaje automático y PNL.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizerd ha demostrado que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino los hiperparámetros para el modelo DistilBERT, el entrenamiento y el procesamiento de datos.\n",
    "MODEL_NAME = 'distilbert-base-uncased'  \n",
    "MAX_LEN = 128  \n",
    "BATCH_SIZE = 16  \n",
    "GRADIENT_ACCUMULATION_STEPS = 2  \n",
    "LEARNING_RATE = 2e-5  \n",
    "WARMUP_STEPS = 100  \n",
    "EPOCHS = 10  \n",
    "VALIDATION_SPLIT = 0.2\n",
    "CLASSIFIER_DROPOUT = 0.3  \n",
    "DENSE_UNITS = 128  \n",
    "L2_REG = 1e-4  \n",
    "FREEZE_BASE = True  \n",
    "UNFREEZE_LAST_N_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos de entrenamiento y muestro las primeras filas para verificar.\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializo el tokenizador de DistilBERT y pruebo con un ejemplo de texto.\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "sample_text = train['text'].iloc[0]\n",
    "encoded = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens=True,  \n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',  \n",
    "    truncation=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='tf'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino una función para tokenizar textos y preparo los datos de entrenamiento y validación.\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Tokeniza una lista de textos con el tokenizer de DistilBERT\n",
    "    Devuelve los input_ids y las attention_masks\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "X_train_ids, X_train_masks = tokenize_texts(X_train_text, tokenizer, MAX_LEN)\n",
    "X_train_ids_final, X_val_ids, X_train_masks_final, X_val_masks, y_train_final, y_val = train_test_split(\n",
    "    X_train_ids, X_train_masks, y_train,\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=seed,\n",
    "    stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c96b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino la clase del modelo DistilBertSpamClassifier y lo compilo.\n",
    "class DistilBertSpamClassifier(keras.Model):\n",
    "    def __init__(self, model_name, dense_units, dropout_rate, l2_reg, freeze_base=True):\n",
    "        super(DistilBertSpamClassifier, self).__init__()\n",
    "        self.distilbert = TFDistilBertModel.from_pretrained(model_name)\n",
    "        if freeze_base:\n",
    "            self.distilbert.trainable = False\n",
    "        self.pooling = GlobalAveragePooling1D()  \n",
    "        self.dense1 = Dense(\n",
    "            dense_units,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(l2_reg),  \n",
    "            name='dense_1'\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.output_layer = Dense(1, activation='sigmoid', name='output')\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = inputs\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            training=training\n",
    "        )\n",
    "        sequence_output = distilbert_output.last_hidden_state\n",
    "        pooled = self.pooling(sequence_output)\n",
    "        x = self.dense1(pooled)\n",
    "        x = self.dropout(x, training=training)\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "model = DistilBertSpamClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    dense_units=DENSE_UNITS,\n",
    "    dropout_rate=CLASSIFIER_DROPOUT,\n",
    "    l2_reg=L2_REG,\n",
    "    freeze_base=FREEZE_BASE\n",
    ")\n",
    "dummy_input_ids = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "dummy_attention_mask = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "_ = model([dummy_input_ids, dummy_attention_mask], training=False)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable_params = model.count_params() - trainable_params\n",
    "distilbert_layer = model.distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuro los callbacks y entreno la primera fase con la base congelada.\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_distilbert_spam_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "history_phase1 = model.fit(\n",
    "    [X_train_ids_final, X_train_masks_final],\n",
    "    y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=3,\n",
    "    validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b36a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelar las últimas capas de DistilBERT y entreno la segunda fase de fine-tuning.\n",
    "if FREEZE_BASE:\n",
    "    distilbert_layer.trainable = True\n",
    "    transformer_layers = distilbert_layer.distilbert.transformer.layer\n",
    "    total_transformer_layers = len(transformer_layers)\n",
    "    layers_to_freeze = max(0, total_transformer_layers - UNFREEZE_LAST_N_LAYERS)\n",
    "    distilbert_layer.distilbert.embeddings.trainable = False\n",
    "    for i, layer in enumerate(transformer_layers):\n",
    "        if i < layers_to_freeze:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    optimizer_phase2 = keras.optimizers.Adam(learning_rate=LEARNING_RATE / 10)\n",
    "    model.compile(\n",
    "        optimizer=optimizer_phase2,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    trainable_params_phase2 = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    history_phase2 = model.fit(\n",
    "        [X_train_ids_final, X_train_masks_final],\n",
    "        y_train_final,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        initial_epoch=3,  \n",
    "        validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    history = history_phase1\n",
    "    for key in history_phase2.history:\n",
    "        history.history[key].extend(history_phase2.history[key])\n",
    "else:\n",
    "    history = history_phase1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fa022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predigo en el conjunto de validación y genero la matriz de confusión.\n",
    "y_pred_proba = model.predict([X_val_ids, X_val_masks], batch_size=BATCH_SIZE)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'DistilBERT Confusion Matrix (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55512af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero las curvas de aprendizaje para analizar el proceso de fine-tuning de DistilBERT.\n",
    "def plot_learning_curves(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs_range = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo los datos de prueba, los tokenizo y hago predicciones.\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "X_test_text = test['text'].values\n",
    "X_test_ids, X_test_masks = tokenize_texts(X_test_text, tokenizer, MAX_LEN)\n",
    "y_pred_proba_test = model.predict([X_test_ids, X_test_masks], batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c583447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el archivo de submission con las predicciones.\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2ebbbe",
   "metadata": {},
   "source": [
    "## Análisis de la Iteración 4\n",
    "A pesar de la potencia teórica de DistilBERT, los resultados han sido decepcionantes en este caso (MCC significativamente más bajo). El modelo es muy pesado y el fine-tuning no ha convergido de forma estable con el tamaño de nuestro dataset. Además, el tiempo de entrenamiento es mucho mayor.\n",
    "\n",
    "**Conclusión:** Descarto el uso de Transformers para esta tarea específica y vuelvo a optimizar mi arquitectura híbrida CNN+LSTM, que ha demostrado ser más eficiente y efectiva."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
