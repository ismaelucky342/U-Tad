{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f065e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'  \n",
    "MAX_LEN = 128  \n",
    "BATCH_SIZE = 16  \n",
    "GRADIENT_ACCUMULATION_STEPS = 2  \n",
    "LEARNING_RATE = 2e-5  \n",
    "WARMUP_STEPS = 100  \n",
    "EPOCHS = 10  \n",
    "VALIDATION_SPLIT = 0.2\n",
    "CLASSIFIER_DROPOUT = 0.3  \n",
    "DENSE_UNITS = 128  \n",
    "L2_REG = 1e-4  \n",
    "FREEZE_BASE = True  \n",
    "UNFREEZE_LAST_N_LAYERS = 2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "sample_text = train['text'].iloc[0]\n",
    "encoded = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens=True,  \n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',  \n",
    "    truncation=True,  \n",
    "    return_attention_mask=True,  \n",
    "    return_tensors='tf'  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Tokeniza una lista de textos con el tokenizer de DistilBERT\n",
    "    Devuelve los input_ids y las attention_masks\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "X_train_ids, X_train_masks = tokenize_texts(X_train_text, tokenizer, MAX_LEN)\n",
    "X_train_ids_final, X_val_ids, X_train_masks_final, X_val_masks, y_train_final, y_val = train_test_split(\n",
    "    X_train_ids, X_train_masks, y_train,\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=seed,\n",
    "    stratify=y_train\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c96b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertSpamClassifier(keras.Model):\n",
    "    def __init__(self, model_name, dense_units, dropout_rate, l2_reg, freeze_base=True):\n",
    "        super(DistilBertSpamClassifier, self).__init__()\n",
    "        self.distilbert = TFDistilBertModel.from_pretrained(model_name)\n",
    "        if freeze_base:\n",
    "            self.distilbert.trainable = False\n",
    "        self.pooling = GlobalAveragePooling1D()  \n",
    "        self.dense1 = Dense(\n",
    "            dense_units,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(l2_reg),  \n",
    "            name='dense_1'\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.output_layer = Dense(1, activation='sigmoid', name='output')\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = inputs\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            training=training\n",
    "        )\n",
    "        sequence_output = distilbert_output.last_hidden_state\n",
    "        pooled = self.pooling(sequence_output)\n",
    "        x = self.dense1(pooled)\n",
    "        x = self.dropout(x, training=training)\n",
    "        output = self.output_layer(x)\n",
    "        return output\n",
    "model = DistilBertSpamClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    dense_units=DENSE_UNITS,\n",
    "    dropout_rate=CLASSIFIER_DROPOUT,\n",
    "    l2_reg=L2_REG,\n",
    "    freeze_base=FREEZE_BASE\n",
    ")\n",
    "dummy_input_ids = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "dummy_attention_mask = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "_ = model([dummy_input_ids, dummy_attention_mask], training=False)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "model.summary()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable_params = model.count_params() - trainable_params\n",
    "distilbert_layer = model.distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_distilbert_spam_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "history_phase1 = model.fit(\n",
    "    [X_train_ids_final, X_train_masks_final],\n",
    "    y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=3,\n",
    "    validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b36a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FREEZE_BASE:\n",
    "    distilbert_layer.trainable = True\n",
    "    transformer_layers = distilbert_layer.distilbert.transformer.layer\n",
    "    total_transformer_layers = len(transformer_layers)\n",
    "    layers_to_freeze = max(0, total_transformer_layers - UNFREEZE_LAST_N_LAYERS)\n",
    "    distilbert_layer.distilbert.embeddings.trainable = False\n",
    "    for i, layer in enumerate(transformer_layers):\n",
    "        if i < layers_to_freeze:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    optimizer_phase2 = keras.optimizers.Adam(learning_rate=LEARNING_RATE / 10)\n",
    "    model.compile(\n",
    "        optimizer=optimizer_phase2,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    trainable_params_phase2 = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    history_phase2 = model.fit(\n",
    "        [X_train_ids_final, X_train_masks_final],\n",
    "        y_train_final,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        initial_epoch=3,  \n",
    "        validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    history = history_phase1\n",
    "    for key in history_phase2.history:\n",
    "        history.history[key].extend(history_phase2.history[key])\n",
    "else:\n",
    "    history = history_phase1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fa022",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict([X_val_ids, X_val_masks], batch_size=BATCH_SIZE)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'DistilBERT Confusion Matrix (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "overfitting_delta = abs(final_val_loss - final_train_loss)\n",
    "comparison_data = {\n",
    "    'V1 LSTM': {'mcc': 0.8665, 'arch': 'Bi-LSTM(128)', 'params': '1.25M'},\n",
    "    'V2 LSTM': {'mcc': 0.8885, 'arch': 'Bi-LSTM(96)+L2', 'params': '1.16M'},\n",
    "    'V3 LSTM': {'mcc': 0.8733, 'arch': 'Bi-LSTM(64)+L2x5', 'params': '~1.0M'},\n",
    "    'V4 DistilBERT': {'mcc': mcc_score, 'arch': 'DistilBERT+FT', 'params': f'{model.count_params()/1e6:.1f}M'}\n",
    "}\n",
    "for version, data in comparison_data.items():\n",
    "best_lstm = 0.8885  \n",
    "improvement = mcc_score - best_lstm\n",
    "if mcc_score > 0.90:\n",
    "elif mcc_score > best_lstm:\n",
    "elif mcc_score > best_lstm - 0.01:\n",
    "else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "X_test_text = test['text'].values\n",
    "X_test_ids, X_test_masks = tokenize_texts(X_test_text, tokenizer, MAX_LEN)\n",
    "y_pred_proba_test = model.predict([X_test_ids, X_test_masks], batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c583447",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame({\n",
    "    'Iteración': ['V1', 'V2', 'V3', 'V4'],\n",
    "    'Arquitectura': ['Bi-LSTM(128)', 'Bi-LSTM(96)+L2', 'Bi-LSTM(64)+L2x5', 'DistilBERT+FT'],\n",
    "    'Val MCC': [0.8665, 0.8885, 0.8733, f'{mcc_score:.4f}'],\n",
    "    'Parámetros': ['1.25M', '1.16M', '~1.0M', f'{model.count_params()/1e6:.1f}M'],\n",
    "    'Trainable': ['1.25M', '1.16M', '~1.0M', f'{trainable_params/1e6:.1f}M'],\n",
    "    'Approach': ['Baseline', 'Regularization', 'Shock Therapy', 'Transfer Learning']\n",
    "})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
