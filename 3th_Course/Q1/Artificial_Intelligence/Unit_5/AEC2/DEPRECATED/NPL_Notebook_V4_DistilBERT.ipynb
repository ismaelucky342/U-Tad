{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f065e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta fue mi iteracion 4: Probé con DistilBERT (Transfer Learning)\n",
    "# Spoiler: Fue un desastre total, pero aprendí mucho del fracaso\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd  # Para manejar los CSV\n",
    "import numpy as np  # Operaciones numericas\n",
    "import matplotlib.pyplot as plt  # Graficos bonitos\n",
    "import seaborn as sns  # Graficos mas bonitos\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Estos warnings no aportan nada util\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMPORTING TENSORFLOW, KERAS AND TRANSFORMERS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# NUEVO: Transformers library - Aqui es donde viene DistilBERT\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT SETUP - SUCCESS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers available: True\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPU Devices: {len(gpu_devices)} device(s)\")\n",
    "    for gpu in gpu_devices:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "print(f\"Random seed: {seed}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036be9f",
   "metadata": {},
   "source": [
    "## Iteración 4 - Mi intento con Transfer Learning (DistilBERT)\n",
    "\n",
    "**Mi razonamiento:**\n",
    "Después de 3 iteraciones con LSTM, pensé: \"¿Y si uso un modelo pre-entrenado tipo BERT?\"\n",
    "DistilBERT parecía perfecto - más ligero que BERT completo pero igual de potente.\n",
    "\n",
    "**¿Por qué DistilBERT?**\n",
    "- Tiene 66 millones de parámetros ya entrenados en millones de textos\n",
    "- Entiende el contexto de las palabras (no como mis embeddings básicos)\n",
    "- 40% más rápido que BERT completo\n",
    "- Todo el mundo dice que Transfer Learning funciona genial\n",
    "\n",
    "**Mi plan:**\n",
    "1. Congelar la base de DistilBERT (para no romper el pre-training)\n",
    "2. Solo entrenar un clasificador pequeño encima\n",
    "3. Después descongelar las últimas 2 capas para fine-tuning\n",
    "4. Learning rate bajito (2e-5) para no liarnos\n",
    "\n",
    "**Objetivo:** Superar el 0.8885 de V2 y llegar a MCC > 0.90\n",
    "\n",
    "Spoiler: No funcionó como esperaba. MCC público = 0.6456 (un desastre)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuracion de hyperparametros para DistilBERT\n",
    "# Estos valores los copié de papers y tutoriales de Hugging Face\n",
    "\n",
    "# DistilBERT Configuration\n",
    "MODEL_NAME = 'distilbert-base-uncased'  # El modelo base en ingles\n",
    "MAX_LEN = 128  # BERT funciona mejor con 128 que con 200 (lee en los docs)\n",
    "BATCH_SIZE = 16  # Lo bajé porque DistilBERT consume mucha memoria\n",
    "GRADIENT_ACCUMULATION_STEPS = 2  # Truco: simulo batch de 32 con esto\n",
    "\n",
    "# Fine-tuning Configuration\n",
    "LEARNING_RATE = 2e-5  # Learning rate estandar para fine-tuning BERT\n",
    "WARMUP_STEPS = 100  # Para que el modelo \"caliente\" antes de entrenar fuerte\n",
    "EPOCHS = 10  # Menos que LSTM porque converge mas rapido\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Classifier Configuration\n",
    "CLASSIFIER_DROPOUT = 0.3  # Dropout en mi clasificador custom\n",
    "DENSE_UNITS = 128  # Una capa intermedia de 128 neuronas\n",
    "L2_REG = 1e-4  # Regularizacion L2 ligera\n",
    "\n",
    "# Layer Freezing Strategy\n",
    "FREEZE_BASE = True  # Empiezo congelando todo DistilBERT\n",
    "UNFREEZE_LAST_N_LAYERS = 2  # Despues descongelaré las ultimas 2 capas\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL CONFIGURATION - ITERACIÓN 4\")\n",
    "print(\"TRANSFER LEARNING - DISTILBERT\")\n",
    "print(\"=\"*60)\n",
    "print(\"CAMBIO ARQUITECTURAL RADICAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base Model: {MODEL_NAME}\")\n",
    "print(f\"  - Pre-trained parameters: ~66M\")\n",
    "print(f\"  - Transformer layers: 6\")\n",
    "print(f\"  - Attention heads: 12\")\n",
    "print(f\"  - Hidden size: 768\")\n",
    "print(\"=\"*60)\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Max Length: {MAX_LEN} (optimal for BERT)\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE} (physical)\")\n",
    "print(f\"  Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS} steps\")\n",
    "print(f\"  Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE} (fine-tuning)\")\n",
    "print(f\"  Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"  Max Epochs: {EPOCHS}\")\n",
    "print(f\"  Classifier Dropout: {CLASSIFIER_DROPOUT}\")\n",
    "print(f\"  Dense Units: {DENSE_UNITS}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Fine-tuning Strategy:\")\n",
    "print(f\"  Freeze base: {FREEZE_BASE}\")\n",
    "print(f\"  Unfreeze last N layers: {UNFREEZE_LAST_N_LAYERS}\")\n",
    "print(f\"  Total trainable: Classifier + Last {UNFREEZE_LAST_N_LAYERS} transformer layers\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEsperado:\")\n",
    "print(\"  - MCC > 0.90 (vs 0.8885 de V2)\")\n",
    "print(\"  - Mejor comprensión semántica\")\n",
    "print(\"  - Menos overfitting (pre-training robusto)\")\n",
    "print(\"  - Tiempo: ~5-8 min (transformers más lentos)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdaa9c",
   "metadata": {},
   "source": [
    "## Carga de datos (igual que siempre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el CSV de entrenamiento (lo de siempre)\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DATA OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(train):,}\")\n",
    "print(f\"\\nClass distribution:\\n{train['spam_label'].value_counts()}\")\n",
    "print(f\"\\nClass balance:\\n{train['spam_label'].value_counts(normalize=True)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8346a9",
   "metadata": {},
   "source": [
    "## Tokenización con DistilBERT (aquí cambia todo)\n",
    "\n",
    "**Diferencia clave con mis versiones LSTM:**\n",
    "- Antes: Tokenizer de Keras (básico, solo divide palabras)\n",
    "- Ahora: Tokenizer de DistilBERT (WordPiece - divide en sub-palabras)\n",
    "\n",
    "El tokenizer de DistilBERT viene con su propio vocabulario de 30,522 tokens ya aprendidos.\n",
    "Añade tokens especiales como [CLS] al inicio, [SEP] al final, y [PAD] para rellenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el tokenizer de DistilBERT directamente de Hugging Face\n",
    "print(\"Cargando tokenizer DistilBERT...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DISTILBERT TOKENIZER LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max length: {MAX_LEN}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pruebo con un texto de ejemplo para ver como tokeniza\n",
    "sample_text = train['text'].iloc[0]\n",
    "print(f\"\\nEjemplo tokenización:\")\n",
    "print(f\"Original: {sample_text[:100]}...\")\n",
    "encoded = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens=True,  # Añade [CLS] y [SEP]\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',  # Rellena con [PAD] hasta MAX_LEN\n",
    "    truncation=True,  # Corta si es mas largo\n",
    "    return_attention_mask=True,  # Mascara para saber que es padding\n",
    "    return_tensors='tf'  # Devuelve tensores de TensorFlow\n",
    ")\n",
    "print(f\"Tokens: {encoded['input_ids'].shape}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask'].shape}\")\n",
    "print(f\"\\nPrimeros 10 tokens: {tokenizer.convert_ids_to_tokens(encoded['input_ids'][0][:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para tokenizar todos los textos de una vez\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Tokeniza una lista de textos con el tokenizer de DistilBERT\n",
    "    Devuelve los input_ids y las attention_masks\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "# Tokenizo todo el dataset de entrenamiento\n",
    "print(\"Tokenizando textos con DistilBERT...\")\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "X_train_ids, X_train_masks = tokenize_texts(X_train_text, tokenizer, MAX_LEN)\n",
    "\n",
    "# Divido en train y validation\n",
    "X_train_ids_final, X_val_ids, X_train_masks_final, X_val_masks, y_train_final, y_val = train_test_split(\n",
    "    X_train_ids, X_train_masks, y_train,\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=seed,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOKENIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(X_train_ids_final):,}\")\n",
    "print(f\"Validation samples: {len(X_val_ids):,}\")\n",
    "print(f\"Input shape: {X_train_ids_final.shape}\")\n",
    "print(f\"Attention mask shape: {X_train_masks_final.shape}\")\n",
    "print(f\"Train class distribution: {np.bincount(y_train_final)}\")\n",
    "print(f\"Val class distribution: {np.bincount(y_val)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc8b43",
   "metadata": {},
   "source": [
    "## Mi modelo DistilBERT custom\n",
    "\n",
    "**La idea:**\n",
    "Uso DistilBERT como base (6 capas transformer con 66M parámetros) y le pongo encima mi propio clasificador.\n",
    "\n",
    "**Arquitectura:**\n",
    "1. DistilBERT base (congelado al principio)\n",
    "2. Global Average Pooling (para resumir la secuencia)\n",
    "3. Dense(128) con ReLU y regularización L2\n",
    "4. Dropout(0.3)\n",
    "5. Dense(1) con sigmoid → probabilidad de SPAM\n",
    "\n",
    "**Estrategia de fine-tuning:**\n",
    "- Fase 1: Solo entreno mi clasificador (3 epochs)\n",
    "- Fase 2: Descongeló las últimas 2 capas de DistilBERT y entreno todo (resto de epochs)\n",
    "\n",
    "Esto evita \"catastrofic forgetting\" - no quiero romper lo que DistilBERT ya aprendió."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c96b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo mi modelo usando Model Subclassing (mas flexible para esto)\n",
    "class DistilBertSpamClassifier(keras.Model):\n",
    "    def __init__(self, model_name, dense_units, dropout_rate, l2_reg, freeze_base=True):\n",
    "        super(DistilBertSpamClassifier, self).__init__()\n",
    "        \n",
    "        # Cargo DistilBERT pre-entrenado desde Hugging Face\n",
    "        self.distilbert = TFDistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Lo congelo si freeze_base=True\n",
    "        if freeze_base:\n",
    "            self.distilbert.trainable = False\n",
    "            print(\"Base DistilBERT congelado. Se entrenará solo el clasificador.\")\n",
    "        \n",
    "        # Mi clasificador custom encima de DistilBERT\n",
    "        self.pooling = GlobalAveragePooling1D()  # Resume la secuencia\n",
    "        self.dense1 = Dense(\n",
    "            dense_units,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(l2_reg),  # L2 para no overfittear\n",
    "            name='dense_1'\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.output_layer = Dense(1, activation='sigmoid', name='output')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = inputs\n",
    "        \n",
    "        # Paso por DistilBERT\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        # Agarro los hidden states de la ultima capa\n",
    "        sequence_output = distilbert_output.last_hidden_state\n",
    "        \n",
    "        # Pooling para reducir dimensiones\n",
    "        pooled = self.pooling(sequence_output)\n",
    "        \n",
    "        # Mi clasificador\n",
    "        x = self.dense1(pooled)\n",
    "        x = self.dropout(x, training=training)\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Construyo el modelo\n",
    "print(\"Construyendo modelo DistilBERT...\")\n",
    "model = DistilBertSpamClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    dense_units=DENSE_UNITS,\n",
    "    dropout_rate=CLASSIFIER_DROPOUT,\n",
    "    l2_reg=L2_REG,\n",
    "    freeze_base=FREEZE_BASE\n",
    ")\n",
    "\n",
    "# Build con inputs dummy para inicializar los pesos\n",
    "dummy_input_ids = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "dummy_attention_mask = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "_ = model([dummy_input_ids, dummy_attention_mask], training=False)\n",
    "\n",
    "print(\"Modelo construido exitosamente.\")\n",
    "\n",
    "# Compilo con Adam y learning rate bajito\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Veo el resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISTILBERT MODEL COMPILED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Cuento trainable vs non-trainable\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable_params = model.count_params() - trainable_params\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "print(f\"\\nComparación con versiones anteriores:\")\n",
    "print(f\"  V1 LSTM: 1,251,009 params (100% trainable)\")\n",
    "print(f\"  V2 LSTM: 1,160,609 params (100% trainable)\")\n",
    "print(f\"  V3 LSTM: ~1,000,000 params (100% trainable)\")\n",
    "print(f\"  V4 DistilBERT: {model.count_params():,} params ({trainable_params:,} trainable)\")\n",
    "print(f\"\\nOptimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"Loss: Binary Crossentropy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Guardo referencia para descongelar despues\n",
    "distilbert_layer = model.distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77bd98",
   "metadata": {},
   "source": [
    "## Fase 1 de entrenamiento: Solo el clasificador\n",
    "\n",
    "Primero entreno solo mi clasificador (las capas Dense) con DistilBERT congelado.\n",
    "Esto deja que el clasificador aprenda sin romper los pesos pre-entrenados.\n",
    "\n",
    "Hago 3 epochs así antes de descongelar capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks (los mismos de siempre)\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_distilbert_spam_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FASE 1: ENTRENAMIENTO CLASIFICADOR\")\n",
    "print(\"=\"*60)\n",
    "print(\"Base DistilBERT: CONGELADO\")\n",
    "print(\"Clasificador: ENTRENABLE\")\n",
    "print(\"Epochs: 3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Entreno solo 3 epochs con la base congelada\n",
    "history_phase1 = model.fit(\n",
    "    [X_train_ids_final, X_train_masks_final],\n",
    "    y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=3,\n",
    "    validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FASE 1 COMPLETADA\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba52b1",
   "metadata": {},
   "source": [
    "## Fase 2: Descongelar últimas capas y fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b36a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora descongeló las últimas 2 capas de DistilBERT para fine-tuning\n",
    "print(\"=\"*60)\n",
    "print(\"FASE 2: FINE-TUNING ÚLTIMAS CAPAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if FREEZE_BASE:\n",
    "    # Descongeló todo DistilBERT primero\n",
    "    distilbert_layer.trainable = True\n",
    "    \n",
    "    # Accedo a las capas transformer internas\n",
    "    # DistilBERT tiene 6 capas transformer\n",
    "    transformer_layers = distilbert_layer.distilbert.transformer.layer\n",
    "    total_transformer_layers = len(transformer_layers)\n",
    "    \n",
    "    # Calculo cuantas capas congelar\n",
    "    layers_to_freeze = max(0, total_transformer_layers - UNFREEZE_LAST_N_LAYERS)\n",
    "    \n",
    "    # Congelo embeddings (estos no los toco nunca)\n",
    "    distilbert_layer.distilbert.embeddings.trainable = False\n",
    "    \n",
    "    # Congelo las primeras capas transformer, dejo entrenables las ultimas N\n",
    "    for i, layer in enumerate(transformer_layers):\n",
    "        if i < layers_to_freeze:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    print(f\"Total transformer layers: {total_transformer_layers}\")\n",
    "    print(f\"Transformer layers congeladas: {layers_to_freeze}\")\n",
    "    print(f\"Transformer layers entrenables: {UNFREEZE_LAST_N_LAYERS}\")\n",
    "    print(f\"Embeddings: CONGELADOS\")\n",
    "    \n",
    "    # Re-compilo con learning rate más bajo (para no romper el fine-tuning)\n",
    "    optimizer_phase2 = keras.optimizers.Adam(learning_rate=LEARNING_RATE / 10)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer_phase2,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    trainable_params_phase2 = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"\\nTrainable parameters fase 2: {trainable_params_phase2:,}\")\n",
    "    print(f\"Learning rate: {LEARNING_RATE / 10}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Entreno el resto de epochs con fine-tuning\n",
    "    history_phase2 = model.fit(\n",
    "        [X_train_ids_final, X_train_masks_final],\n",
    "        y_train_final,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        initial_epoch=3,  # Continuo desde epoch 3\n",
    "        validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FASE 2 COMPLETADA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Combino los historiales de ambas fases\n",
    "    history = history_phase1\n",
    "    for key in history_phase2.history:\n",
    "        history.history[key].extend(history_phase2.history[key])\n",
    "else:\n",
    "    history = history_phase1\n",
    "\n",
    "print(\"\\nENTRENAMIENTO COMPLETO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29ea18",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fa022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predigo en validación para ver como fue\n",
    "y_pred_proba = model.predict([X_val_ids, X_val_masks], batch_size=BATCH_SIZE)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION METRICS - DISTILBERT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc_score:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Matriz de confusion para visualizar\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'DistilBERT Confusion Matrix (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b672",
   "metadata": {},
   "source": [
    "## Comparativa: LSTM vs DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparo con mis versiones anteriores\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "overfitting_delta = abs(final_val_loss - final_train_loss)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS COMPARATIVO - LSTM vs DISTILBERT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = {\n",
    "    'V1 LSTM': {'mcc': 0.8665, 'arch': 'Bi-LSTM(128)', 'params': '1.25M'},\n",
    "    'V2 LSTM': {'mcc': 0.8885, 'arch': 'Bi-LSTM(96)+L2', 'params': '1.16M'},\n",
    "    'V3 LSTM': {'mcc': 0.8733, 'arch': 'Bi-LSTM(64)+L2x5', 'params': '~1.0M'},\n",
    "    'V4 DistilBERT': {'mcc': mcc_score, 'arch': 'DistilBERT+FT', 'params': f'{model.count_params()/1e6:.1f}M'}\n",
    "}\n",
    "\n",
    "print(\"\\nEvolucion del MCC:\")\n",
    "for version, data in comparison_data.items():\n",
    "    print(f\"  {version}: {data['mcc']:.4f} | {data['arch']} | {data['params']} params\")\n",
    "\n",
    "best_lstm = 0.8885  # Mi V2 sigue siendo el mejor\n",
    "improvement = mcc_score - best_lstm\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADO TRANSFER LEARNING:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"MCC DistilBERT: {mcc_score:.4f}\")\n",
    "print(f\"Best LSTM (V2): {best_lstm:.4f}\")\n",
    "print(f\"Diferencia: {'+' if improvement > 0 else ''}{improvement:.4f} ({improvement/best_lstm*100:+.1f}%)\")\n",
    "print(f\"\\nOverfitting Delta: {overfitting_delta:.4f}\")\n",
    "print(f\"Val Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "if mcc_score > 0.90:\n",
    "    print(\"\\nOBJETIVO CUMPLIDO: MCC > 0.90\")\n",
    "elif mcc_score > best_lstm:\n",
    "    print(\"\\nMEJORA CONFIRMADA: Transfer learning funciono!\")\n",
    "elif mcc_score > best_lstm - 0.01:\n",
    "    print(\"\\nRESULTADO SIMILAR: Transfer learning competitivo con LSTM\")\n",
    "else:\n",
    "    print(\"\\nATENCIÓN: LSTM V2 sigue siendo mejor\")\n",
    "    print(\"DistilBERT no funcionó como esperaba\")\n",
    "    print(\"Posibles causas:\")\n",
    "    print(\"  - Fine-tuning insuficiente o mal ajustado\")\n",
    "    print(\"  - Hyperparameters no optimizados para este dataset\")\n",
    "    print(\"  - Dataset demasiado pequeño para transfer learning\")\n",
    "    print(\"  - LSTM más adecuado para textos cortos tipo SMS\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3337f",
   "metadata": {},
   "source": [
    "## Predicciones en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo el test data\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total test samples: {len(test):,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenizo el test igual que el train\n",
    "print(\"\\nTokenizando test data con DistilBERT...\")\n",
    "X_test_text = test['text'].values\n",
    "X_test_ids, X_test_masks = tokenize_texts(X_test_text, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"Test shape: {X_test_ids.shape}\")\n",
    "\n",
    "# Genero predicciones\n",
    "print(\"\\nGenerando predicciones...\")\n",
    "y_pred_proba_test = model.predict([X_test_ids, X_test_masks], batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"\\nPredicciones generadas: {len(y_pred_test):,}\")\n",
    "print(f\"Distribución:\")\n",
    "print(f\"  Not SPAM: {np.sum(y_pred_test == 0):,} ({np.mean(y_pred_test == 0)*100:.2f}%)\")\n",
    "print(f\"  SPAM: {np.sum(y_pred_test == 1):,} ({np.mean(y_pred_test == 1)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e647004",
   "metadata": {},
   "source": [
    "## Submission para Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c583447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el submission file\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUBMISSION FILE CREATED - DISTILBERT V4\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(submission):,}\")\n",
    "print(f\"File: submission.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b496a",
   "metadata": {},
   "source": [
    "## Resumen final de esta iteración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen de todas las iteraciones\n",
    "summary_df = pd.DataFrame({\n",
    "    'Iteración': ['V1', 'V2', 'V3', 'V4'],\n",
    "    'Arquitectura': ['Bi-LSTM(128)', 'Bi-LSTM(96)+L2', 'Bi-LSTM(64)+L2x5', 'DistilBERT+FT'],\n",
    "    'Val MCC': [0.8665, 0.8885, 0.8733, f'{mcc_score:.4f}'],\n",
    "    'Parámetros': ['1.25M', '1.16M', '~1.0M', f'{model.count_params()/1e6:.1f}M'],\n",
    "    'Trainable': ['1.25M', '1.16M', '~1.0M', f'{trainable_params/1e6:.1f}M'],\n",
    "    'Approach': ['Baseline', 'Regularization', 'Shock Therapy', 'Transfer Learning']\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"RESUMEN FINAL - TODAS LAS ITERACIONES\")\n",
    "print(\"=\"*90)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nMIS CONCLUSIONES DE V4:\")\n",
    "print(f\"  - MCC validation: {mcc_score:.4f}\")\n",
    "print(f\"  - MCC público Kaggle: 0.6456 (UN DESASTRE)\")\n",
    "print(f\"  - Mejor LSTM (V2): 0.8885\")\n",
    "print(\"\\n¿QUÉ SALIÓ MAL?\")\n",
    "print(\"  - Transfer learning no funcionó para este dataset\")\n",
    "print(\"  - Probablemente DistilBERT es overkill para textos cortos tipo SMS\")\n",
    "print(\"  - LSTM es más ligero y eficiente para este caso\")\n",
    "print(\"  - El fine-tuning puede haber roto el pre-training\")\n",
    "print(\"\\nLECCIONES APRENDIDAS:\")\n",
    "print(\"  - No siempre un modelo más complejo es mejor\")\n",
    "print(\"  - Transfer learning funciona bien con textos largos, no con SMS\")\n",
    "print(\"  - A veces lo simple (LSTM) gana a lo complejo (Transformers)\")\n",
    "print(\"  - Dataset pequeño → modelo simple funciona mejor\")\n",
    "print(\"\\nPRÓXIMOS PASOS:\")\n",
    "print(\"  - Volver a arquitecturas LSTM pero explorar híbridos\")\n",
    "print(\"  - Probar CNN + LSTM\")\n",
    "print(\"  - Enfocarme en regularización, no en complejidad\")\n",
    "print(\"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
