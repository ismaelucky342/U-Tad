{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL FIX: Must be executed FIRST\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "print(\"Environment variables set successfully\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f065e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMPORTING TENSORFLOW, KERAS AND TRANSFORMERS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling1D, Layer\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "# NUEVO: Transformers library\n",
    "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT SETUP - SUCCESS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers available: True\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPU Devices: {len(gpu_devices)} device(s)\")\n",
    "    for gpu in gpu_devices:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "print(f\"Random seed: {seed}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036be9f",
   "metadata": {},
   "source": [
    "## Iteración 4 - TRANSFER LEARNING CON DISTILBERT\n",
    "\n",
    "**Evolución Arquitecturas:**\n",
    "- V1-V3: LSTM Bidireccional (embeddings entrenables desde cero)\n",
    "- V4: DistilBERT (66M parámetros pre-entrenados)\n",
    "\n",
    "**Ventajas DistilBERT:**\n",
    "1. Embeddings contextuales (entiende significado según contexto)\n",
    "2. Pre-entrenado en millones de textos\n",
    "3. Captura relaciones complejas entre palabras\n",
    "4. Mejor comprensión semántica para SPAM\n",
    "5. 40% más rápido que BERT completo\n",
    "\n",
    "**Estrategia Fine-tuning:**\n",
    "- Congelar capas base (primeras 4/6)\n",
    "- Fine-tune últimas 2 capas\n",
    "- Clasificador custom con dropout\n",
    "- Learning rate bajo (2e-5)\n",
    "- Gradient accumulation para batch efectivo mayor\n",
    "\n",
    "**Objetivo:** MCC > 0.90 (salto significativo vs 0.8885 de V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af86bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Configuration - ITERACIÓN 4 - TRANSFER LEARNING\n",
    "\n",
    "# DistilBERT Configuration\n",
    "MODEL_NAME = 'distilbert-base-uncased'  # Modelo pre-entrenado\n",
    "MAX_LEN = 128                           # BERT optimal length (vs 200 de LSTM)\n",
    "BATCH_SIZE = 16                         # Reducido por memoria (vs 32)\n",
    "GRADIENT_ACCUMULATION_STEPS = 2         # Batch efectivo = 16*2 = 32\n",
    "\n",
    "# Fine-tuning Configuration\n",
    "LEARNING_RATE = 2e-5                    # Standard fine-tuning LR\n",
    "WARMUP_STEPS = 100                      # Warmup para estabilidad\n",
    "EPOCHS = 10                             # Menos epochs que LSTM (converge rápido)\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# Classifier Configuration\n",
    "CLASSIFIER_DROPOUT = 0.3                # Dropout en clasificador custom\n",
    "DENSE_UNITS = 128                       # Unidades capa intermedia\n",
    "L2_REG = 1e-4                          # Regularización L2\n",
    "\n",
    "# Layer Freezing Strategy\n",
    "FREEZE_BASE = True                      # Congelar base inicialmente\n",
    "UNFREEZE_LAST_N_LAYERS = 2             # Descongelar últimas 2 capas\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL CONFIGURATION - ITERACIÓN 4\")\n",
    "print(\"TRANSFER LEARNING - DISTILBERT\")\n",
    "print(\"=\"*60)\n",
    "print(\"CAMBIO ARQUITECTURAL RADICAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base Model: {MODEL_NAME}\")\n",
    "print(f\"  - Pre-trained parameters: ~66M\")\n",
    "print(f\"  - Transformer layers: 6\")\n",
    "print(f\"  - Attention heads: 12\")\n",
    "print(f\"  - Hidden size: 768\")\n",
    "print(\"=\"*60)\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Max Length: {MAX_LEN} (optimal for BERT)\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE} (physical)\")\n",
    "print(f\"  Gradient Accumulation: {GRADIENT_ACCUMULATION_STEPS} steps\")\n",
    "print(f\"  Effective Batch Size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE} (fine-tuning)\")\n",
    "print(f\"  Warmup Steps: {WARMUP_STEPS}\")\n",
    "print(f\"  Max Epochs: {EPOCHS}\")\n",
    "print(f\"  Classifier Dropout: {CLASSIFIER_DROPOUT}\")\n",
    "print(f\"  Dense Units: {DENSE_UNITS}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Fine-tuning Strategy:\")\n",
    "print(f\"  Freeze base: {FREEZE_BASE}\")\n",
    "print(f\"  Unfreeze last N layers: {UNFREEZE_LAST_N_LAYERS}\")\n",
    "print(f\"  Total trainable: Classifier + Last {UNFREEZE_LAST_N_LAYERS} transformer layers\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nEsperado:\")\n",
    "print(\"  - MCC > 0.90 (vs 0.8885 de V2)\")\n",
    "print(\"  - Mejor comprensión semántica\")\n",
    "print(\"  - Menos overfitting (pre-training robusto)\")\n",
    "print(\"  - Tiempo: ~5-8 min (transformers más lentos)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebdaa9c",
   "metadata": {},
   "source": [
    "## Carga y Exploración de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301afea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DATA OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(train):,}\")\n",
    "print(f\"\\nClass distribution:\\n{train['spam_label'].value_counts()}\")\n",
    "print(f\"\\nClass balance:\\n{train['spam_label'].value_counts(normalize=True)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8346a9",
   "metadata": {},
   "source": [
    "## Tokenización con DistilBERT\n",
    "\n",
    "**Diferencias vs Custom Tokenizer:**\n",
    "- WordPiece tokenization (subpalabras)\n",
    "- Vocabulario pre-entrenado 30,522 tokens\n",
    "- Tokens especiales: [CLS], [SEP], [PAD]\n",
    "- Attention masks para padding\n",
    "- Compatible con embeddings DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar tokenizer DistilBERT\n",
    "print(\"Cargando tokenizer DistilBERT...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DISTILBERT TOKENIZER LOADED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max length: {MAX_LEN}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ejemplo de tokenización\n",
    "sample_text = train['text'].iloc[0]\n",
    "print(f\"\\nEjemplo tokenización:\")\n",
    "print(f\"Original: {sample_text[:100]}...\")\n",
    "encoded = tokenizer.encode_plus(\n",
    "    sample_text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=MAX_LEN,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "print(f\"Tokens: {encoded['input_ids'].shape}\")\n",
    "print(f\"Attention mask: {encoded['attention_mask'].shape}\")\n",
    "print(f\"\\nPrimeros 10 tokens: {tokenizer.convert_ids_to_tokens(encoded['input_ids'][0][:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60b5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar dataset completo\n",
    "def tokenize_texts(texts, tokenizer, max_len):\n",
    "    \"\"\"\n",
    "    Tokeniza lista de textos usando DistilBERT tokenizer\n",
    "    Retorna input_ids y attention_masks\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "print(\"Tokenizando textos con DistilBERT...\")\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "X_train_ids, X_train_masks = tokenize_texts(X_train_text, tokenizer, MAX_LEN)\n",
    "\n",
    "# Split train/validation\n",
    "X_train_ids_final, X_val_ids, X_train_masks_final, X_val_masks, y_train_final, y_val = train_test_split(\n",
    "    X_train_ids, X_train_masks, y_train,\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=seed,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TOKENIZATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(X_train_ids_final):,}\")\n",
    "print(f\"Validation samples: {len(X_val_ids):,}\")\n",
    "print(f\"Input shape: {X_train_ids_final.shape}\")\n",
    "print(f\"Attention mask shape: {X_train_masks_final.shape}\")\n",
    "print(f\"Train class distribution: {np.bincount(y_train_final)}\")\n",
    "print(f\"Val class distribution: {np.bincount(y_val)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc8b43",
   "metadata": {},
   "source": [
    "## Construcción del Modelo - DistilBERT + Custom Classifier\n",
    "\n",
    "**Arquitectura:**\n",
    "```\n",
    "Input (text) → DistilBERT Tokenizer → [CLS] token\n",
    "    ↓\n",
    "DistilBERT Base (6 transformer layers)\n",
    "    ↓ (freeze first 4 layers)\n",
    "Trainable Transformer Layers (last 2)\n",
    "    ↓\n",
    "Global Average Pooling\n",
    "    ↓\n",
    "Dense(128, relu) + L2 + Dropout(0.3)\n",
    "    ↓\n",
    "Dense(1, sigmoid) → SPAM probability\n",
    "```\n",
    "\n",
    "**Fine-tuning Strategy:**\n",
    "- Fase 1: Solo entrenar clasificador (2-3 epochs)\n",
    "- Fase 2: Descongelar últimas capas (resto de epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c96b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo custom usando Model Subclassing\n",
    "class DistilBertSpamClassifier(keras.Model):\n",
    "    def __init__(self, model_name, dense_units, dropout_rate, l2_reg, freeze_base=True):\n",
    "        super(DistilBertSpamClassifier, self).__init__()\n",
    "        \n",
    "        # Cargar DistilBERT pre-entrenado\n",
    "        self.distilbert = TFDistilBertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Congelar si es necesario\n",
    "        if freeze_base:\n",
    "            self.distilbert.trainable = False\n",
    "            print(\"Base DistilBERT congelado. Se entrenará solo el clasificador.\")\n",
    "        \n",
    "        # Capas del clasificador\n",
    "        self.pooling = GlobalAveragePooling1D()\n",
    "        self.dense1 = Dense(\n",
    "            dense_units,\n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(l2_reg),\n",
    "            name='dense_1'\n",
    "        )\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.output_layer = Dense(1, activation='sigmoid', name='output')\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = inputs\n",
    "        \n",
    "        # Forward pass DistilBERT\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            training=training\n",
    "        )\n",
    "        \n",
    "        # Usar hidden states\n",
    "        sequence_output = distilbert_output.last_hidden_state\n",
    "        \n",
    "        # Pooling\n",
    "        pooled = self.pooling(sequence_output)\n",
    "        \n",
    "        # Clasificador\n",
    "        x = self.dense1(pooled)\n",
    "        x = self.dropout(x, training=training)\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Construir modelo\n",
    "print(\"Construyendo modelo DistilBERT...\")\n",
    "model = DistilBertSpamClassifier(\n",
    "    model_name=MODEL_NAME,\n",
    "    dense_units=DENSE_UNITS,\n",
    "    dropout_rate=CLASSIFIER_DROPOUT,\n",
    "    l2_reg=L2_REG,\n",
    "    freeze_base=FREEZE_BASE\n",
    ")\n",
    "\n",
    "# Build model con inputs dummy para inicializar pesos\n",
    "dummy_input_ids = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "dummy_attention_mask = tf.zeros((1, MAX_LEN), dtype=tf.int32)\n",
    "_ = model([dummy_input_ids, dummy_attention_mask], training=False)\n",
    "\n",
    "print(\"Modelo construido exitosamente.\")\n",
    "\n",
    "# Compilar\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Resumen\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISTILBERT MODEL COMPILED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "# Contar trainable params\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable_params = model.count_params() - trainable_params\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "print(f\"\\nComparación con versiones anteriores:\")\n",
    "print(f\"  V1 LSTM: 1,251,009 params (100% trainable)\")\n",
    "print(f\"  V2 LSTM: 1,160,609 params (100% trainable)\")\n",
    "print(f\"  V3 LSTM: ~1,000,000 params (100% trainable)\")\n",
    "print(f\"  V4 DistilBERT: {model.count_params():,} params ({trainable_params:,} trainable)\")\n",
    "print(f\"\\nOptimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"Loss: Binary Crossentropy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Guardar referencia para fine-tuning\n",
    "distilbert_layer = model.distilbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec77bd98",
   "metadata": {},
   "source": [
    "## Entrenamiento - Fase 1: Solo Clasificador\n",
    "\n",
    "**Estrategia 2 fases:**\n",
    "1. Entrenar solo clasificador (base congelado) - 3 epochs\n",
    "2. Descongelar últimas capas y fine-tune - resto epochs\n",
    "\n",
    "Esta estrategia evita catastrofic forgetting del pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3966d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_distilbert_spam_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FASE 1: ENTRENAMIENTO CLASIFICADOR\")\n",
    "print(\"=\"*60)\n",
    "print(\"Base DistilBERT: CONGELADO\")\n",
    "print(\"Clasificador: ENTRENABLE\")\n",
    "print(\"Epochs: 3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fase 1: Solo clasificador (3 epochs)\n",
    "history_phase1 = model.fit(\n",
    "    [X_train_ids_final, X_train_masks_final],\n",
    "    y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=3,\n",
    "    validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FASE 1 COMPLETADA\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba52b1",
   "metadata": {},
   "source": [
    "## Entrenamiento - Fase 2: Fine-tuning Últimas Capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b36a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelar últimas N capas de DistilBERT\n",
    "print(\"=\"*60)\n",
    "print(\"FASE 2: FINE-TUNING ÚLTIMAS CAPAS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if FREEZE_BASE:\n",
    "    # Descongelar todo el modelo DistilBERT\n",
    "    distilbert_layer.trainable = True\n",
    "    \n",
    "    # Acceder a las capas transformer internas\n",
    "    # DistilBERT tiene 6 capas transformer en distilbert.transformer.layer\n",
    "    transformer_layers = distilbert_layer.distilbert.transformer.layer\n",
    "    total_transformer_layers = len(transformer_layers)\n",
    "    \n",
    "    # Congelar todas las capas transformer excepto las últimas N\n",
    "    layers_to_freeze = max(0, total_transformer_layers - UNFREEZE_LAST_N_LAYERS)\n",
    "    \n",
    "    # Congelar embeddings\n",
    "    distilbert_layer.distilbert.embeddings.trainable = False\n",
    "    \n",
    "    # Congelar capas transformer según estrategia\n",
    "    for i, layer in enumerate(transformer_layers):\n",
    "        if i < layers_to_freeze:\n",
    "            layer.trainable = False\n",
    "        else:\n",
    "            layer.trainable = True\n",
    "    \n",
    "    print(f\"Total transformer layers: {total_transformer_layers}\")\n",
    "    print(f\"Transformer layers congeladas: {layers_to_freeze}\")\n",
    "    print(f\"Transformer layers entrenables: {UNFREEZE_LAST_N_LAYERS}\")\n",
    "    print(f\"Embeddings: CONGELADOS\")\n",
    "    \n",
    "    # Re-compilar con learning rate más bajo\n",
    "    optimizer_phase2 = keras.optimizers.Adam(learning_rate=LEARNING_RATE / 10)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer_phase2,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.Precision(name='precision'),\n",
    "            keras.metrics.Recall(name='recall'),\n",
    "            keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    trainable_params_phase2 = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "    print(f\"\\nTrainable parameters fase 2: {trainable_params_phase2:,}\")\n",
    "    print(f\"Learning rate: {LEARNING_RATE / 10}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Fase 2: Fine-tuning (resto de epochs)\n",
    "    history_phase2 = model.fit(\n",
    "        [X_train_ids_final, X_train_masks_final],\n",
    "        y_train_final,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        initial_epoch=3,\n",
    "        validation_data=([X_val_ids, X_val_masks], y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FASE 2 COMPLETADA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Combinar historiales\n",
    "    history = history_phase1\n",
    "    for key in history_phase2.history:\n",
    "        history.history[key].extend(history_phase2.history[key])\n",
    "else:\n",
    "    history = history_phase1\n",
    "\n",
    "print(\"\\nENTRENAMIENTO COMPLETO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29ea18",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fa022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en validación\n",
    "y_pred_proba = model.predict([X_val_ids, X_val_masks], batch_size=BATCH_SIZE)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION METRICS - DISTILBERT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc_score:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'DistilBERT Confusion Matrix (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b672",
   "metadata": {},
   "source": [
    "## Análisis Comparativo - LSTM vs DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis comparativo\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "overfitting_delta = abs(final_val_loss - final_train_loss)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS COMPARATIVO - LSTM vs DISTILBERT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = {\n",
    "    'V1 LSTM': {'mcc': 0.8665, 'arch': 'Bi-LSTM(128)', 'params': '1.25M'},\n",
    "    'V2 LSTM': {'mcc': 0.8885, 'arch': 'Bi-LSTM(96)+L2', 'params': '1.16M'},\n",
    "    'V3 LSTM': {'mcc': 0.8733, 'arch': 'Bi-LSTM(64)+L2x5', 'params': '~1.0M'},\n",
    "    'V4 DistilBERT': {'mcc': mcc_score, 'arch': 'DistilBERT+FT', 'params': f'{model.count_params()/1e6:.1f}M'}\n",
    "}\n",
    "\n",
    "print(\"\\nMCC Evolution:\")\n",
    "for version, data in comparison_data.items():\n",
    "    print(f\"  {version}: {data['mcc']:.4f} | {data['arch']} | {data['params']} params\")\n",
    "\n",
    "best_lstm = 0.8885  # V2\n",
    "improvement = mcc_score - best_lstm\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTADO TRANSFER LEARNING:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"MCC DistilBERT: {mcc_score:.4f}\")\n",
    "print(f\"Best LSTM (V2): {best_lstm:.4f}\")\n",
    "print(f\"Mejora: {'+' if improvement > 0 else ''}{improvement:.4f} ({improvement/best_lstm*100:+.1f}%)\")\n",
    "print(f\"\\nOverfitting Delta: {overfitting_delta:.4f}\")\n",
    "print(f\"Val Accuracy: {final_val_acc:.4f}\")\n",
    "\n",
    "if mcc_score > 0.90:\n",
    "    print(\"\\nOBJETIVO CUMPLIDO: MCC > 0.90\")\n",
    "elif mcc_score > best_lstm:\n",
    "    print(\"\\nMEJORA CONFIRMADA: Transfer learning superior a LSTM\")\n",
    "elif mcc_score > best_lstm - 0.01:\n",
    "    print(\"\\nRESULTADO SIMILAR: Transfer learning competitivo\")\n",
    "else:\n",
    "    print(\"\\nATENCIÓN: LSTM V2 sigue siendo mejor\")\n",
    "    print(\"Posibles causas: fine-tuning insuficiente, hyperparams subóptimos\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3337f",
   "metadata": {},
   "source": [
    "## Predicciones en Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f571d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total test samples: {len(test):,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Tokenizar test data\n",
    "print(\"\\nTokenizando test data con DistilBERT...\")\n",
    "X_test_text = test['text'].values\n",
    "X_test_ids, X_test_masks = tokenize_texts(X_test_text, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"Test shape: {X_test_ids.shape}\")\n",
    "\n",
    "# Generar predicciones\n",
    "print(\"\\nGenerando predicciones...\")\n",
    "y_pred_proba_test = model.predict([X_test_ids, X_test_masks], batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"\\nPredicciones generadas: {len(y_pred_test):,}\")\n",
    "print(f\"Distribución:\")\n",
    "print(f\"  Not SPAM: {np.sum(y_pred_test == 0):,} ({np.mean(y_pred_test == 0)*100:.2f}%)\")\n",
    "print(f\"  SPAM: {np.sum(y_pred_test == 1):,} ({np.mean(y_pred_test == 1)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e647004",
   "metadata": {},
   "source": [
    "## Generación de Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c583447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear submission file\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUBMISSION FILE CREATED - DISTILBERT V4\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(submission):,}\")\n",
    "print(f\"File: submission.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b496a",
   "metadata": {},
   "source": [
    "## Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla resumen final\n",
    "summary_df = pd.DataFrame({\n",
    "    'Iteración': ['V1', 'V2', 'V3', 'V4'],\n",
    "    'Arquitectura': ['Bi-LSTM(128)', 'Bi-LSTM(96)+L2', 'Bi-LSTM(64)+L2x5', 'DistilBERT+FT'],\n",
    "    'Val MCC': [0.8665, 0.8885, 0.8733, f'{mcc_score:.4f}'],\n",
    "    'Parámetros': ['1.25M', '1.16M', '~1.0M', f'{model.count_params()/1e6:.1f}M'],\n",
    "    'Trainable': ['1.25M', '1.16M', '~1.0M', f'{trainable_params/1e6:.1f}M'],\n",
    "    'Approach': ['Baseline', 'Regularization', 'Shock Therapy', 'Transfer Learning']\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"RESUMEN FINAL - TODAS LAS ITERACIONES\")\n",
    "print(\"=\"*90)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nCONCLUSIONES:\")\n",
    "print(f\"  - Transfer Learning MCC: {mcc_score:.4f}\")\n",
    "print(f\"  - Mejor LSTM (V2): 0.8885\")\n",
    "print(f\"  - Diferencia: {mcc_score - 0.8885:.4f}\")\n",
    "print(\"\\nVENTAJAS DISTILBERT:\")\n",
    "print(\"  + Embeddings contextuales pre-entrenados\")\n",
    "print(\"  + Comprensión semántica superior\")\n",
    "print(\"  + Menor overfitting (pre-training robusto)\")\n",
    "print(\"\\nDESVENTAJAS:\")\n",
    "print(\"  - Mayor consumo memoria\")\n",
    "print(\"  - Entrenamiento más lento\")\n",
    "print(\"  - Requiere fine-tuning cuidadoso\")\n",
    "print(\"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
