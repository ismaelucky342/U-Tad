{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528b41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#      Competición - SPAM/NOT SPAM - ITERACIÓN 7                                                    #\n",
    "#      V7: FEATURE ENGINEERING + LSTM HÍBRIDO                                                       #\n",
    "#      Basado en análisis del dataset                                                               #\n",
    "#                                                                                                    #\n",
    "#      HALLAZGOS DEL ANÁLISIS:                                                                      #\n",
    "#      - URLs en SPAM: 19.6% vs 1.7% Not SPAM (+17.9%)                                             #\n",
    "#      - Money words en SPAM: 42.3% vs 18.9% (+23.4%)                                              #\n",
    "#      - Palabras clave: nbsp, width, font, pills, viagra, prescription                            #\n",
    "#      - Problema: 1201 textos Not SPAM muy cortos confunden al modelo                             #\n",
    "#                                                                                                    #\n",
    "#      ESTRATEGIA V7:                                                                               #\n",
    "#      ✅ LSTM base V3 (funciona bien)                                                              #\n",
    "#      ✅ + Features manuales concatenadas (URLs, money, HTML, pharma)                              #\n",
    "#      ✅ + Class weights para imbalance                                                            #\n",
    "#====================================================================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ec982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fec0b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Bidirectional, Dense, Dropout,\n",
    "    SpatialDropout1D, Concatenate, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63197a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - V7 = V3 base + features\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# V3 exacto\n",
    "LSTM_UNITS = 64\n",
    "DENSE_UNITS = 32\n",
    "SPATIAL_DROPOUT = 0.4\n",
    "DROPOUT_RATE = 0.7\n",
    "L2_REG = 5e-4\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 5e-4\n",
    "CLIPNORM = 1.0\n",
    "\n",
    "# Número de features manuales\n",
    "N_FEATURES = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51e0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(f\"Train: {len(train)} | Test: {len(test)}\")\n",
    "print(f\"Class balance: {train['spam_label'].value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ae6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING - Basado en análisis del dataset\n",
    "def extract_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Convertir texto a minúsculas para los counts\n",
    "    text_lower = df['text'].str.lower()\n",
    "    \n",
    "    # 1. Longitud del texto\n",
    "    features['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    features['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "    \n",
    "    # 2. URLs (SPAM: 19.6% vs Not SPAM: 1.7%)\n",
    "    features['has_url'] = text_lower.str.contains(r'http|www\\.|href', regex=True).fillna(False).astype(int)\n",
    "    features['url_count'] = text_lower.str.count(r'http|www\\.').fillna(0)\n",
    "    \n",
    "    # 3. HTML tags (SPAM tiene mucho HTML: nbsp, width, font, border)\n",
    "    features['has_html'] = text_lower.str.contains(r'<[^>]+>|nbsp|width=|font|border|valign', regex=True).fillna(False).astype(int)\n",
    "    \n",
    "    # 4. Money/Free words (SPAM: 42.3% vs 18.9%)\n",
    "    features['money_words'] = text_lower.str.count(r'\\$|free|money|cash|win|prize|offer|discount|sale|cheap').fillna(0)\n",
    "    \n",
    "    # 5. Pharma words (viagra, cialis, pills, prescription)\n",
    "    features['pharma_words'] = text_lower.str.count(r'viagra|cialis|pills?|prescription|pharmacy|medication|drug').fillna(0)\n",
    "    \n",
    "    # 6. Urgency words\n",
    "    features['urgent_words'] = text_lower.str.count(r'urgent|immediately|hurry|limited|act now').fillna(0)\n",
    "    \n",
    "    # 7. Subject line (más común en SPAM: 51.1% vs 40.4%)\n",
    "    features['has_subject'] = text_lower.str.startswith('subject:').fillna(False).astype(int)\n",
    "    \n",
    "    # 8. Ratio mayúsculas\n",
    "    features['caps_ratio'] = df['text'].apply(lambda x: sum(1 for c in str(x) if c.isupper()) / max(len(str(x)), 1))\n",
    "    \n",
    "    # 9. Caracteres especiales (spam tiende a tener más)\n",
    "    features['special_chars'] = df['text'].apply(lambda x: sum(1 for c in str(x) if not c.isalnum() and not c.isspace()) / max(len(str(x)), 1))\n",
    "    \n",
    "    # 10. Texto muy corto (difícil de clasificar)\n",
    "    features['is_short'] = (features['word_count'] < 10).astype(int)\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Extrayendo features...\")\n",
    "train_features = extract_features(train)\n",
    "test_features = extract_features(test)\n",
    "\n",
    "print(f\"Features extraídas: {train_features.shape[1]}\")\n",
    "print(train_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccbc6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar features\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n",
    "\n",
    "print(f\"Features shape: {train_features_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9ea7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization (igual que V3)\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train['text'])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Train/val split (mantener misma división para features y secuencias)\n",
    "X_train_text, X_val_text, X_train_feat, X_val_feat, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, train_features_scaled, y_train, \n",
    "    test_size=VALIDATION_SPLIT, random_state=seed, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train text shape: {X_train_text.shape}\")\n",
    "print(f\"Train features shape: {X_train_feat.shape}\")\n",
    "print(f\"Val text shape: {X_val_text.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights para imbalance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train_final),\n",
    "    y=y_train_final\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96eb29fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_v7_hybrid_model():\n",
    "    # Input 1: Secuencias de texto\n",
    "    text_input = Input(shape=(MAX_LEN,), name='text_input')\n",
    "    \n",
    "    # Rama LSTM (V3 exacto)\n",
    "    x = Embedding(\n",
    "        input_dim=MAX_WORDS,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_LEN\n",
    "    )(text_input)\n",
    "    \n",
    "    x = SpatialDropout1D(SPATIAL_DROPOUT)(x)\n",
    "    \n",
    "    lstm_out = Bidirectional(\n",
    "        LSTM(\n",
    "            LSTM_UNITS,\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            recurrent_regularizer=l2(L2_REG),\n",
    "            bias_regularizer=l2(L2_REG)\n",
    "        )\n",
    "    )(x)\n",
    "    \n",
    "    # Input 2: Features manuales\n",
    "    features_input = Input(shape=(train_features_scaled.shape[1],), name='features_input')\n",
    "    \n",
    "    # Procesar features\n",
    "    feat_dense = Dense(16, activation='relu', kernel_regularizer=l2(L2_REG))(features_input)\n",
    "    feat_dense = BatchNormalization()(feat_dense)\n",
    "    feat_dense = Dropout(0.3)(feat_dense)\n",
    "    \n",
    "    # CONCATENAR LSTM + Features\n",
    "    combined = Concatenate()([lstm_out, feat_dense])\n",
    "    \n",
    "    # Classifier final\n",
    "    combined = Dense(\n",
    "        DENSE_UNITS,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(L2_REG)\n",
    "    )(combined)\n",
    "    \n",
    "    combined = Dropout(DROPOUT_RATE)(combined)\n",
    "    \n",
    "    outputs = Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs=[text_input, features_input],\n",
    "        outputs=outputs,\n",
    "        name='V7_Hybrid_LSTM_Features'\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = build_v7_hybrid_model()\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=1e-4,\n",
    "    clipnorm=CLIPNORM\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks - V3 exacto\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,  # Un poco más de paciencia para las features\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_spam_model_v7.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_text, X_train_feat], y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=([X_val_text, X_val_feat], y_val),\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eeb017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_pred_proba = model.predict([X_val_text, X_val_feat], batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "\n",
    "best_threshold = 0.5\n",
    "y_pred = (y_pred_proba > best_threshold).astype(int)\n",
    "mcc_val = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "# Debug\n",
    "final_epoch = len(history.history['loss'])\n",
    "train_loss_final = history.history['loss'][-1]\n",
    "val_loss_final = history.history['val_loss'][-1]\n",
    "overfitting_delta = val_loss_final - train_loss_final\n",
    "\n",
    "spam_probs = y_pred_proba[y_val == 1]\n",
    "notspam_probs = y_pred_proba[y_val == 0]\n",
    "separation = spam_probs.mean() - notspam_probs.mean()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"V7 - HYBRID LSTM + FEATURES RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Val MCC: {mcc_val:.4f}\")\n",
    "print(f\"Epochs: {final_epoch}\")\n",
    "print(f\"Train Loss: {train_loss_final:.4f} | Val Loss: {val_loss_final:.4f}\")\n",
    "print(f\"Overfitting Δ: {overfitting_delta:.4f}\")\n",
    "print(f\"Class Separation: {separation:.4f}\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCOMPARACIÓN:\")\n",
    "print(f\"V3 (baseline):  0.8849 val / 0.87 test\")\n",
    "print(f\"V7 (hybrid):    {mcc_val:.4f} val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d39ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "y_test_proba = model.predict([X_test_pad, test_features_scaled], batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_test_pred = (y_test_proba > best_threshold).astype(int)\n",
    "\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_test_pred\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission: {len(y_test_pred)} predictions\")\n",
    "print(f\"SPAM: {y_test_pred.sum()} ({y_test_pred.mean()*100:.1f}%)\")\n",
    "print(f\"Not SPAM: {len(y_test_pred) - y_test_pred.sum()} ({(1-y_test_pred.mean())*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
