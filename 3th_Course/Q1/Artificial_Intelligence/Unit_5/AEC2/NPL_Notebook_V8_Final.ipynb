{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#      CompeticiÃ³n - SPAM/NOT SPAM - ITERACIÃ“N 8 (FINAL)                                            #\n",
    "#      V8: V3 PURO + MICRO-AJUSTES                                                                  #\n",
    "#                                                                                                    #\n",
    "#      LECCIONES APRENDIDAS:                                                                        #\n",
    "#      - V3 (LSTM puro): 0.8849 val / 0.87 test â† MEJOR                                            #\n",
    "#      - V4 (DistilBERT): 0.64 â† FRACASO                                                           #\n",
    "#      - V5 (CNN+LSTM): 0.81-0.86 â† overfitting                                                    #\n",
    "#      - V6 (attention): 0.78-0.82 â† overfitting                                                   #\n",
    "#      - V7 (features): 0.8610 â† peor que V3                                                       #\n",
    "#                                                                                                    #\n",
    "#      ESTRATEGIA V8:                                                                               #\n",
    "#      âœ… V3 EXACTO (lo que funciona)                                                               #\n",
    "#      ðŸ”§ LSTM: 64 â†’ 72 (+12.5% capacidad)                                                          #\n",
    "#      ðŸ”§ L2: 5e-4 â†’ 4e-4 (ligeramente menos restrictivo)                                          #\n",
    "#      ðŸ”§ Patience: 2 â†’ 3 (dar mÃ¡s tiempo a converger)                                             #\n",
    "#                                                                                                    #\n",
    "#      OBJETIVO: MCC >= 0.89                                                                        #\n",
    "#====================================================================================================#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68136b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Bidirectional, Dense, Dropout,\n",
    "    GlobalMaxPooling1D, SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters - V8 = V3 + micro-ajustes\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Model - MICRO-AJUSTES vs V3\n",
    "LSTM_UNITS = 72        # V3: 64 â†’ 72 (+12.5% capacidad)\n",
    "DENSE_UNITS = 32       # IGUAL V3\n",
    "SPATIAL_DROPOUT = 0.4  # IGUAL V3\n",
    "DROPOUT_RATE = 0.7     # IGUAL V3\n",
    "L2_REG = 4e-4          # V3: 5e-4 â†’ 4e-4 (menos restrictivo)\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 5e-4   # IGUAL V3\n",
    "CLIPNORM = 1.0         # IGUAL V3\n",
    "\n",
    "print(\"V8 Config: LSTM=72, L2=4e-4, patience=3\")\n",
    "print(\"Cambios vs V3: +12.5% capacidad LSTM, -20% L2 regularizaciÃ³n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc95c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(f\"Train: {len(train)} | Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization - IGUAL V3\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(train['text'])\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(train['text'])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "X_test_seq = tokenizer.texts_to_sequences(test['text'])\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Train/val split\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, test_size=VALIDATION_SPLIT, random_state=seed, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train_final)} | Val: {len(X_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model V8 - V3 PURO con micro-ajustes\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=MAX_WORDS,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_LEN\n",
    "    ),\n",
    "    SpatialDropout1D(SPATIAL_DROPOUT),\n",
    "    Bidirectional(\n",
    "        LSTM(\n",
    "            LSTM_UNITS,\n",
    "            return_sequences=True,\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            recurrent_regularizer=l2(L2_REG),\n",
    "            bias_regularizer=l2(L2_REG)\n",
    "        )\n",
    "    ),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(\n",
    "        DENSE_UNITS,\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(L2_REG),\n",
    "        bias_regularizer=l2(L2_REG)\n",
    "    ),\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(1, activation='sigmoid')\n",
    "], name='V8_LSTM_Pure_Tuned')\n",
    "\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=1e-4,\n",
    "    clipnorm=CLIPNORM\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), keras.metrics.AUC()]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdcfaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks - ligeramente mÃ¡s paciencia\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,  # V3: 2 â†’ 3 (mÃ¡s tiempo)\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_spam_model_v8.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,  # V3: 1 â†’ 2\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b28f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "y_pred_proba = model.predict(X_val, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "\n",
    "best_threshold = 0.5\n",
    "y_pred = (y_pred_proba > best_threshold).astype(int)\n",
    "mcc_val = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "# Debug\n",
    "final_epoch = len(history.history['loss'])\n",
    "train_loss_final = history.history['loss'][-1]\n",
    "val_loss_final = history.history['val_loss'][-1]\n",
    "overfitting_delta = val_loss_final - train_loss_final\n",
    "\n",
    "spam_probs = y_pred_proba[y_val == 1]\n",
    "notspam_probs = y_pred_proba[y_val == 0]\n",
    "separation = spam_probs.mean() - notspam_probs.mean()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"V8 - LSTM PURE + MICRO-TUNING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Val MCC: {mcc_val:.4f}\")\n",
    "print(f\"Epochs: {final_epoch}\")\n",
    "print(f\"Train Loss: {train_loss_final:.4f} | Val Loss: {val_loss_final:.4f}\")\n",
    "print(f\"Overfitting Î”: {overfitting_delta:.4f}\")\n",
    "print(f\"Class Separation: {separation:.4f}\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nCOMPARACIÃ“N:\")\n",
    "print(f\"V3 (baseline):     0.8849 val / 0.87 test\")\n",
    "print(f\"V7 (features):     0.8610 val â† PEOR\")\n",
    "print(f\"V8 (micro-tuned):  {mcc_val:.4f} val\")\n",
    "\n",
    "if mcc_val > 0.8849:\n",
    "    print(\"\\nâœ… V8 SUPERA V3 - USAR ESTE MODELO\")\n",
    "elif mcc_val > 0.88:\n",
    "    print(\"\\nâœ“ V8 similar a V3 - Probar en test\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ V8 no supera V3 - V3 sigue siendo el mejor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31702ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "y_test_proba = model.predict(X_test_pad, batch_size=BATCH_SIZE, verbose=0).flatten()\n",
    "y_test_pred = (y_test_proba > best_threshold).astype(int)\n",
    "\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_test_pred\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission: {len(y_test_pred)} predictions\")\n",
    "print(f\"SPAM: {y_test_pred.sum()} ({y_test_pred.mean()*100:.1f}%)\")\n",
    "print(f\"Not SPAM: {len(y_test_pred) - y_test_pred.sum()} ({(1-y_test_pred.mean())*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
