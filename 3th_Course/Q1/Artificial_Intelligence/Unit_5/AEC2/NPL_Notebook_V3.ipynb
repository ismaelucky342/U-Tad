{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4190c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CRITICAL FIX: Must be executed FIRST before any other imports\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "\n",
    "import sys\n",
    "try:\n",
    "    import google.protobuf\n",
    "    if hasattr(google.protobuf, '__version__'):\n",
    "        print(f\"Protobuf version: {google.protobuf.__version__}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(\"Environment variables set successfully\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b705b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMPORTING TENSORFLOW AND KERAS...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Embedding, LSTM, Bidirectional, Dense, Dropout, \n",
    "    GlobalMaxPooling1D, Conv1D, SpatialDropout1D\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "tf.random.set_seed(seed)\n",
    "keras.utils.set_random_seed(seed)\n",
    "\n",
    "from sklearn.metrics import matthews_corrcoef, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_rows', 36)\n",
    "pd.set_option(\"display.max_colwidth\", 150)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT SETUP - SUCCESS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(f\"GPU Devices: {len(gpu_devices)} device(s)\")\n",
    "    for gpu in gpu_devices:\n",
    "        print(f\"  - {gpu.name}\")\n",
    "print(f\"Random seed: {seed}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de058565",
   "metadata": {},
   "source": [
    "## Iteración 3 - Configuración del Modelo - TERAPIA DE CHOQUE\n",
    "\n",
    "**Problema Persistente:**\n",
    "- V1: Overfitting Delta = 0.184 (MCC 0.8665)\n",
    "- V2: Overfitting Delta = 0.166 (MCC 0.8885) - Mejora insuficiente 9.6%\n",
    "\n",
    "**Terapia de Choque - Cambios Radicales:**\n",
    "1. Reducir capacidad modelo 50% total vs V1\n",
    "2. Dropout extremo (0.7 en dense, 0.4 spatial)\n",
    "3. L2 regularization x5 más fuerte\n",
    "4. Learning rate más bajo (control de gradientes)\n",
    "5. Gradient clipping (evitar explosión)\n",
    "6. Early stopping ultra agresivo (patience=2)\n",
    "\n",
    "**Meta:** Overfitting Delta < 0.08 manteniendo MCC > 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef09c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters Configuration - ITERACIÓN 3 - TERAPIA DE CHOQUE\n",
    "MAX_WORDS = 10000\n",
    "MAX_LEN = 200\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Model Architecture - CAMBIOS AGRESIVOS\n",
    "LSTM_UNITS = 64           # ITER3: 96 → 64 (-33% vs V2, -50% vs V1)\n",
    "DENSE_UNITS = 32          # ITER3: 48 → 32 (-33% vs V2, -50% vs V1)\n",
    "DROPOUT_RATE = 0.7        # ITER3: 0.6 → 0.7 (EXTREMO)\n",
    "SPATIAL_DROPOUT = 0.4     # ITER3: 0.3 → 0.4 (EXTREMO)\n",
    "L2_REG = 5e-4             # ITER3: 1e-4 → 5e-4 (x5 más fuerte)\n",
    "\n",
    "# Training Configuration - AJUSTES AGRESIVOS\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "LEARNING_RATE = 5e-4      # ITER3: 1e-3 → 5e-4 (reduce velocidad)\n",
    "CLIPNORM = 1.0            # ITER3: NUEVO - Gradient clipping\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL CONFIGURATION - ITERACIÓN 3\")\n",
    "print(\"TERAPIA DE CHOQUE - OVERFITTING KILLER\")\n",
    "print(\"=\"*60)\n",
    "print(\"OBJETIVO: Overfitting Delta < 0.08 y MCC > 0.87\")\n",
    "print(\"=\"*60)\n",
    "print(\"CAMBIOS AGRESIVOS respecto a V2:\")\n",
    "print(\"  - LSTM Units: 96 → 64 (-33%)\")\n",
    "print(\"  - Dense Units: 48 → 32 (-33%)\")\n",
    "print(\"  - Spatial Dropout: 0.3 → 0.4 (EXTREMO)\")\n",
    "print(\"  - Dropout: 0.6 → 0.7 (EXTREMO)\")\n",
    "print(\"  - L2 Reg: 1e-4 → 5e-4 (x5 más fuerte)\")\n",
    "print(\"  - Learning Rate: 1e-3 → 5e-4 (más lento)\")\n",
    "print(\"  - Gradient Clipping: None → 1.0 (NUEVO)\")\n",
    "print(\"  - Early Stop Patience: 3 → 2 (ultra agresivo)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary Size: {MAX_WORDS:,}\")\n",
    "print(f\"Sequence Length: {MAX_LEN}\")\n",
    "print(f\"LSTM Units: {LSTM_UNITS}\")\n",
    "print(f\"Dense Units: {DENSE_UNITS}\")\n",
    "print(f\"Dropout Rate: {DROPOUT_RATE}\")\n",
    "print(f\"Spatial Dropout: {SPATIAL_DROPOUT}\")\n",
    "print(f\"L2 Regularization: {L2_REG}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"Gradient Clip Norm: {CLIPNORM}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comparación de parámetros\n",
    "estimated_params_v3 = (\n",
    "    MAX_WORDS * EMBEDDING_DIM +\n",
    "    4 * LSTM_UNITS * (EMBEDDING_DIM + LSTM_UNITS + 1) * 2 +\n",
    "    (LSTM_UNITS * 2) * DENSE_UNITS + DENSE_UNITS +\n",
    "    DENSE_UNITS + 1\n",
    ")\n",
    "print(f\"\\nComparación de parámetros:\")\n",
    "print(f\"   V1: 1,251,009 parámetros\")\n",
    "print(f\"   V2: 1,160,609 parámetros (-7.2% vs V1)\")\n",
    "print(f\"   V3: ~{estimated_params_v3:,} parámetros (estimado)\")\n",
    "print(f\"   Reducción total V3 vs V1: ~{((1251009 - estimated_params_v3) / 1251009 * 100):.1f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401aa07",
   "metadata": {},
   "source": [
    "## Carga y Exploración de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43721cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/train.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING DATA OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(train):,}\")\n",
    "print(f\"\\nColumns: {list(train.columns)}\")\n",
    "print(f\"\\nClass distribution:\\n{train['spam_label'].value_counts()}\")\n",
    "print(f\"\\nClass balance:\\n{train['spam_label'].value_counts(normalize=True)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563ec7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de longitud de textos\n",
    "train['text_length'] = train['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train['text_length'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribución de Longitud de Textos')\n",
    "plt.xlabel('Número de palabras')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.axvline(train['text_length'].mean(), color='red', linestyle='--', label=f'Media: {train[\"text_length\"].mean():.1f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=train, x='spam_label', y='text_length')\n",
    "plt.title('Longitud por Clase')\n",
    "plt.xlabel('Clase (0=Not SPAM, 1=SPAM)')\n",
    "plt.ylabel('Número de palabras')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Longitud promedio: {train['text_length'].mean():.2f} palabras\")\n",
    "print(f\"Longitud mediana: {train['text_length'].median():.2f} palabras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5865a3c0",
   "metadata": {},
   "source": [
    "## Preprocesamiento de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb58216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar textos y etiquetas\n",
    "X_train_text = train['text'].values\n",
    "y_train = train['spam_label'].values\n",
    "\n",
    "# Tokenización\n",
    "print(\"Tokenizando textos...\")\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train_text)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "# Split train/validation\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_pad, y_train, \n",
    "    test_size=VALIDATION_SPLIT, \n",
    "    random_state=seed,\n",
    "    stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index):,}\")\n",
    "print(f\"Training samples: {len(X_train_final):,}\")\n",
    "print(f\"Validation samples: {len(X_val):,}\")\n",
    "print(f\"Sequence shape: {X_train_pad.shape}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0add3",
   "metadata": {},
   "source": [
    "## Construcción del Modelo - ITERACIÓN 3 - TERAPIA DE CHOQUE\n",
    "\n",
    "**Arquitectura Ultra-Regularizada:**\n",
    "- Embedding → SpatialDropout(0.4) → Bi-LSTM(64) + L2(5e-4) → GlobalMaxPool → Dense(32) + L2(5e-4) → Dropout(0.7) → Output\n",
    "\n",
    "**Estrategia:**\n",
    "- Capacidad reducida 50% vs V1\n",
    "- Regularización extrema en todas las capas\n",
    "- Gradient clipping para estabilidad\n",
    "- Learning rate bajo para convergencia suave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9f291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construcción del modelo V3 - TERAPIA DE CHOQUE\n",
    "def build_model_v3():\n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(\n",
    "            input_dim=MAX_WORDS,\n",
    "            output_dim=EMBEDDING_DIM,\n",
    "            input_length=MAX_LEN,\n",
    "            name='embedding'\n",
    "        ),\n",
    "        \n",
    "        # ITER3: Spatial Dropout EXTREMO 0.4\n",
    "        SpatialDropout1D(SPATIAL_DROPOUT),\n",
    "        \n",
    "        # ITER3: LSTM reducido a 64 units + L2 x5 más fuerte\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                LSTM_UNITS, \n",
    "                return_sequences=True,\n",
    "                kernel_regularizer=l2(L2_REG),\n",
    "                recurrent_regularizer=l2(L2_REG),\n",
    "                bias_regularizer=l2(L2_REG)     # NUEVO: regularizar bias también\n",
    "            ), \n",
    "            name='bidirectional_lstm'\n",
    "        ),\n",
    "        \n",
    "        GlobalMaxPooling1D(),\n",
    "        \n",
    "        # ITER3: Dense reducido a 32 units + L2 x5 más fuerte\n",
    "        Dense(\n",
    "            DENSE_UNITS, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=l2(L2_REG),\n",
    "            bias_regularizer=l2(L2_REG),        # NUEVO: regularizar bias también\n",
    "            name='dense_1'\n",
    "        ),\n",
    "        \n",
    "        # ITER3: Dropout EXTREMO 0.7\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(1, activation='sigmoid', name='output')\n",
    "    ], name='spam_classifier_v3_shock_therapy')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "model = build_model_v3()\n",
    "\n",
    "# ITER3: AdamW con LR reducido + Gradient Clipping\n",
    "optimizer = keras.optimizers.AdamW(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=1e-4,\n",
    "    clipnorm=CLIPNORM  # NUEVO: Gradient clipping\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc')\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.build(input_shape=(None, MAX_LEN))\n",
    "model.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL V3 COMPILED - SHOCK THERAPY MODE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(f\"\\nComparación:\")\n",
    "print(f\"   V1: 1,251,009 params | MCC 0.8665 | Overfitting Δ 0.184\")\n",
    "print(f\"   V2: 1,160,609 params | MCC 0.8885 | Overfitting Δ 0.166\")\n",
    "print(f\"   V3: {model.count_params():,} params\")\n",
    "print(f\"\\n   Reducción V3 vs V1: {((1251009 - model.count_params()) / 1251009 * 100):.1f}%\")\n",
    "print(f\"   Reducción V3 vs V2: {((1160609 - model.count_params()) / 1160609 * 100):.1f}%\")\n",
    "print(f\"\\nOptimizer: AdamW (lr={LEARNING_RATE}, clipnorm={CLIPNORM})\")\n",
    "print(f\"L2 Regularization: {L2_REG} (x5 más fuerte que V2)\")\n",
    "print(f\"\\nOBJETIVO: Overfitting Delta < 0.08 y MCC > 0.87\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f2fae3",
   "metadata": {},
   "source": [
    "## Entrenamiento del Modelo - ITERACIÓN 3\n",
    "\n",
    "**Callbacks Ultra-Agresivos:**\n",
    "- EarlyStopping: patience=2 (detiene al mínimo signo de estancamiento)\n",
    "- ReduceLROnPlateau: patience=1 (reduce LR inmediatamente)\n",
    "- ModelCheckpoint: guarda mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e80b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ITER3: Callbacks ULTRA-AGRESIVOS\n",
    "callbacks = [\n",
    "    # ITER3: Patience 2 (ultra agresivo)\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=2,  # CAMBIO: 3 → 2\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        'best_spam_model_v3.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # ITER3: Patience 1 (reduce LR inmediatamente)\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=1,  # CAMBIO: 2 → 1\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING - ITERACIÓN 3 - SHOCK THERAPY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Monitoreando overfitting con configuración ultra-agresiva...\")\n",
    "print(\"META: Train Loss > 0.05 | Val Loss < 0.14 | Delta < 0.08\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED - ITERACIÓN 3\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb64bfc",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en validación\n",
    "y_pred_proba = model.predict(X_val)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "mcc_score = matthews_corrcoef(y_val, y_pred)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VALIDATION METRICS - ITERACIÓN 3\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Matthews Correlation Coefficient: {mcc_score:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred, target_names=['Not SPAM', 'SPAM']))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=['Not SPAM', 'SPAM'],\n",
    "            yticklabels=['Not SPAM', 'SPAM'])\n",
    "plt.title(f'Confusion Matrix V3 - Shock Therapy (MCC: {mcc_score:.4f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01439d0b",
   "metadata": {},
   "source": [
    "## Análisis Crítico - V1 vs V2 vs V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c69f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis comparativo completo\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "overfitting_delta = abs(final_val_loss - final_train_loss)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANÁLISIS CRÍTICO - EVOLUCIÓN V1 → V2 → V3\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Datos de iteraciones anteriores\n",
    "v1_data = {'train_loss': 0.0055, 'val_loss': 0.1895, 'mcc': 0.8665, 'params': 1251009}\n",
    "v2_data = {'train_loss': 0.0411, 'val_loss': 0.2075, 'mcc': 0.8885, 'params': 1160609}\n",
    "v3_data = {\n",
    "    'train_loss': final_train_loss, \n",
    "    'val_loss': final_val_loss, \n",
    "    'mcc': mcc_score,\n",
    "    'params': model.count_params()\n",
    "}\n",
    "\n",
    "v1_delta = abs(v1_data['val_loss'] - v1_data['train_loss'])\n",
    "v2_delta = abs(v2_data['val_loss'] - v2_data['train_loss'])\n",
    "v3_delta = overfitting_delta\n",
    "\n",
    "print(\"\\nITERACIÓN 1 (Baseline):\")\n",
    "print(f\"   Train Loss: {v1_data['train_loss']:.4f} | Val Loss: {v1_data['val_loss']:.4f}\")\n",
    "print(f\"   Overfitting Delta: {v1_delta:.4f}\")\n",
    "print(f\"   MCC: {v1_data['mcc']:.4f} | Params: {v1_data['params']:,}\")\n",
    "\n",
    "print(\"\\nITERACIÓN 2 (Regularización Moderada):\")\n",
    "print(f\"   Train Loss: {v2_data['train_loss']:.4f} | Val Loss: {v2_data['val_loss']:.4f}\")\n",
    "print(f\"   Overfitting Delta: {v2_delta:.4f}\")\n",
    "print(f\"   MCC: {v2_data['mcc']:.4f} | Params: {v2_data['params']:,}\")\n",
    "print(f\"   Mejora overfitting: {((v1_delta - v2_delta) / v1_delta * 100):.1f}%\")\n",
    "print(f\"   Mejora MCC: +{(v2_data['mcc'] - v1_data['mcc']):.4f}\")\n",
    "\n",
    "print(\"\\nITERACIÓN 3 (Terapia de Choque):\")\n",
    "print(f\"   Train Loss: {v3_data['train_loss']:.4f} | Val Loss: {v3_data['val_loss']:.4f}\")\n",
    "print(f\"   Overfitting Delta: {v3_delta:.4f}\")\n",
    "print(f\"   MCC: {v3_data['mcc']:.4f} | Params: {v3_data['params']:,}\")\n",
    "print(f\"   Mejora overfitting vs V2: {((v2_delta - v3_delta) / v2_delta * 100):.1f}%\")\n",
    "print(f\"   Mejora overfitting vs V1: {((v1_delta - v3_delta) / v1_delta * 100):.1f}%\")\n",
    "print(f\"   Cambio MCC vs V2: {(v3_data['mcc'] - v2_data['mcc']):.4f}\")\n",
    "print(f\"   Cambio MCC vs V1: {(v3_data['mcc'] - v1_data['mcc']):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUACIÓN FINAL:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "success_v3 = [\n",
    "    (\"Overfitting Delta < 0.08\", v3_delta < 0.08),\n",
    "    (\"MCC >= 0.87\", v3_data['mcc'] >= 0.87),\n",
    "    (\"Val Loss < 0.14\", v3_data['val_loss'] < 0.14),\n",
    "    (\"Train Loss > 0.05\", v3_data['train_loss'] > 0.05),\n",
    "    (\"Mejora vs V2\", v3_delta < v2_delta)\n",
    "]\n",
    "\n",
    "for criterion, success in success_v3:\n",
    "    status = \"OK\" if success else \"FAIL\"\n",
    "    print(f\"[{status}] {criterion}\")\n",
    "\n",
    "total_success = sum([s for _, s in success_v3])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"CRITERIOS CUMPLIDOS: {total_success}/5\")\n",
    "\n",
    "if total_success >= 4:\n",
    "    print(\"\\nRESULTADO: TERAPIA DE CHOQUE EXITOSA\")\n",
    "    print(\"PRÓXIMO PASO: Iteración 4 con GloVe embeddings pre-entrenados\")\n",
    "elif total_success >= 3:\n",
    "    print(\"\\nRESULTADO: MEJORA SIGNIFICATIVA\")\n",
    "    print(\"PRÓXIMO PASO: Ajustar threshold o probar arquitecturas alternativas\")\n",
    "else:\n",
    "    print(\"\\nRESULTADO: INSUFICIENTE - Considerar cambio de estrategia\")\n",
    "    print(\"PRÓXIMO PASO: CNN+LSTM híbrido o arquitectura completamente diferente\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b487e4e",
   "metadata": {},
   "source": [
    "## Curvas de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daed813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de curvas\n",
    "def plot_learning_curves(history, title_prefix=\"\"):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history.history['loss'], label='Train Loss', linewidth=2, color='red')\n",
    "    axes[0, 0].plot(history.history['val_loss'], label='Val Loss', linewidth=2, color='darkred')\n",
    "    axes[0, 0].set_title(f'{title_prefix} Loss', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[0, 1].plot(history.history['accuracy'], label='Train Acc', linewidth=2, color='red')\n",
    "    axes[0, 1].plot(history.history['val_accuracy'], label='Val Acc', linewidth=2, color='darkred')\n",
    "    axes[0, 1].set_title(f'{title_prefix} Accuracy', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision\n",
    "    axes[1, 0].plot(history.history['precision'], label='Train Prec', linewidth=2, color='red')\n",
    "    axes[1, 0].plot(history.history['val_precision'], label='Val Prec', linewidth=2, color='darkred')\n",
    "    axes[1, 0].set_title(f'{title_prefix} Precision', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Precision')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Recall\n",
    "    axes[1, 1].plot(history.history['recall'], label='Train Recall', linewidth=2, color='red')\n",
    "    axes[1, 1].plot(history.history['val_recall'], label='Val Recall', linewidth=2, color='darkred')\n",
    "    axes[1, 1].set_title(f'{title_prefix} Recall', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Recall')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_learning_curves(history, title_prefix=\"V3 Shock Therapy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c004f8",
   "metadata": {},
   "source": [
    "## Predicciones en Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443772fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/test.csv\", index_col=\"row_id\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total test samples: {len(test):,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772fe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar y predecir\n",
    "X_test_text = test['text'].values\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(\"Generando predicciones en test data...\")\n",
    "y_pred_proba_test = model.predict(X_test_pad, batch_size=BATCH_SIZE)\n",
    "y_pred_test = (y_pred_proba_test > 0.5).astype(int).flatten()\n",
    "\n",
    "print(f\"Predicciones generadas: {len(y_pred_test):,}\")\n",
    "print(f\"Distribución:\")\n",
    "print(f\"  Not SPAM: {np.sum(y_pred_test == 0):,} ({np.mean(y_pred_test == 0)*100:.2f}%)\")\n",
    "print(f\"  SPAM: {np.sum(y_pred_test == 1):,} ({np.mean(y_pred_test == 1)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3ffe8",
   "metadata": {},
   "source": [
    "## Generación de Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73547050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear submission file\n",
    "submission = pd.read_csv(\"/kaggle/input/u-tad-spam-not-spam-2025-edition/sample_submission.csv\")\n",
    "submission[\"spam_label\"] = y_pred_test\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUBMISSION FILE CREATED - ITERACIÓN 3\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total predictions: {len(submission):,}\")\n",
    "print(f\"File: submission.csv\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243380c",
   "metadata": {},
   "source": [
    "## Resumen Final Comparativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa final\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Métrica': [\n",
    "        'Validation MCC',\n",
    "        'Train Loss',\n",
    "        'Val Loss',\n",
    "        'Overfitting Delta',\n",
    "        'Train Accuracy',\n",
    "        'Val Accuracy',\n",
    "        'Total Parameters',\n",
    "        'Training Epochs'\n",
    "    ],\n",
    "    'V1 Baseline': [\n",
    "        '0.8665',\n",
    "        '0.0055',\n",
    "        '0.1895',\n",
    "        '0.1840',\n",
    "        '99.91%',\n",
    "        '95.77%',\n",
    "        '1,251,009',\n",
    "        '8'\n",
    "    ],\n",
    "    'V2 Moderate': [\n",
    "        '0.8885',\n",
    "        '0.0411',\n",
    "        '0.2075',\n",
    "        '0.1663',\n",
    "        '99.56%',\n",
    "        '95.07%',\n",
    "        '1,160,609',\n",
    "        '6'\n",
    "    ],\n",
    "    'V3 Shock': [\n",
    "        f'{mcc_score:.4f}',\n",
    "        f'{final_train_loss:.4f}',\n",
    "        f'{final_val_loss:.4f}',\n",
    "        f'{overfitting_delta:.4f}',\n",
    "        f'{final_train_acc:.2%}',\n",
    "        f'{final_val_acc:.2%}',\n",
    "        f'{model.count_params():,}',\n",
    "        f'{len(history.history[\"loss\"])}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(\"TABLA COMPARATIVA FINAL - V1 vs V2 vs V3\")\n",
    "print(\"=\"*90)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*90)\n",
    "\n",
    "print(\"\\nEVOLUCIÓN OVERFITTING:\")\n",
    "print(f\"  V1: 0.1840 (baseline)\")\n",
    "print(f\"  V2: 0.1663 (-9.6%)\")\n",
    "print(f\"  V3: {overfitting_delta:.4f} ({((0.1840 - overfitting_delta) / 0.1840 * 100):.1f}% vs V1)\")\n",
    "\n",
    "print(\"\\nEVOLUCIÓN MCC:\")\n",
    "print(f\"  V1: 0.8665 (baseline)\")\n",
    "print(f\"  V2: 0.8885 (+0.0220)\")\n",
    "print(f\"  V3: {mcc_score:.4f} ({'+' if mcc_score > 0.8665 else ''}{(mcc_score - 0.8665):.4f} vs V1)\")\n",
    "\n",
    "print(\"=\"*90)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
