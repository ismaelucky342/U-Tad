# Unidad 6

Creado: 15 de diciembre de 2025 22:29

![image.png](Unit_6/image.png)

El aprendizaje por refuerzo y el aprendizaje generativo son dos de las √°reas m√°s din√°micas y revolucionarias de la inteligencia artificial.

El aprendizaje por refuerzo se basa en la interacci√≥n continua de un agente con su entorno, permitiendo que el agente aprenda comportamientos √≥ptimos a trav√©s de un sistema de recompensas y penalizaciones. Esta t√©cnica ha sido clave para lograr avances impresionantes en √°reas como los videojuegos, la rob√≥tica y la toma de decisiones aut√≥noma.

Por otro lado, el aprendizaje generativo ha transformado la capacidad de los modelos de IA para crear contenido nuevo, como im√°genes, m√∫sica y texto, bas√°ndose en patrones aprendidos. Con modelos como las redes generativas antag√≥nicas (GAN) y los autoencoders variacionales (VAE), las aplicaciones en la creaci√≥n de medios sint√©ticos y en la mejora de la calidad de los datos han tomado un rol protagonista en la investigaci√≥n de IA moderna.

## Tipos de Aprendizaje Autom√°tico

En el campo de la inteligencia artificial, existen diferentes enfoques para que los modelos aprendan a partir de datos, y estos se clasifican principalmente en tres tipos: aprendizaje supervisado, aprendizaje no supervisado, y aprendizaje por refuerzo. Adicionalmente, el aprendizaje generativo ha ganado relevancia debido a su capacidad para generar datos nuevos a partir de distribuciones aprendidas.

### Aprendizaje Supervisado

En este paradigma el modelo aprende a partir de ejemplos etiquetados, donde cada entrada tiene una salida correspondiente. El objetivo es predecir la etiqueta correcta para nuevas entradas. Es com√∫n en tareas como la clasificaci√≥n de im√°genes o el reconocimiento de voz.

Un ejemplo de aprendizaje supervisado es el algoritmo de Retropropagaci√≥n.

![image.png](Unit_6/image%201.png)

### Aprendizaje no Supervisado

Aqu√≠, el modelo se entrena con datos no etiquetados y busca descubrir patrones o estructuras subyacentes en los datos. Los algoritmos no supervisados son √∫tiles en tareas como la agrupaci√≥n (clustering) o la reducci√≥n de dimensionalidad donde a diferencia del aprendizaje supervisado, no se basa en un resultado conocido para entrenar el modelo

![image.png](Unit_6/image%202.png)

- **Datos sin etiquetar:** trabajan con datos en bruto, sin ninguna gu√≠a o instrucci√≥n expl√≠cita sobre lo que representan.
- **Descubrimiento de patrones:** Encontrar la estructura inherente de los datos y extraer informaci√≥n directamente de su naturaleza.
- **Ausencia de variable objetivo:** No existe una variable "target" o resultado deseado al cual dirigir el algoritmo.

| Tipo | Descripci√≥n | Algoritmos Comunes |
| --- | --- | --- |
| **Agrupamiento (Clustering)** | Agrupa elementos bas√°ndose en caracter√≠sticas similares o su proximidad. | **K-Means**, **DBSCAN**, Agrupamiento Jer√°rquico |
| **Reducci√≥n de Dimensionalidad** | Simplifica los conjuntos de datos al reducir el n√∫mero de variables (atributos), manteniendo la informaci√≥n m√°s relevante. | An√°lisis de Componentes Principales (**PCA**) |

### **Aprendizaje generativo**

Este enfoque se centra en la creaci√≥n de datos nuevos que sigan la misma distribuci√≥n que el conjunto de datos de entrenamiento. Los modelos generativos, como las Redes Generativas Antag√≥nicas (GAN) y los Autoencoders Variacionales (VAE), permiten la generaci√≥n de im√°genes, texto y otros tipos de datos. A diferencia de los modelos discriminativos, que solo intentan clasificar los datos, los modelos generativos tambi√©n pueden crear nuevos ejemplos que son coherentes con los datos aprendidos.

![image.png](Unit_6/image%203.png)

Estos modelos utilizan t√©cnicas de *machine learning* y *deep learning* para analizar grandes vol√∫menes de datos. 

Durante este entrenamiento masivo, la IA no solo identifica la informaci√≥n, sino que aprende las reglas subyacentes, la gram√°tica y los estilos presentes en los datos. 

Una vez entrenado, el modelo puede generar secuencias o elementos que imitan la calidad y el estilo de los datos originales, pero que son creaciones nuevas.¬†

> **Fiorella y Mayer** propusieron ocho estrategias espec√≠ficas para promover el aprendizaje generativo:
> 

| Estrategia | Descripci√≥n | Ejemplo de Actividad |
| --- | --- | --- |
| **Resumir** | Producir una versi√≥n condensada del material, enfoc√°ndose en las ideas principales. | Escribir un p√°rrafo corto con las ideas clave de un cap√≠tulo. |
| **Mapear** | Crear representaciones gr√°ficas de las relaciones entre conceptos. | Elaborar un **mapa conceptual** o diagrama de flujo. |
| **Dibujar** | Representar visualmente el contenido para fomentar la comprensi√≥n espacial. | Dibujar el ciclo del agua o las partes de una c√©lula. |
| **Imaginar** | Formarse im√°genes mentales detalladas de la informaci√≥n presentada. | Visualizar un proceso hist√≥rico o un escenario descrito en un texto. |
| **Autoevaluar** | Poner a prueba el propio conocimiento sobre el tema. | Crear y responder preguntas de pr√°ctica sobre el material estudiado. |
| **Autoexplicar** | Expresar en voz alta o por escrito c√≥mo funciona algo o por qu√© un concepto es v√°lido. | Explicar los pasos para resolver un problema de matem√°ticas a uno mismo. |
| **Ense√±ar** | Prepararse para o realmente ense√±ar el material a otra persona. | Explicar un tema a un compa√±ero de clase o a un familiar. |
| **Representar** | Utilizar un formato diferente (tabla, gr√°fico) para presentar la misma informaci√≥n. | Convertir un texto narrativo sobre estad√≠sticas en una **tabla de datos**. |

# Introducci√≥n al Aprendizaje por Refuerzo

A diferencia de los enfoques anteriores, en RL el modelo (agente) interact√∫a con un entorno y aprende mediante ensayo y error. En lugar de etiquetas fijas, el agente recibe recompensas o castigos en funci√≥n de las acciones que toma, y ajusta su comportamiento para maximizar una recompensa acumulada a largo plazo.

- **Agente:** El sistema de IA (por ejemplo, un robot o un programa de *software*) que toma decisiones y realiza acciones.
- **Entorno:** El mundo exterior o contexto con el que interact√∫a el agente.
- **Estado:** La situaci√≥n actual del entorno en un momento dado.
- **Acci√≥n:** Un movimiento o decisi√≥n tomada por el agente en un estado particular.
- **Pol√≠tica (o Estrategia):** El m√©todo o conjunto de reglas que el agente utiliza para decidir qu√© acci√≥n tomar en cada estado. El objetivo del aprendizaje es encontrar la pol√≠tica √≥ptima.

![image.png](Unit_6/image%204.png)

> Por √∫ltimo definimos la **Recompensa** como una se√±al num√©rica (positiva o negativa) que recibe el agente tras realizar una acci√≥n, indicando si la acci√≥n fue buena o mala para su objetivo final.
> 

En el aprendizaje por refuerzo, el algoritmo comienza sin conocimiento previo y, mediante la experimentaci√≥n y la acumulaci√≥n de recompensas, desarrolla una estrategia √≥ptima. 

### Ciclo del Aprendizaje por Refuerzo

El ciclo del aprendizaje por refuerzo es un bucle continuo e interactivo entre un agente y su entorno. El objetivo del agente es aprender a tomar la mejor secuencia de acciones posible para maximizar una recompensa total a largo plazo.

Este ciclo, que se basa en un Proceso de Decisi√≥n de Markov (MDP), se compone de los siguientes pasos fundamentales que se repiten constantemente:

1. **Observaci√≥n del Estado (S):** En un momento dado, el agente percibe y eval√∫a su situaci√≥n o estado actual dentro del entorno.
2. **Toma de Acci√≥n (A)**: Basado en su pol√≠tica actual (su estrategia o conjunto de reglas), el
agente decide y ejecuta una acci√≥n espec√≠fica en ese estado.
3. **Interacci√≥n con el Entorno**: La acci√≥n del agente interact√∫a con el entorno, lo que provoca un cambio en la situaci√≥n general.
4. **Recepci√≥n de Recompensa (R):** El entorno proporciona una se√±al de retroalimentaci√≥n num√©rica, conocida como recompensa. Esta recompensa puede ser positiva (premio) o negativa (penalizaci√≥n) y le indica al agente qu√© tan buena o mala fue la acci√≥n tomada en
funci√≥n del objetivo final.
5. **Observaci√≥n del Nuevo Estado (S')**: El agente observa el nuevo estado resultante de su acci√≥n anterior.
6. **Actualizaci√≥n de la Pol√≠tica y Funci√≥n de Valor:** El agente utiliza la informaci√≥n de la recompensa y la transici√≥n de estado para actualizar su conocimiento (por ejemplo, su tabla Q o red neuronal). Este paso es crucial para refinar su estrategia o "pol√≠tica" y mejorar la toma de decisiones futuras.

Este proceso se repite una y otra vez, permitiendo al agente aprender por prueba y error a lo largo del tiempo, sin recibir instrucciones expl√≠citas sobre qu√© hacer en cada momento, sino aprendiendo de las consecuencias de sus propias acciones.

## M√©todolog√≠as y modelos

### **M√©todos Basados en el Valor (Value-Based Methods)**

Estos m√©todos se centran en aprender una **funci√≥n de valor** que estima cu√°nta recompensa futura puede esperar un agente por estar en un estado o tomar una acci√≥n espec√≠fica. El objetivo es elegir siempre la acci√≥n con el valor m√°s alto. Son ideales para **entornos discretos** (espacios de estados y acciones finitos).

**Algoritmos y Ejemplos**

- **Q-Learning:** Es quiz√°s el algoritmo m√°s conocido. Crea y actualiza una tabla Q (o una red neuronal profunda, en el caso de Deep Q-Networks o DQN) que asocia cada par estado-acci√≥n con un valor de recompensa esperado.
    - **Ejemplo Pr√°ctico:** Ense√±ar a un agente a jugar al **Pac-Man** o al **solitario Buscaminas** en una cuadr√≠cula (grid-world). El agente aprende, mediante prueba y error, a qu√© casillas moverse para maximizar puntos y evitar penalizaciones (minas).
- **SARSA:** Similar a Q-Learning, pero "on-policy" (aprende de las acciones que realmente ejecuta), mientras que Q-Learning es "off-policy" (aprende de la acci√≥n √≥ptima, independientemente de la que realmente hizo para explorar).
    - **Ejemplo Pr√°ctico:** Un **coche aut√≥nomo de juguete** que aprende a navegar un circuito simple. SARSA podr√≠a ser m√°s seguroen las primeras etapas porque es m√°s cauteloso, aprendiendo las penalizaciones de los movimientos "cercanos al borde" que realmente
    toma, no solo las √≥ptimas te√≥ricas.

### **M√©todos Basados en Pol√≠ticas (Policy-Based Methods)**

Estos m√©todos aprenden directamente la **pol√≠tica** (la estrategia o mapeo de estados a acciones) sin necesidad de calcular expl√≠citamente el valor de cada estado o acci√≥n. Son muy potentes para **entornos continuos o de alta dimensi√≥n**, como la rob√≥tica, donde el n√∫mero de acciones posibles es infinito o muy grande.

**Algoritmos y Ejemplos**

- **REINFORCE (o Monte-Carlo Policy Gradient):** Un algoritmo fundamental que ajusta los par√°metros de la pol√≠tica (a menudo una red neuronal) en funci√≥n del resultado final de un episodio completo (por ejemplo, una partida completa de un juego).
    - **Ejemplo Pr√°ctico:** Ense√±ar a un **brazo rob√≥tico** a realizar una tarea de control motor fino, como "alcanzar y agarrar" un objeto. La acci√≥n es un valor continuo (ej. √°ngulo exacto del motor), no discreto. REINFORCE ajusta los par√°metros de la red neuronal que
    controla los √°ngulos para mejorar la probabilidad de agarres exitosos.
- **PPO (Proximal Policy Optimization):** Un algoritmo moderno muy popular y estable, utilizado por organizaciones como OpenAI, que optimiza la pol√≠tica mediante gradiente de manera eficiente y segura.
    - **Ejemplo Pr√°ctico:** Entrenar a agentes de IA para **jugar videojuegos complejos** como Dota 2 o a **robots humanoides** para caminar y correr de forma fluida en terrenos irregulares, tareas que requieren una coordinaci√≥n continua y compleja.

### Modelos Actor-Critic (Actor-Critic Models)

Estos m√©todos combinan las fortalezas de los enfoques anteriores, utilizando dos componentes que trabajan juntos: un **Actor** (que decide la acci√≥n, basado en pol√≠ticas) y un **Cr√≠tico** (que eval√∫a la acci√≥n, basado en valores).

**Algoritmos y Ejemplos**

- **A2C (Advantage Actor-Critic):** Una implementaci√≥n eficiente y s√≠ncrona del marco Actor-Critic.
    - **Ejemplo Pr√°ctico:** **Optimizaci√≥n de procesos industriales** o sistemas de **control de tr√°fico**. El Actor decide par√°metros de control (ej. temperatura del horno, tiempo del sem√°foro) y el Cr√≠tico eval√∫a la eficiencia energ√©tica o la fluidez del tr√°fico resultante, proporcionando una se√±al de error para mejorar la pol√≠tica del actor de forma continua.
- **SAC (Soft Actor-Critic):** Un algoritmo avanzado que incorpora un componente de entrop√≠a para fomentar la exploraci√≥n, resultando en un aprendizaje muy robusto y estable.
    - **Ejemplo Pr√°ctico:** **Rob√≥tica avanzada para entornos del mundo real**, como un robot de almac√©n que navega entre personas y obst√°culos din√°micos, donde necesita una pol√≠tica fiable (explotaci√≥n) pero tambi√©n la flexibilidad para adaptarse a cambios inesperados (exploraci√≥n).

## Desafios del Aprendizaje por Refuerzo

### El Dilema Exploraci√≥n / Explotaci√≥n

Este es el desaf√≠o fundamental del RL. El agente debe decidir constantemente entre dos estrategias opuestas:¬†

- **Explotaci√≥n:** Utilizar el conocimiento actual para elegir la acci√≥n que sabe que proporciona la mayor recompensa inmediata o esperada a largo plazo.
    - (Ejemplo: Seguir comiendo en tu restaurante favorito que sabes que es bueno).
- **Exploraci√≥n:** Probar acciones nuevas o desconocidas para descubrir informaci√≥n nueva sobre el entorno que podr√≠a llevar a recompensas a√∫n mayores en el futuro.
    - (Ejemplo: Probar un restaurante nuevo que podr√≠a ser mejor que tu favorito, pero tambi√©n podr√≠a ser decepcionante).

> Un enfoque demasiado **exploratorio** hace que el aprendizaje sea lento e ineficiente a corto plazo. Un enfoque demasiado **explotador** puede hacer que el agente se quede "atascado" en un √≥ptimo local (una soluci√≥n buena, pero no la mejor globalmente).
> 

*‚ÄúPor ejemplo un agente jugando al ajedrez que solo explota sus conocimientos iniciales podr√≠a aprender una apertura b√°sica y nunca desviarse de ella, perdiendo la oportunidad de descubrir estrategias m√°s potentes que nunca ha explorado.¬†‚Äù*

**Formas de Evitarlo (Soluciones)**¬†

- **Estrategia √âpsilon-Glotona (œµepsilonùúñ-greedy):** Es la soluci√≥n m√°s simple. El agente explota (elige la mejor acci√≥n conocida) la mayor parte del tiempo, pero con una peque√±a probabilidad œµepsilonùúñ (ej. 10%), elige una acci√≥n completamente al azar para explorar. La epsilon suele disminuir con el tiempo a medida que el agente aprende m√°s.
- **Motivaci√≥n Intr√≠nseca (Curiosidad):** El agente recibe una "recompensa interna" o **bonificaci√≥n de exploraci√≥n** por visitar estados novedosos o poco conocidos, lo que le incentiva a explorar √°reas inexploradas del entorno por s√≠ mismo, sin depender solo de la recompensa externa.
- **Algoritmos Avanzados (PPO, UCB):** Algoritmos como la Optimizaci√≥n de Pol√≠ticas Proximas (**PPO**) o Upper Confidence Bound (**UCB**) incorporan mecanismos m√°s sofisticados para medir la incertidumbre y priorizar la exploraci√≥n de acciones con alto potencial de recompensa, en lugar de acciones puramente aleatorias.

### Escalabilidad (Large State & Action Spaces)

La escalabilidad se refiere a la capacidad de los algoritmos de RL para manejar entornos con un n√∫mero gigantesco (o infinito/continuo) de estados y acciones.¬†

- **Problema:** Los m√©todos tradicionales basados en tablas (como el Q-Learning tabular) simplemente no pueden funcionar cuando hay millones de estados (ej. cada frame de un videojuego en alta definici√≥n) o acciones continuas (ej. los √°ngulos exactos de 20 articulaciones de un robot). El entrenamiento requiere recursos computacionales abrumadores y una cantidad de datos ingente.

**Formas de Evitarlo (Soluciones)**¬†

- **Aproximaci√≥n de Funciones (Deep Learning):** El avance clave para la escalabilidad fue el uso de **redes neuronales profundas (Deep RL)**. En lugar de almacenar el valor de cada estado/acci√≥n individualmente, la red neuronal *aprende a generalizar* y estimar los valores o las pol√≠ticas a partir de los datos de entrada brutos (como p√≠xeles de una imagen).
- **Computaci√≥n Paralela/Distribuida:** El uso de plataformas de computaci√≥n en la nube permite distribuir la carga de trabajo y entrenar agentes de forma m√°s eficiente en entornos complejos, como se menciona en la documentaci√≥n de [Tencent Cloud](https://www.tencentcloud.com/techpedia/106696).

### Estabilidad y Eficiencia de Datos

El entrenamiento de algoritmos de RL, especialmente los que usan *deep learning*, es notoriamente **inestable** y sensible a los hiperpar√°metros.¬†
Los datos generados durante el aprendizaje est√°n altamente correlacionados (un paso sigue al anterior) y no son estacionarios (la distribuci√≥n de los datos cambia constantemente a medida que el agente mejora su pol√≠tica). Esto dificulta la convergencia y puede llevar a que un agente "olvide" pol√≠ticas que funcionaban bien (deterioro del rendimiento).¬†

**Formas de Evitarlo (Soluciones)**¬†

- **Experience Replay (Memoria de Experiencias):** Una t√©cnica fundamental introducida en DQN. El agente almacena sus experiencias (transiciones estado-acci√≥n-recompensa-nuevo estado) en un b√∫fer de memoria. Durante el entrenamiento, muestrea aleatoriamente lotes de estas experiencias pasadas, rompiendo la correlaci√≥n secuencial y haciendo que los datos sean m√°s estables para el entrenamiento de la red neuronal.
- **Redes Objetivo (Target Networks):** Para los m√©todos basados en valor, se usa una segunda red neuronal "objetivo" congelada temporalmente para calcular los valores futuros, proporcionando un objetivo de aprendizaje estable durante cientos o miles de pasos antes de ser actualizada, lo que reduce la inestabilidad.
- **Optimizaci√≥n de Hiperpar√°metros y Arquitecturas Robustas:** Algoritmos como **PPO** fueron dise√±ados espec√≠ficamente para ser m√°s robustos y menos sensibles a la sintonizaci√≥n fina de par√°metros (hiperpar√°metros), mejorando la estabilidad general del proceso de entrenamiento.

## Aplicaciones del Aprendizaje por refuerzo

El **aprendizaje por refuerzo (RL)** ha evolucionado m√°s all√° de la academia para convertirse en una tecnolog√≠a con aplicaciones impactantes en m√∫ltiples industrias, permitiendo a los sistemas tomar decisiones complejas en entornos din√°micos.

### Videojuegos e IA Maestra

El campo de los videojuegos ha sido el banco de pruebas principal para el RL, logrando hitos que superan las capacidades humanas.

- **AlphaGo y AlphaStar (DeepMind):** Agentes que aprendieron a jugar y **derrotaron a los campeones mundiales** en juegos de mesa complejos como Go y StarCraft II, partiendo de cero conocimiento y aprendiendo √∫nicamente mediante prueba y error y jugando
contra s√≠ mismos.
- **Juegos de Atari:** Los primeros grandes √©xitos del *Deep Reinforcement Learning* (DQN) demostraron que una sola arquitectura de IA pod√≠a aprender a jugar docenas de juegos cl√°sicos de Atari (como Pac-Man o Pong) a niveles sobrehumanos simplemente a partir de la entrada de p√≠xeles en pantalla.

### Rob√≥tica y Sistemas Aut√≥nomos

El RL permite a los robots aprender habilidades motoras finas y navegar en el mundo real sin una programaci√≥n manual exhaustiva de cada movimiento.

- **Brazos Rob√≥ticos:** Se utilizan algoritmos de RL para ense√±ar a los brazos rob√≥ticos a
realizar tareas delicadas como agarrar objetos de formas y tama√±os variados, o ensamblar componentes industriales con precisi√≥n.
- **Navegaci√≥n Aut√≥noma:** En rob√≥tica m√≥vil y **veh√≠culos aut√≥nomos** (como los sistemas de autopiloto de Tesla), el RL ayuda a optimizar la detecci√≥n de obst√°culos, la planificaci√≥n de rutas y la toma de decisiones de conducci√≥n en tiempo real para evitar colisiones y llegar a destinos.

### Optimizaci√≥n Industrial y Eficiencia Energ√©tica

En  entornos industriales, el RL se aplica para optimizar procesos complejos donde una peque√±a mejora en la eficiencia puede generar grandes ahorros.

- **Gesti√≥n de Centros de Datos de Google:** Google DeepMind utiliz√≥ RL para optimizar el **control de los sistemas de refrigeraci√≥n** de sus centros de datos, lo que result√≥ en una reducci√≥n significativa del consumo de energ√≠a (hasta un 40% en algunas √°reas) al ajustar la
temperatura, la ventilaci√≥n y las bombas de manera √≥ptima.
- **Optimizaci√≥n de la Cadena de Suministro:** Las empresas de log√≠stica utilizan RL para optimizar rutas de entrega, gesti√≥n de inventarios y operaciones de almac√©n, maximizando la
eficiencia y minimizando costes.

# Introducci√≥n al Aprendizaje generativo

El aprendizaje generativo es una rama del aprendizaje autom√°tico que se enfoca en la generaci√≥n de nuevos datos a partir de modelos que aprenden las distribuciones subyacentes en los datos de entrenamiento. A diferencia de los modelos discriminativos, que se enfocan en clasificar o predecir etiquetas a partir de caracter√≠sticas observadas, los modelos generativos intentan capturar la estructura y las dependencias en los datos, lo que les permite generar muestras nuevas que son consistentes con las observaciones originales.

## Definicion y concepto b√°sico

El aprendizaje generativo implica el uso de modelos de *machine learning* para aprender los patrones y la distribuci√≥n subyacente de un conjunto de datos de entrenamiento. Su objetivo principal es generar nuevos datos que sean similares a los datos originales, pero que no sean copias exactas.

El concepto se basa en modelar c√≥mo se generan los datos. Una vez que el modelo "entiende" la estructura de los datos (por ejemplo, la gram√°tica del espa√±ol, la composici√≥n de caras humanas, o la estructura del c√≥digo de programaci√≥n), puede producir muestras sint√©ticas, creativas y 
realistas a demanda.

## Diferencia entre modelos Generativos y Discriminatorios

Los modelos discriminativos aprenden la distribuci√≥n condicional P(Y|X), lo que significa que est√°n dise√±ados para predecir una etiqueta o salida basada en las caracter√≠sticas de entrada. Este enfoque los hace m√°s adecuados para tareas de clasificaci√≥n y predicci√≥n directa. Los modelos discriminativos son m√°s sencillos de entrenar y suelen ser m√°s eficientes en t√©rminos de precisi√≥n cuando se trata de problemas de clasificaci√≥n espec√≠ficos.

| **Caracter√≠stica** | **Modelos Generativos** | **Modelos Discriminativos** |
| --- | --- | --- |
| **Objetivo principal** | Crear o sintetizar nuevos datos y contenido (im√°genes, texto, audio). | Clasificar o predecir etiquetas para datos existentes. |
| **Lo que aprenden** | Aprenden la distribuci√≥n completa de los datos para entender c√≥mo se generan las caracter√≠sticas y las etiquetas conjuntamente (P(datos, etiquetas)). | Aprenden el l√≠mite de decisi√≥n entre clases; la probabilidad de una etiqueta dado un dato espec√≠fico (P(etiqueta \mid dato)). |
| **Uso t√≠pico** | Chatbots, asistentes de escritura, creaci√≥n de arte digital, traducci√≥n autom√°tica. | Filtros de spam, detecci√≥n de fraude, reconocimiento de objetos en im√°genes, diagn√≥stico de enfermedades. |
| **Ejemplos de algoritmos** | Redes Generativas Antag√≥nicas (GANs), Modelos de Lenguaje Grande (LLMs como GPT), Autoencoders Variacionales (VAEs). | Regresi√≥n Log√≠stica, M√°quinas de Vectores Soporte (SVM), la mayor√≠a de las Redes Neuronales Convolucionales (CNNs) usadas para clasificaci√≥n. |

# Modelos Probabilisticos y Deterministas

Existen dos enfoques principales en el aprendizaje generativo, diferenciados por c√≥mo incorporan la aleatoriedad y la incertidumbre en el proceso de creaci√≥n de nuevos datos: el probabil√≠stico y el determinista.

## Modelos Generativos Probabil√≠sticos

Estos modelos se centran en **aprender y modelar expl√≠citamente la distribuci√≥n de probabilidad subyacente** (P(X)) de los datos de entrada. La generaci√≥n de datos es esencialmente un proceso de muestreo a partir de esta distribuci√≥n aprendida.

El objetivo es capturar las propiedades estad√≠sticas de los datos para poder generar nuevas muestras que sigan fielmente esa misma distribuci√≥n. Se busca una representaci√≥n concisa en un espacio latente donde la distribuci√≥n es simple y manejable (a menudo una distribuci√≥n normal o gaussiana).

### **Variational Autoencoders (VAE)**

![image.png](Unit_6/image%205.png)

**Autoencoders:** 

Un **autoencoder** es un tipo de red neuronal entrenada para reproducir su entrada. Se compone de dos partes principales:

- **codificador**: La funci√≥n del codificador es reducir la dimensionalidad de los datos de entrada y comprimirlos en una representaci√≥n m√°s peque√±a, llamada **c√≥digo latente o espacio latente.**
 El codificador act√∫a como un extractor de caracter√≠sticas, identificando patrones o estructuras importantes dentro de los datos.
- **Decodificador:** toma el c√≥digo latente y lo transforma de nuevo en el espacio de los datos originales, tratando de reconstruir la entrada original lo m√°s fielmente posible. El objetivo del entrenamiento es minimizar la diferencia entre la entrada y la salida, normalmente utilizando una funci√≥n de p√©rdida como el **error cuadr√°tico medio (MSE).**

El proceso de aprendizaje de un autoencoder se basa en la compresi√≥n y posterior reconstrucci√≥n de la informaci√≥n. Aunque los autoencoders no se dise√±aron inicialmente como modelos generativos, pueden usarse para la generaci√≥n de datos nuevos al muestrear del espacio latente y decodificar esos puntos latentes de vuelta a datos completos. 

> En este sentido, los autoencoders permiten generar variaciones de los datos de entrada.
> 

**Variational Autoencoders (VAE)**

El **Autoencoder Variacional (VAE)** es un modelo generativo probabil√≠stico que evoluciona a partir del Autoencoder tradicional al introducir un enfoque basado en la distribuci√≥n de probabilidad en el espacio latente, se caracterizan por: 

**Representaci√≥n Latente Probabil√≠stica**

- **Distribuci√≥n, no un Punto:** A diferencia del Autoencoder cl√°sico, el VAE no codifica la entrada a un √∫nico punto z en el espacio latente, sino a una **distribuci√≥n probabil√≠stica** para cada entrada x.
- **Modelado:** El VAE aprende los par√°metros (Œº - media y œÉ - desviaci√≥n est√°ndar) de una **distribuci√≥n normal (Gaussiana)** que representa la entrada en el espacio latente.
- **Generaci√≥n:** Se introducen nuevas muestras al **muestrear aleatoriamente** puntos del espacio latente a partir de estas distribuciones, lo que asegura la variabilidad en la generaci√≥n.

**La Funci√≥n de P√©rdida (Loss Function)**

La funci√≥n de p√©rdida del VAE es cr√≠tica y se compone de dos t√©rminos que equilibran la calidad de la reconstrucci√≥n y la estructura del espacio latente:

1. **Error de Reconstrucci√≥n:** Mide la fidelidad con la que el decodificador logra reproducir la entrada original x. (Similar al Autoencoder tradicional).
2. **Divergencia de Kullback-Leibler (KL):**
    - Mide la diferencia entre la distribuci√≥n latente aprendida (posterior) y una **distribuci√≥n *prior*** predefinida (generalmente una Normal Est√°ndar).
    - **Prop√≥sito:** Este t√©rmino obliga al espacio latente a ser **continuo y suave**, asegurando que los puntos latentes est√©n bien agrupados y que el muestreo de nuevos datos sea coherente.

### VAE y GAN Diferencias

La diferencia fundamental reside en c√≥mo se estructura el proceso de aprendizaje y generaci√≥n.

| **Caracter√≠stica** | **VAE (Probabil√≠stico)** | **GAN (Determinista)** |
| --- | --- | --- |
| **Arquitectura** | Una sola red (Codificador + Decodificador). | Dos redes que compiten (Generador + Discriminador). |
| **Enfoque de Aprendizaje** | Aprende la **distribuci√≥n de probabilidad** expl√≠cita del espacio latente. | Aprende una **funci√≥n de mapeo** por medio de una competencia adversarial. |
| **Generaci√≥n de Muestras** | Se basa en el **muestreo probabil√≠stico** de la distribuci√≥n latente. | El generador transforma un ruido de entrada de forma **determinista**. |
| **Funci√≥n de P√©rdida** | Basada en **Divergencia KL** + **Error de Reconstrucci√≥n**. | Basada en la **Teor√≠a de Juegos** (p√©rdida adversarial). |
| **Fidelidad** | Tiende a producir muestras con buena **coherencia y estructura**, facilitando la interpolaci√≥n. | Tiende a producir muestras con **mayor realismo visual y nitidez** (Estado del arte en generaci√≥n de im√°genes). |

## Modelos Generativos Deterministas

Estos modelos se centran en la **transformaci√≥n directa** de datos de entrada (generalmente ruido aleatorio) en datos coherentes, utilizando un proceso que, aunque produce resultados variables, no se basa en el modelado expl√≠cito de una distribuci√≥n de probabilidad.

El modelo aprende una **funci√≥n de mapeo complejo** (G) que transforma un vector de ruido inicial (z) en una muestra de datos deseada (x). La aleatoriedad se introduce en el *input* (z), pero el proceso de transformaci√≥n interna (G) es una funci√≥n fija y determinista definida por los pesos de la red neuronal.

### **Redes Generativas Antagonistas (GAN)**

![image.png](Unit_6/image%206.png)

Consisten en dos redes que compiten: un **Generador** (G) y un **Discriminador** (D). El generador intenta crear muestras realistas a partir del ruido, mientras que el discriminador intenta distinguir entre las muestras reales y las generadas.

Son reconocidas por su capacidad para generar muestras con un **realismo visual excepcional**.

El generador toma el ruido inicial y lo transforma a trav√©s de capas neuronales. El resultado final es determinado por la funci√≥n aprendida por la red, no por un muestreo probabil√≠stico del espacio latente.

**Arquitectura de las GAN**

Las GAN son una clase de modelos generativos que utilizan una estructura de **Teor√≠a de Juegos** (Antag√≥nica) para el entrenamiento. Se componen de dos redes neuronales que compiten y se mejoran mutuamente: el **Generador** y el **Discriminador**.

| **Red** | **Rol** | **Objetivo** |
| --- | --- | --- |
| **Generador (G)** | **Creador de Falsificaciones.** Toma una entrada de **ruido aleatorio** y la transforma, capa tras capa, en una muestra de datos sint√©ticos (ej. una imagen). | **Enga√±ar al Discriminador.** Trata de **maximizar** la probabilidad de que el Discriminador clasifique su salida como "real". |
| **Discriminador (D)** | **Clasificador Binario.** Recibe datos reales (entrenamiento) y datos falsos (del Generador). | **Clasificar Correctamente.** Trata de **maximizar** su precisi√≥n al clasificar datos reales como reales y datos generados como falsos. |

**Proceso de Entrenamiento Iterativo**

El entrenamiento de una GAN es un juego de suma cero donde ambas redes se entrenan de forma alternativa en ciclos continuos, buscando un **Equilibrio de Nash**.

- **Paso del Generador:** El Generador produce datos falsos a partir del ruido. Se le penaliza si el Discriminador identifica correctamente que los datos son falsos. Su objetivo es actualizar sus pesos para que sus pr√≥ximas muestras sean m√°s convincentes.
- **Paso del Discriminado:** que recibe un lote de datos reales y otro de datos falsos. Se le entrena para que sus clasificaciones sean lo m√°s precisas posible. Esto actualiza sus pesos para ser mejor detector de falsificaciones.

**Resultado Final (Equilibrio de Nash):** El proceso termina cuando el Generador es tan bueno que el Discriminador ya no puede distinguir los datos reales de los falsos con una precisi√≥n superior al azar (50%). En este punto, el Generador ha aprendido efectivamente a replicar la distribuci√≥n de los datos reales.

**Variantes Populares de las GAN**

Para superar problemas como la inestabilidad en el entrenamiento y permitir una generaci√≥n m√°s controlada, se han desarrollado diversas arquitecturas:

- **DCGAN (Deep Convolutional GAN):**
    - **Enfoque:** Utiliza **redes convolucionales profundas (CNN)** en lugar de redes totalmente conectadas.
    - **Aplicaci√≥n:** Mejor√≥ dr√°sticamente la estabilidad y la calidad en la generaci√≥n de **im√°genes**.
- **WGAN (Wasserstein GAN):**
    - **Enfoque:** Reemplaza la funci√≥n de p√©rdida tradicional por la **Distancia de Wasserstein**.
    - **Beneficio:** Proporciona un gradiente m√°s robusto y suave, lo que resulta en un **entrenamiento m√°s estable** y controlado.
- **GANs Condicionales (cGAN):**
    - **Enfoque:** Permite **controlar** la generaci√≥n proporcionando una **etiqueta o condici√≥n** (ej. "generar un gato" o "generar la imagen con esta descripci√≥n") como entrada tanto al Generador como al Discriminador.
- **CycleGAN:**
    - **Enfoque:** Permite la **traducci√≥n de im√°genes entre dominios** (ej. caballo a cebra) **sin necesidad de pares de entrenamiento** correspondientes.
    - **Aplicaci√≥n:** √ötil para transferencia de estilo y simulaci√≥n sin datos etiquetados.

## Aplicaciones del aprendizaje generativo

### Generaci√≥n de im√°genes

Los modelos generativos permiten crear im√°genes realistas o art√≠sticas a partir de ruido, texto u otras im√°genes.

**Aplicaciones principales:**

- Creaci√≥n de arte digital y dise√±o gr√°fico
- Generaci√≥n de rostros o escenarios sint√©ticos
- Restauraci√≥n y mejora de im√°genes (super-resoluci√≥n, colorizaci√≥n)
- Generaci√≥n de im√°genes a partir de texto (text-to-image)

**Modelos t√≠picos:** GAN, Diffusion Models

### Generaci√≥n de texto y m√∫sica

Los modelos generativos pueden producir **secuencias estructuradas** como texto, m√∫sica o c√≥digo.

**Aplicaciones en texto:**

- Chatbots y asistentes virtuales
- Redacci√≥n autom√°tica y resumen de documentos
- Traducci√≥n autom√°tica
- Generaci√≥n de c√≥digo

**Aplicaciones en m√∫sica y audio:**

- Composici√≥n musical autom√°tica
- Generaci√≥n de efectos de sonido
- S√≠ntesis de voz (text-to-speech)

**Modelos t√≠picos:** Transformers, LLMs, modelos autoregresivos

### Aumento de datos (Data Augmentation)

El aprendizaje generativo permite **crear datos sint√©ticos** para ampliar conjuntos de entrenamiento cuando los datos reales son escasos o costosos de obtener.

**Ventajas:**

- Reduce el sobreajuste
- Mejora la generalizaci√≥n del modelo
- Equilibra conjuntos de datos desbalanceados

**Ejemplo:** generar im√°genes adicionales para clases minoritarias en clasificaci√≥n.

### Mejora del entrenamiento de modelos

Los datos generados pueden usarse para **entrenar o preentrenar otros modelos**.

**Usos comunes:**

- Simulaci√≥n de entornos complejos
- Entrenamiento previo con datos sint√©ticos
- Evaluaci√≥n de modelos en escenarios poco frecuentes

Esto es habitual en rob√≥tica, conducci√≥n aut√≥noma y visi√≥n artificial.

### Im√°genes m√©dicas sint√©ticas

En el √°mbito sanitario, los modelos generativos permiten generar im√°genes m√©dicas artificiales que respetan la distribuci√≥n de datos reales.

**Aplicaciones:**

- Aumento de datos para entrenamiento de modelos diagn√≥sticos
- Protecci√≥n de la privacidad del paciente
- Simulaci√≥n de patolog√≠as poco frecuentes

**Ejemplos de im√°genes:** resonancias magn√©ticas, radiograf√≠as, TAC.

### Generaci√≥n de datos en finanzas

El aprendizaje generativo se emplea para crear **series temporales financieras sint√©ticas** que imitan el comportamiento real del mercado.

**Aplicaciones:**

- Simulaci√≥n de escenarios de riesgo
- Pruebas de estr√©s financiero
- Entrenamiento de modelos de detecci√≥n de fraude
- Evaluaci√≥n de estrategias de inversi√≥n

---