# ğŸ•ğŸ± Dogs vs. Cats 2025 - Notebook Optimizado

## ğŸ“‹ DescripciÃ³n del Proyecto

Este notebook implementa una soluciÃ³n completa y optimizada para la competiciÃ³n **[U-Tad] Dogs vs. Cats 2025** de Kaggle, utilizando tÃ©cnicas modernas de Deep Learning y Transfer Learning con PyTorch.

## ğŸ”„ Cambios Respecto a la VersiÃ³n Anterior

### 1. **MigraciÃ³n Completa de Framework**
- âŒ **Antes**: Keras/TensorFlow
- âœ… **Ahora**: PyTorch con timm y Albumentations
  - Mayor flexibilidad y control sobre el entrenamiento
  - Soporte nativo para mixed precision training
  - Mejor integraciÃ³n con modelos preentrenados modernos

### 2. **Modelo y Arquitectura**
- âŒ **Antes**: Modelo bÃ¡sico sin especificar
- âœ… **Ahora**: **EfficientNet-B3** preentrenado en ImageNet
  - Transfer learning desde ImageNet
  - Funciones para congelar/descongelar el backbone
  - Arquitectura eficiente y moderna

### 3. **Estrategia de ValidaciÃ³n**
- âŒ **Antes**: Train/validation split simple
- âœ… **Ahora**: **K-Fold Cross-Validation con 5 folds**
  - Mejor estimaciÃ³n del rendimiento real
  - Reduce el riesgo de overfitting
  - Ensemble de 5 modelos para predicciones mÃ¡s robustas

### 4. **Entrenamiento por Etapas**
- âŒ **Antes**: Entrenamiento en una sola etapa
- âœ… **Ahora**: **Dos etapas de entrenamiento**
  - **Etapa 1**: 5 epochs con backbone congelado (solo cabeza)
  - **Etapa 2**: 15 epochs de fine-tuning completo
  - Learning rates adaptativos (1e-3 â†’ 1e-4)

### 5. **Data Augmentation Mejorado**
- âŒ **Antes**: Augmentation bÃ¡sico o inexistente
- âœ… **Ahora**: **Albumentations con transformaciones modernas**
  - RandomResizedCrop (scale 0.8-1.0)
  - HorizontalFlip (p=0.5)
  - RandomBrightnessContrast (Â±0.2)
  - ShiftScaleRotate (shift=0.1, scale=0.1, rotate=15Â°)
  - NormalizaciÃ³n ImageNet

### 6. **Optimizaciones de Entrenamiento**
- âŒ **Antes**: Entrenamiento estÃ¡ndar
- âœ… **Ahora**: MÃºltiples optimizaciones
  - **Mixed Precision Training** (torch.cuda.amp) â†’ ~2x mÃ¡s rÃ¡pido
  - **AdamW optimizer** con weight_decay=1e-2
  - **Label Smoothing** (0.1) para mejor generalizaciÃ³n
  - **Early Stopping** (patience=5) para evitar overfitting
  - Guardado del mejor modelo por validation accuracy

### 7. **Inference y Ensemble**
- âŒ **Antes**: Predicciones de un solo modelo
- âœ… **Ahora**: **Ensemble de 5 modelos (K-Fold)**
  - Promedio de predicciones de los 5 folds
  - Mayor estabilidad y precisiÃ³n
  - ReducciÃ³n de la varianza

### 8. **Estructura y OrganizaciÃ³n**
- âŒ **Antes**: CÃ³digo bÃ¡sico sin estructura clara
- âœ… **Ahora**: **Notebook modular y bien organizado**
  - Secciones claramente definidas
  - Funciones reutilizables
  - Comentarios detallados en espaÃ±ol
  - Dataset y DataLoader personalizados

### 9. **Reproducibilidad**
- âŒ **Antes**: Sin control de semillas
- âœ… **Ahora**: **100% reproducible**
  - Todas las semillas fijadas (torch, numpy, cuda)
  - ConfiguraciÃ³n determinÃ­stica
  - Pin memory y workers configurados

### 10. **VisualizaciÃ³n y AnÃ¡lisis**
- âŒ **Antes**: Sin visualizaciÃ³n
- âœ… **Ahora**: **Visualizaciones incluidas**
  - FunciÃ³n para visualizar predicciones
  - EstadÃ­sticas detalladas del entrenamiento
  - Resumen de mÃ©tricas por fold

## ğŸ—ï¸ Estructura del Notebook

1. **ConfiguraciÃ³n**
   - Imports de PyTorch, timm, Albumentations
   - ConfiguraciÃ³n de seeds y device
   - HiperparÃ¡metros globales

2. **PreparaciÃ³n del Dataset**
   - CustomDataset de PyTorch
   - FunciÃ³n para cargar datos de entrenamiento
   - SeparaciÃ³n de imÃ¡genes y etiquetas

3. **Transformaciones y Data Augmentation**
   - Train transforms (con augmentation)
   - Validation/Test transforms (sin augmentation)

4. **Modelo con Transfer Learning**
   - CreaciÃ³n de EfficientNet-B3
   - Funciones freeze/unfreeze
   - ConfiguraciÃ³n para clasificaciÃ³n binaria

5. **Funciones de Entrenamiento y ValidaciÃ³n**
   - train_epoch con mixed precision
   - validate_epoch sin actualizaciÃ³n de pesos
   - MÃ©tricas (loss, accuracy)

6. **Entrenamiento con K-Fold**
   - StratifiedKFold (5 folds)
   - Loop por cada fold
   - Etapa 1: backbone congelado
   - Etapa 2: fine-tuning completo
   - Early stopping y guardado del mejor modelo

7. **Inferencia en Test Set**
   - TestDataset personalizado
   - Predicciones de los 5 folds
   - Promedio de predicciones

8. **GeneraciÃ³n de Submission**
   - CreaciÃ³n de submission.csv
   - Formato correcto para Kaggle
   - EstadÃ­sticas de las predicciones

9. **VisualizaciÃ³n (Opcional)**
   - FunciÃ³n para mostrar predicciones
   - VisualizaciÃ³n de confianza del modelo

10. **Conclusiones**
    - Resumen de tÃ©cnicas implementadas
    - Mejoras posibles
    - Checklist de features

## ğŸš€ TecnologÃ­as Utilizadas

- **PyTorch** - Framework principal
- **timm** - Transfer learning con modelos preentrenados
- **Albumentations** - Data augmentation avanzado
- **scikit-learn** - K-Fold cross-validation
- **pandas** - ManipulaciÃ³n de datos
- **matplotlib** - VisualizaciÃ³n

## ğŸ“Š HiperparÃ¡metros Principales

```python
CONFIG = {
    'model_name': 'efficientnet_b3',
    'img_size': 300,
    'num_classes': 2,
    'batch_size': 32,
    'num_folds': 5,
    'epochs_stage1': 5,      # Backbone congelado
    'epochs_stage2': 15,     # Fine-tuning completo
    'lr_stage1': 1e-3,
    'lr_stage2': 1e-4,
    'weight_decay': 1e-2,
    'label_smoothing': 0.1,
    'num_workers': 2
}
```

## ğŸ¯ CaracterÃ­sticas Clave

âœ… Transfer Learning con EfficientNet-B3  
âœ… K-Fold Cross-Validation (5 folds)  
âœ… Entrenamiento por etapas (congelado + fine-tuning)  
âœ… Data Augmentation moderado con Albumentations  
âœ… Mixed Precision Training (AMP)  
âœ… Label Smoothing (0.1)  
âœ… AdamW optimizer con weight decay  
âœ… Early Stopping (patience=5)  
âœ… Ensemble de predicciones (promedio de 5 folds)  
âœ… 100% reproducible (seeds fijadas)  
âœ… CÃ³digo limpio y comentado  

## ğŸ”§ Requisitos

```bash
torch>=2.0.0
torchvision>=0.15.0
timm>=0.9.0
albumentations>=1.3.0
scikit-learn>=1.3.0
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
Pillow>=10.0.0
```

## ğŸ’» Uso

1. **En Kaggle**:
   - Crear un nuevo notebook
   - Copiar todo el cÃ³digo
   - Asegurarse de tener GPU activada
   - Ejecutar todas las celdas
   - Descargar `submission.csv`

2. **Localmente**:
   - Instalar dependencias
   - Ajustar rutas de datos en CONFIG
   - Ejecutar el notebook
   - Generar submission.csv

## ğŸ“ˆ Mejoras Futuras Posibles

- [ ] Usar EfficientNet-B4 o B5 (mÃ¡s precisiÃ³n)
- [ ] Aumentar epochs en etapa 2
- [ ] Probar SGD con momentum
- [ ] AÃ±adir CutOut, MixUp, CutMix
- [ ] Test Time Augmentation (TTA)
- [ ] Probar otros modelos (ResNet, ConvNeXt)
- [ ] Ajuste fino de learning rate con scheduler

## ğŸ‘¤ Autor

**Ismael Hormigo Castro** - U-Tad 2025

## ğŸ“… Fecha

30 de octubre de 2025

## ğŸ“ Notas

Este notebook estÃ¡ diseÃ±ado para ser ejecutado en el entorno de Kaggle GPU sin necesidad de dependencias manuales. Todo el cÃ³digo es reproducible y estÃ¡ optimizado para maximizar la precisiÃ³n en la competiciÃ³n Dogs vs. Cats.

---

**Â¡Listo para subir a Kaggle y conseguir un buen score! ğŸ¯**