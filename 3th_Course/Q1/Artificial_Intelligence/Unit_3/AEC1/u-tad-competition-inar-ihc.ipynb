{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR                                ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        29/10/2025  -  23:00:15           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    30/10/2025  -  02:55:40           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gatos vs Perretes \n",
    "\n",
    "idea de diseño: \n",
    "- **Transfer Learning** con EfficientNet-B3 igual que el video este -> (https://www.youtube.com/watch?v=fCtMf6qHtdk)\n",
    "- **K-Fold con validación cruzada** (5 folds) para mejor generalización\n",
    "- **Entrenamiento por etapas**: primero solo la cabeza, luego fine-tuning completo\n",
    "- **Data Augmentation** con Albumentations\n",
    "- **Mixed Precision Training** para acelerar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Importo las librerías\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch y movidas varias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# modelos preentrenados con transfer\n",
    "import timm\n",
    "\n",
    "# Data augmentation y transformaciones\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# K-Fold validación cruzada \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# fijo todas las semillas\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Para usar las graficas de kagle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración Global\n",
    "**[v1.3 - 30/10/2025 00:45 AM]** - Actualizado con Mixed Precision y AdamW\n",
    "\n",
    "Aquí defino los hiperparámetros principales del modelo y las rutas de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuración de parámetros y rutas\n",
    "CONFIG = {\n",
    "    'train_dir': '/kaggle/input/u-tad-dogs-vs-cats-2025/train/train',\n",
    "    'test_dir': '/kaggle/input/u-tad-dogs-vs-cats-2025/test/test',\n",
    "    'supplementary_dir': '/kaggle/input/u-tad-dogs-vs-cats-2025/supplementary_data/supplementary_data',\n",
    "    \n",
    "    'model_name': 'efficientnet_b3',\n",
    "    'img_size': 300,\n",
    "    'num_classes': 2,\n",
    "    \n",
    "    'batch_size': 32,\n",
    "    'num_folds': 5,\n",
    "    'epochs_stage1': 5,\n",
    "    'epochs_stage2': 15,\n",
    "    'lr_stage1': 1e-3,\n",
    "    'lr_stage2': 1e-4,\n",
    "    'weight_decay': 1e-2,\n",
    "    'label_smoothing': 0.1,\n",
    "    \n",
    "    'num_workers': 2,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"Configuración cargada:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del Dataset\n",
    "\n",
    "Creo un dataset personalizado de PyTorch y preparo los datos para validación cruzada K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset personalizado\n",
    "class DogsVsCatsDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Preparo los datos de entrenamiento\n",
    "def prepare_data(train_dir):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for filename in os.listdir(train_dir):\n",
    "        if filename.endswith('.jpg'):\n",
    "            filepath = os.path.join(train_dir, filename)\n",
    "            image_paths.append(filepath)\n",
    "            label = 0 if filename.startswith('cat') else 1\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(image_paths), np.array(labels)\n",
    "\n",
    "train_paths, train_labels = prepare_data(CONFIG['train_dir'])\n",
    "print(f\"Datos cargados: {len(train_paths)} imágenes\")\n",
    "print(f\"   - Gatos: {(train_labels == 0).sum()}\")\n",
    "print(f\"   - Perros: {(train_labels == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones y Data Augmentation\n",
    "**[v1.5 - 30/10/2025 02:20 AM]** - Augmentation mejorado con ShiftScaleRotate\n",
    "\n",
    "Defino las transformaciones de entrenamiento con augmentation y las de validación/test sin augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones de entrenamiento con augmentation\n",
    "train_transforms = A.Compose([\n",
    "    A.RandomResizedCrop(height=CONFIG['img_size'], width=CONFIG['img_size'], scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Transformaciones de validación/test sin augmentation\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=CONFIG['img_size'], width=CONFIG['img_size']),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"Transformaciones definidas\")\n",
    "print(f\"   - Train: RandomCrop, HorizontalFlip, Brightness/Contrast, Rotation\")\n",
    "print(f\"   - Val/Test: Solo resize y normalización\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con Transfer Learning\n",
    "**[v1.0 - 29/10/2025 23:15 PM]** - Implementado Transfer Learning con EfficientNet-B3\n",
    "\n",
    "Cargo EfficientNet-B3 preentrenado y lo adapto para clasificación binaria (perros vs gatos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo modelo con transfer learning\n",
    "def create_model(model_name, num_classes, pretrained=True):\n",
    "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# Congelo el backbone para etapa 1\n",
    "def freeze_backbone(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name:\n",
    "            param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# Descongelo todo para etapa 2\n",
    "def unfreeze_backbone(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "model = create_model(CONFIG['model_name'], CONFIG['num_classes'], pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Modelo creado: {CONFIG['model_name']}\")\n",
    "print(f\"   - Parámetros totales: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   - Parámetros entrenables: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Entrenamiento y Validación\n",
    "**[v1.3 - 30/10/2025 00:45 AM]** - Añadido Mixed Precision Training (AMP)\n",
    "\n",
    "Implemento las funciones para entrenar y validar el modelo con mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento con mixed precision\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Función de validación\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"Funciones de entrenamiento y validación definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con K-Fold Cross-Validation\n",
    "\n",
    "commits Hechos:\n",
    "\n",
    "**[v1.1 - 30/10/2025 00:10 AM]** - Implementado K-Fold (5 folds)  \n",
    "**[v1.2 - 30/10/2025 00:30 AM]** - Añadido Early Stopping y Label Smoothing\n",
    "\n",
    "Entreno el modelo con 5 folds y 2 etapas por fold:\n",
    "1. **Etapa 1**: Solo entreno la cabeza con el backbone congelado\n",
    "2. **Etapa 2**: Fine-tuning completo descongelando todo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "fold_models = []\n",
    "fold_metrics = []\n",
    "\n",
    "print(f\"Iniciando entrenamiento con {CONFIG['num_folds']} folds\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_paths, train_labels)):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"FOLD {fold + 1}/{CONFIG['num_folds']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    fold_train_paths = train_paths[train_idx]\n",
    "    fold_train_labels = train_labels[train_idx]\n",
    "    fold_val_paths = train_paths[val_idx]\n",
    "    fold_val_labels = train_labels[val_idx]\n",
    "    \n",
    "    train_dataset = DogsVsCatsDataset(fold_train_paths, fold_train_labels, transforms=train_transforms)\n",
    "    val_dataset = DogsVsCatsDataset(fold_val_paths, fold_val_labels, transforms=val_transforms)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                             shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                           shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    \n",
    "    model = create_model(CONFIG['model_name'], CONFIG['num_classes'], pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # ETAPA 1: entreno solo la cabeza\n",
    "    print(f\"\\nEtapa 1: Entrenando solo la cabeza ({CONFIG['epochs_stage1']} epochs)\")\n",
    "    model = freeze_backbone(model)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                           lr=CONFIG['lr_stage1'], weight_decay=CONFIG['weight_decay'])\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs_stage1']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs_stage1']} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    # ETAPA 2: fine-tuning completo\n",
    "    print(f\"\\nEtapa 2: Fine-tuning completo ({CONFIG['epochs_stage2']} epochs)\")\n",
    "    model = unfreeze_backbone(model)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr_stage2'], \n",
    "                           weight_decay=CONFIG['weight_decay'])\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience_limit = 5\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs_stage2']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs_stage2']} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"   -> Nuevo mejor modelo (Val Acc: {val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience_limit:\n",
    "            print(f\"   -> Early stopping en epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    fold_models.append(model)\n",
    "    fold_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} completado - Mejor Val Acc: {best_val_acc:.2f}%\\n\")\n",
    "\n",
    "# Resumen de todos los folds\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUMEN DE ENTRENAMIENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "for metric in fold_metrics:\n",
    "    print(f\"Fold {metric['fold']}: Val Acc = {metric['best_val_acc']:.2f}%\")\n",
    "\n",
    "avg_val_acc = np.mean([m['best_val_acc'] for m in fold_metrics])\n",
    "print(f\"\\nAccuracy promedio en validación: {avg_val_acc:.2f}%\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia en Test Set\n",
    "\n",
    "Cargo las imágenes de test y genero predicciones promediando los 5 modelos de los folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de test\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transforms=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = sorted([f for f in os.listdir(test_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.test_dir, img_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        img_id = int(img_name.split('.')[0])\n",
    "        return image, img_id\n",
    "\n",
    "test_dataset = TestDataset(CONFIG['test_dir'], transforms=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], \n",
    "                         shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "print(f\"Dataset de test cargado: {len(test_dataset)} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero predicciones promediando los 5 folds\n",
    "print(\"Generando predicciones...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_ids = []\n",
    "\n",
    "for model in fold_models:\n",
    "    model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, img_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        fold_preds = []\n",
    "        for model in fold_models:\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            fold_preds.append(probs.cpu().numpy())\n",
    "        \n",
    "        avg_probs = np.mean(fold_preds, axis=0)\n",
    "        all_predictions.extend(avg_probs)\n",
    "        all_ids.extend(img_ids.numpy())\n",
    "\n",
    "print(f\"Predicciones generadas: {len(all_predictions)} imágenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del archivo Submission\n",
    "\n",
    "Creo el archivo `submission.csv` con las predicciones finales para subir a Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': all_ids,\n",
    "    'label': all_predictions\n",
    "})\n",
    "\n",
    "submission_df = submission_df.sort_values('id').reset_index(drop=True)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Archivo submission.csv generado correctamente\")\n",
    "print(f\"\\nPrimeras predicciones:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nEstadísticas de las predicciones:\")\n",
    "print(f\"   - Media: {submission_df['label'].mean():.4f}\")\n",
    "print(f\"   - Desviación estándar: {submission_df['label'].std():.4f}\")\n",
    "print(f\"   - Mínimo: {submission_df['label'].min():.4f}\")\n",
    "print(f\"   - Máximo: {submission_df['label'].max():.4f}\")\n",
    "print(f\"\\nTotal de predicciones: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de Predicciones (mi bonus)\n",
    "\n",
    "Muestro algunas imágenes del test set con sus predicciones para verificar visualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo predicciones aleatorias\n",
    "def visualize_predictions(num_images=9):\n",
    "    random_indices = np.random.choice(len(submission_df), size=num_images, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, idx in enumerate(random_indices):\n",
    "        img_id = submission_df.iloc[idx]['id']\n",
    "        prediction = submission_df.iloc[idx]['label']\n",
    "        \n",
    "        img_path = os.path.join(CONFIG['test_dir'], f\"{img_id}.jpg\")\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        predicted_class = \"Perro\" if prediction > 0.5 else \"Gato\"\n",
    "        confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"ID: {img_id}\\n{predicted_class} ({confidence*100:.1f}%)\", \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(num_images=9)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13710467,
     "sourceId": 114912,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
