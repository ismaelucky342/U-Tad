{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR                                ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        29/10/2025  -  23:00:15           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    30/10/2025  -  02:55:40           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gatos vs Perretes \n",
    "\n",
    "idea de diseño: \n",
    "- **Transfer Learning** con EfficientNet-B3 igual que el video este -> (https://www.youtube.com/watch?v=fCtMf6qHtdk)\n",
    "- **K-Fold con validación cruzada** (5 folds) para mejor generalización\n",
    "- **Entrenamiento por etapas**: primero solo la cabeza, luego fine-tuning completo\n",
    "- **Data Augmentation** con Albumentations\n",
    "- **Mixed Precision Training** para acelerar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "# Importo las librerías\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch y movidas varias\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# modelos preentrenados con transfer\n",
    "import timm\n",
    "\n",
    "# Data augmentation y transformaciones\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# K-Fold validación cruzada \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# fijo todas las semillas\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Para usar las graficas de kagle\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuración Global\n",
    "**[v1.3 - 30/10/2025 00:45 AM]** - Actualizado con Mixed Precision y AdamW\n",
    "\n",
    "Aquí defino los hiperparámetros principales del modelo y las rutas de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuración cargada:\n",
      "   train_dir: /kaggle/input/u-tad-dogs-vs-cats-2025/train/train\n",
      "   test_dir: /kaggle/input/u-tad-dogs-vs-cats-2025/test/test\n",
      "   supplementary_dir: /kaggle/input/u-tad-dogs-vs-cats-2025/supplementary_data/supplementary_data\n",
      "   model_name: efficientnet_b3\n",
      "   img_size: 300\n",
      "   num_classes: 2\n",
      "   batch_size: 32\n",
      "   num_folds: 5\n",
      "   epochs_stage1: 5\n",
      "   epochs_stage2: 15\n",
      "   lr_stage1: 0.001\n",
      "   lr_stage2: 0.0001\n",
      "   weight_decay: 0.01\n",
      "   label_smoothing: 0.1\n",
      "   num_workers: 2\n",
      "   seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuración de parámetros y rutas\n",
    "CONFIG = {\n",
    "    'train_dir': '/kaggle/input/u-tad-dogs-vs-cats-2025/train/train',\n",
    "    'test_dir': '/kaggle/input/u-tad-dogs-vs-cats-2025/test/test',\n",
    "    'supplementary_dir': '/kaggle/input/u-tad-dogs-vs-cats-2025/supplementary_data/supplementary_data',\n",
    "    \n",
    "    'model_name': 'efficientnet_b3',\n",
    "    'img_size': 300,\n",
    "    'num_classes': 2,\n",
    "    \n",
    "    'batch_size': 32,\n",
    "    'num_folds': 5,\n",
    "    'epochs_stage1': 5,\n",
    "    'epochs_stage2': 15,\n",
    "    'lr_stage1': 1e-3,\n",
    "    'lr_stage2': 1e-4,\n",
    "    'weight_decay': 1e-2,\n",
    "    'label_smoothing': 0.1,\n",
    "    \n",
    "    'num_workers': 2,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"Configuración cargada:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNÓSTICO: Verificar rutas disponibles en Kaggle\n",
    "# ============================================================================\n",
    "# Ejecuta esta celda primero para ver qué datasets están disponibles\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"Verificando estructura de directorios en Kaggle...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Verifico si estamos en Kaggle\n",
    "if os.path.exists('/kaggle/input'):\n",
    "    print(\"Estamos en Kaggle!\")\n",
    "    print(\"\\nDatasets disponibles en /kaggle/input/:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for dataset in os.listdir('/kaggle/input'):\n",
    "        dataset_path = os.path.join('/kaggle/input', dataset)\n",
    "        print(f\"\\n[Dataset: {dataset}]\")\n",
    "        \n",
    "        # Listo el contenido del dataset\n",
    "        try:\n",
    "            contents = os.listdir(dataset_path)\n",
    "            for item in contents[:10]:  # Muestro solo los primeros 10 items\n",
    "                item_path = os.path.join(dataset_path, item)\n",
    "                if os.path.isdir(item_path):\n",
    "                    print(f\"  [DIR]  {item}/\")\n",
    "                    # Muestro contenido de subdirectorios\n",
    "                    try:\n",
    "                        subitems = os.listdir(item_path)\n",
    "                        print(f\"         Contiene {len(subitems)} archivos/carpetas\")\n",
    "                        if len(subitems) <= 5:\n",
    "                            for subitem in subitems:\n",
    "                                print(f\"         - {subitem}\")\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    print(f\"  [FILE] {item}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error al listar: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"\\nINSTRUCCIONES:\")\n",
    "    print(\"1. Identifica el nombre exacto del dataset arriba\")\n",
    "    print(\"2. Actualiza CONFIG['train_dir'] con la ruta correcta\")\n",
    "    print(\"   Ejemplo: '/kaggle/input/NOMBRE_DEL_DATASET/train/train'\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"NO estamos en Kaggle (entorno local)\")\n",
    "    print(\"\\nPara ejecutar localmente:\")\n",
    "    print(\"1. Descarga el dataset de Kaggle\")\n",
    "    print(\"2. Extráelo en una carpeta local\")\n",
    "    print(\"3. Actualiza CONFIG['train_dir'] con la ruta local\")\n",
    "    print(\"   Ejemplo: '/home/usuario/datasets/dogs-vs-cats/train/train'\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del Dataset\n",
    "\n",
    "Creo un dataset personalizado de PyTorch y preparo los datos para validación cruzada K-Fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CARGANDO DATOS DE ENTRENAMIENTO\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ERROR: El directorio de entrenamiento no existe: /kaggle/input/u-tad-dogs-vs-cats-2025/train/train\n   Por favor, verifica que:\n   1. Estás ejecutando en Kaggle (si usas paths de Kaggle)\n   2. O actualiza CONFIG['train_dir'] con la ruta local correcta\n   3. O descarga el dataset en la ubicación esperada",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCARGANDO DATOS DE ENTRENAMIENTO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m train_paths, train_labels = \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[OK] Datos cargados correctamente:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Total de imágenes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mprepare_data\u001b[39m\u001b[34m(train_dir)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Verifico que el directorio existe\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(train_dir):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     36\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mERROR: El directorio de entrenamiento no existe: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     37\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Por favor, verifica que:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   1. Estás ejecutando en Kaggle (si usas paths de Kaggle)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   2. O actualiza CONFIG[\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtrain_dir\u001b[39m\u001b[33m'\u001b[39m\u001b[33m] con la ruta local correcta\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   3. O descarga el dataset en la ubicación esperada\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m     )\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLeyendo imágenes desde: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m image_paths = []\n",
      "\u001b[31mFileNotFoundError\u001b[39m: ERROR: El directorio de entrenamiento no existe: /kaggle/input/u-tad-dogs-vs-cats-2025/train/train\n   Por favor, verifica que:\n   1. Estás ejecutando en Kaggle (si usas paths de Kaggle)\n   2. O actualiza CONFIG['train_dir'] con la ruta local correcta\n   3. O descarga el dataset en la ubicación esperada"
     ]
    }
   ],
   "source": [
    "# Dataset personalizado\n",
    "class DogsVsCatsDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# Preparo los datos de entrenamiento\n",
    "def prepare_data(train_dir):\n",
    "    \"\"\"\n",
    "    Carga las rutas de las imágenes y sus etiquetas desde el directorio de entrenamiento.\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Ruta al directorio con las imágenes de entrenamiento\n",
    "        \n",
    "    Returns:\n",
    "        image_paths: Array numpy con las rutas completas a las imágenes\n",
    "        labels: Array numpy con las etiquetas (0=gato, 1=perro)\n",
    "    \"\"\"\n",
    "    # Verifico que el directorio existe\n",
    "    if not os.path.exists(train_dir):\n",
    "        raise FileNotFoundError(\n",
    "            f\"ERROR: El directorio de entrenamiento no existe: {train_dir}\\n\"\n",
    "            f\"   Por favor, verifica que:\\n\"\n",
    "            f\"   1. Estás ejecutando en Kaggle (si usas paths de Kaggle)\\n\"\n",
    "            f\"   2. O actualiza CONFIG['train_dir'] con la ruta local correcta\\n\"\n",
    "            f\"   3. O descarga el dataset en la ubicación esperada\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Leyendo imágenes desde: {train_dir}\")\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Listo todos los archivos del directorio\n",
    "    all_files = os.listdir(train_dir)\n",
    "    jpg_files = [f for f in all_files if f.endswith('.jpg')]\n",
    "    \n",
    "    print(f\"   Total de archivos encontrados: {len(all_files)}\")\n",
    "    print(f\"   Archivos .jpg encontrados: {len(jpg_files)}\")\n",
    "    \n",
    "    # Proceso cada archivo .jpg\n",
    "    for filename in jpg_files:\n",
    "        # Construyo la ruta completa correctamente\n",
    "        filepath = os.path.join(train_dir, filename)\n",
    "        image_paths.append(filepath)\n",
    "        \n",
    "        # Etiqueto según el nombre del archivo\n",
    "        # 'cat.123.jpg' -> 0 (gato)\n",
    "        # 'dog.456.jpg' -> 1 (perro)\n",
    "        if filename.startswith('cat'):\n",
    "            label = 0\n",
    "        elif filename.startswith('dog'):\n",
    "            label = 1\n",
    "        else:\n",
    "            print(f\"   [!] Archivo ignorado (nombre no reconocido): {filename}\")\n",
    "            continue\n",
    "        \n",
    "        labels.append(label)\n",
    "    \n",
    "    # Verifico que se encontraron imágenes\n",
    "    if len(image_paths) == 0:\n",
    "        raise ValueError(\n",
    "            f\"ERROR: No se encontraron imágenes .jpg válidas en: {train_dir}\\n\"\n",
    "            f\"   Primeros 10 archivos en el directorio: {all_files[:10]}\\n\"\n",
    "            f\"   Verifica que el dataset está descargado correctamente\"\n",
    "        )\n",
    "    \n",
    "    # Convierto a arrays numpy\n",
    "    image_paths = np.array(image_paths)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# Cargo los datos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CARGANDO DATOS DE ENTRENAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_paths, train_labels = prepare_data(CONFIG['train_dir'])\n",
    "\n",
    "print(f\"\\n[OK] Datos cargados correctamente:\")\n",
    "print(f\"   Total de imágenes: {len(train_paths)}\")\n",
    "print(f\"   Gatos (label=0): {(train_labels == 0).sum()}\")\n",
    "print(f\"   Perros (label=1): {(train_labels == 1).sum()}\")\n",
    "print(f\"   Forma de train_paths: {train_paths.shape}\")\n",
    "print(f\"   Forma de train_labels: {train_labels.shape}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformaciones y Data Augmentation\n",
    "**[v1.5 - 30/10/2025 02:20 AM]** - Augmentation mejorado con ShiftScaleRotate\n",
    "\n",
    "Defino las transformaciones de entrenamiento con augmentation y las de validación/test sin augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones de entrenamiento con augmentation\n",
    "train_transforms = A.Compose([\n",
    "    A.RandomResizedCrop(height=CONFIG['img_size'], width=CONFIG['img_size'], scale=(0.8, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Transformaciones de validación/test sin augmentation\n",
    "val_transforms = A.Compose([\n",
    "    A.Resize(height=CONFIG['img_size'], width=CONFIG['img_size']),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"Transformaciones definidas\")\n",
    "print(f\"   - Train: RandomCrop, HorizontalFlip, Brightness/Contrast, Rotation\")\n",
    "print(f\"   - Val/Test: Solo resize y normalización\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo con Transfer Learning\n",
    "**[v1.0 - 29/10/2025 23:15 PM]** - Implementado Transfer Learning con EfficientNet-B3\n",
    "\n",
    "Cargo EfficientNet-B3 preentrenado y lo adapto para clasificación binaria (perros vs gatos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo modelo con transfer learning\n",
    "def create_model(model_name, num_classes, pretrained=True):\n",
    "    model = timm.create_model(model_name, pretrained=pretrained, num_classes=num_classes)\n",
    "    return model\n",
    "\n",
    "# Congelo el backbone para etapa 1\n",
    "def freeze_backbone(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name:\n",
    "            param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "# Descongelo todo para etapa 2\n",
    "def unfreeze_backbone(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    return model\n",
    "\n",
    "model = create_model(CONFIG['model_name'], CONFIG['num_classes'], pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Modelo creado: {CONFIG['model_name']}\")\n",
    "print(f\"   - Parámetros totales: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   - Parámetros entrenables: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Entrenamiento y Validación\n",
    "**[v1.3 - 30/10/2025 00:45 AM]** - Añadido Mixed Precision Training (AMP)\n",
    "\n",
    "Implemento las funciones para entrenar y validar el modelo con mixed precision training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entrenamiento con mixed precision\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Función de validación\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "print(\"Funciones de entrenamiento y validación definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con K-Fold Cross-Validation\n",
    "\n",
    "commits Hechos:\n",
    "\n",
    "**[v1.1 - 30/10/2025 00:10 AM]** - Implementado K-Fold (5 folds)  \n",
    "**[v1.2 - 30/10/2025 00:30 AM]** - Añadido Early Stopping y Label Smoothing\n",
    "\n",
    "Entreno el modelo con 5 folds y 2 etapas por fold:\n",
    "1. **Etapa 1**: Solo entreno la cabeza con el backbone congelado\n",
    "2. **Etapa 2**: Fine-tuning completo descongelando todo el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VERIFICACIÓN PRE-ENTRENAMIENTO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICACION DE DATOS ANTES DEL ENTRENAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Verifico que las variables existen y no están vacías\n",
    "print(\"\\n[1] Verificando variables train_paths y train_labels:\")\n",
    "try:\n",
    "    print(f\"   [OK] train_paths existe: {type(train_paths)}\")\n",
    "    print(f\"   [OK] train_labels existe: {type(train_labels)}\")\n",
    "    print(f\"   [OK] Número de muestras: {len(train_paths)}\")\n",
    "    print(f\"   [OK] Tipos correctos: {train_paths.dtype}, {train_labels.dtype}\")\n",
    "except NameError as e:\n",
    "    print(f\"   [ERROR] Variable no definida - {e}\")\n",
    "    raise\n",
    "\n",
    "# 2. Verifico que las longitudes coinciden\n",
    "print(\"\\n[2] Verificando que las listas son paralelas:\")\n",
    "if len(train_paths) == len(train_labels):\n",
    "    print(f\"   [OK] Longitudes coinciden: {len(train_paths)} == {len(train_labels)}\")\n",
    "else:\n",
    "    raise ValueError(f\"   [ERROR] Longitudes no coinciden: {len(train_paths)} != {len(train_labels)}\")\n",
    "\n",
    "# 3. Verifico distribución de clases\n",
    "print(\"\\n[3] Verificando distribución de clases:\")\n",
    "unique_labels, counts = np.unique(train_labels, return_counts=True)\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    class_name = \"Gato\" if label == 0 else \"Perro\"\n",
    "    percentage = (count / len(train_labels)) * 100\n",
    "    print(f\"   Clase {label} ({class_name}): {count} muestras ({percentage:.2f}%)\")\n",
    "\n",
    "# 4. Verifico que las rutas de archivos existen\n",
    "print(\"\\n[4] Verificando existencia de archivos (muestreo de 5):\")\n",
    "sample_indices = np.random.choice(len(train_paths), size=min(5, len(train_paths)), replace=False)\n",
    "for idx in sample_indices:\n",
    "    path = train_paths[idx]\n",
    "    exists = os.path.exists(path)\n",
    "    label = train_labels[idx]\n",
    "    status = \"[OK]\" if exists else \"[ERROR]\"\n",
    "    print(f\"   {status} {os.path.basename(path)} (label={label})\")\n",
    "\n",
    "# 5. Verifico que puedo cargar una imagen de muestra\n",
    "print(\"\\n[5] Probando carga de una imagen de muestra:\")\n",
    "try:\n",
    "    test_img_path = train_paths[0]\n",
    "    test_img = Image.open(test_img_path).convert('RGB')\n",
    "    test_img_array = np.array(test_img)\n",
    "    print(f\"   [OK] Imagen cargada correctamente\")\n",
    "    print(f\"   Dimensiones: {test_img_array.shape}\")\n",
    "    print(f\"   Rango de valores: [{test_img_array.min()}, {test_img_array.max()}]\")\n",
    "except Exception as e:\n",
    "    print(f\"   [ERROR] al cargar imagen: {e}\")\n",
    "    raise\n",
    "\n",
    "# 6. Verifico compatibilidad con StratifiedKFold\n",
    "print(\"\\n[6] Verificando compatibilidad con StratifiedKFold:\")\n",
    "try:\n",
    "    test_skf = StratifiedKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "    splits = list(test_skf.split(train_paths, train_labels))\n",
    "    print(f\"   [OK] StratifiedKFold generó {len(splits)} splits correctamente\")\n",
    "    print(f\"   Tamaños de los splits:\")\n",
    "    for i, (train_idx, val_idx) in enumerate(splits[:3]):  # Muestro solo los primeros 3\n",
    "        print(f\"      Fold {i+1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   [ERROR] en StratifiedKFold: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"[OK] TODAS LAS VERIFICACIONES PASADAS - LISTO PARA ENTRENAR\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=CONFIG['num_folds'], shuffle=True, random_state=CONFIG['seed'])\n",
    "\n",
    "fold_models = []\n",
    "fold_metrics = []\n",
    "\n",
    "print(f\"Iniciando entrenamiento con {CONFIG['num_folds']} folds\\n\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_paths, train_labels)):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"FOLD {fold + 1}/{CONFIG['num_folds']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    fold_train_paths = train_paths[train_idx]\n",
    "    fold_train_labels = train_labels[train_idx]\n",
    "    fold_val_paths = train_paths[val_idx]\n",
    "    fold_val_labels = train_labels[val_idx]\n",
    "    \n",
    "    train_dataset = DogsVsCatsDataset(fold_train_paths, fold_train_labels, transforms=train_transforms)\n",
    "    val_dataset = DogsVsCatsDataset(fold_val_paths, fold_val_labels, transforms=val_transforms)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], \n",
    "                             shuffle=True, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], \n",
    "                           shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "    \n",
    "    model = create_model(CONFIG['model_name'], CONFIG['num_classes'], pretrained=True)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # ETAPA 1: entreno solo la cabeza\n",
    "    print(f\"\\nEtapa 1: Entrenando solo la cabeza ({CONFIG['epochs_stage1']} epochs)\")\n",
    "    model = freeze_backbone(model)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                           lr=CONFIG['lr_stage1'], weight_decay=CONFIG['weight_decay'])\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs_stage1']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs_stage1']} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "    \n",
    "    # ETAPA 2: fine-tuning completo\n",
    "    print(f\"\\nEtapa 2: Fine-tuning completo ({CONFIG['epochs_stage2']} epochs)\")\n",
    "    model = unfreeze_backbone(model)\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr_stage2'], \n",
    "                           weight_decay=CONFIG['weight_decay'])\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience_limit = 5\n",
    "    \n",
    "    for epoch in range(CONFIG['epochs_stage2']):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "        val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['epochs_stage2']} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"   -> Nuevo mejor modelo (Val Acc: {val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience_limit:\n",
    "            print(f\"   -> Early stopping en epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    fold_models.append(model)\n",
    "    fold_metrics.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} completado - Mejor Val Acc: {best_val_acc:.2f}%\\n\")\n",
    "\n",
    "# Resumen de todos los folds\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESUMEN DE ENTRENAMIENTO\")\n",
    "print(f\"{'='*60}\")\n",
    "for metric in fold_metrics:\n",
    "    print(f\"Fold {metric['fold']}: Val Acc = {metric['best_val_acc']:.2f}%\")\n",
    "\n",
    "avg_val_acc = np.mean([m['best_val_acc'] for m in fold_metrics])\n",
    "print(f\"\\nAccuracy promedio en validación: {avg_val_acc:.2f}%\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencia en Test Set\n",
    "\n",
    "Cargo las imágenes de test y genero predicciones promediando los 5 modelos de los folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de test\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transforms=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.transforms = transforms\n",
    "        self.image_files = sorted([f for f in os.listdir(test_dir) if f.endswith('.jpg')])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.test_dir, img_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        img_id = int(img_name.split('.')[0])\n",
    "        return image, img_id\n",
    "\n",
    "test_dataset = TestDataset(CONFIG['test_dir'], transforms=val_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], \n",
    "                         shuffle=False, num_workers=CONFIG['num_workers'], pin_memory=True)\n",
    "\n",
    "print(f\"Dataset de test cargado: {len(test_dataset)} imágenes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero predicciones promediando los 5 folds\n",
    "print(\"Generando predicciones...\")\n",
    "\n",
    "all_predictions = []\n",
    "all_ids = []\n",
    "\n",
    "for model in fold_models:\n",
    "    model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, img_ids in test_loader:\n",
    "        images = images.to(device)\n",
    "        \n",
    "        fold_preds = []\n",
    "        for model in fold_models:\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "            fold_preds.append(probs.cpu().numpy())\n",
    "        \n",
    "        avg_probs = np.mean(fold_preds, axis=0)\n",
    "        all_predictions.extend(avg_probs)\n",
    "        all_ids.extend(img_ids.numpy())\n",
    "\n",
    "print(f\"Predicciones generadas: {len(all_predictions)} imágenes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación del archivo Submission\n",
    "\n",
    "Creo el archivo `submission.csv` con las predicciones finales para subir a Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': all_ids,\n",
    "    'label': all_predictions\n",
    "})\n",
    "\n",
    "submission_df = submission_df.sort_values('id').reset_index(drop=True)\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Archivo submission.csv generado correctamente\")\n",
    "print(f\"\\nPrimeras predicciones:\")\n",
    "print(submission_df.head(10))\n",
    "print(f\"\\nEstadísticas de las predicciones:\")\n",
    "print(f\"   - Media: {submission_df['label'].mean():.4f}\")\n",
    "print(f\"   - Desviación estándar: {submission_df['label'].std():.4f}\")\n",
    "print(f\"   - Mínimo: {submission_df['label'].min():.4f}\")\n",
    "print(f\"   - Máximo: {submission_df['label'].max():.4f}\")\n",
    "print(f\"\\nTotal de predicciones: {len(submission_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de Predicciones (mi bonus)\n",
    "\n",
    "Muestro algunas imágenes del test set con sus predicciones para verificar visualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo predicciones aleatorias\n",
    "def visualize_predictions(num_images=9):\n",
    "    random_indices = np.random.choice(len(submission_df), size=num_images, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, idx in enumerate(random_indices):\n",
    "        img_id = submission_df.iloc[idx]['id']\n",
    "        prediction = submission_df.iloc[idx]['label']\n",
    "        \n",
    "        img_path = os.path.join(CONFIG['test_dir'], f\"{img_id}.jpg\")\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        predicted_class = \"Perro\" if prediction > 0.5 else \"Gato\"\n",
    "        confidence = prediction if prediction > 0.5 else 1 - prediction\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"ID: {img_id}\\n{predicted_class} ({confidence*100:.1f}%)\", \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_predictions(num_images=9)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 13710467,
     "sourceId": 114912,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
