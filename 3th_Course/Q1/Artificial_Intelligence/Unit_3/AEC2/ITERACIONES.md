# Iteraciones - Dogs vs Cats Competition# Documentaci√≥n de Iteraciones - Perros vs Gatos



**Autor**: Ismael Hern√°ndez Clemente  

**Asignatura**: Inteligencia Artificial - U-Tad  ### Hip√≥tesis/Justificaci√≥n

**Competici√≥n**: Kaggle Dogs vs Cats 2025  

**Objetivo**: Score >0.80 en el leaderboard p√∫blico## ITERACI√ìN 1 - CNN Baseline (DEPRECATED)Mi hip√≥tesis es que una arquitectura CNN b√°sica con BatchNormalization y Dropout deber√≠a superar 



---ampliamente el baseline (0.569) al ser capaz de extraer caracter√≠sticas relevantes de las im√°genes.



## Estrategia General**Fecha**: 29/10/2025  El uso de BatchNormalization deber√≠a acelerar la convergencia y el Dropout prevenir el overfitting.



Mi plan para esta competici√≥n es ir paso a paso, sin quemar etapas. Primero quiero entender el problema con algo simple, una CNN b√°sica para ver c√≥mo se comporta el modelo, identificar problemas y luego ir a√±adiendo mejoras incrementales. Si al final veo que la CNN no da para m√°s, pues tiro de Transfer Learning.**Estado**: ‚ùå Movido a MODELOS_DEPRECATED



**Las fases que ten√≠a en mente:**### Resultados Obtenidos

1. **Baseline simple** - Ver qu√© pasa sin complicarme la vida

2. **Data augmentation** - Intentar controlar el overfitting que seguro va a aparecer### Cambios Realizados

3. **Ajustes finos** - Probar hiperpar√°metros y callbacks

4. **Transfer Learning** - Si todo lo anterior no funciona bien**M√©tricas de Entrenamiento:**



Voy a documentar especialmente los errores, porque al final es donde m√°s aprendes. Cada iteraci√≥n tiene el mismo formato: qu√© cambi√©, por qu√© lo hice, qu√© pas√≥ y qu√© aprend√≠ de verdad.Constru√≠ una CNN desde cero con arquitectura cl√°sica:- Training Accuracy final: 0.901



---- 3 bloques convolucionales (32‚Üí64‚Üí128 filtros)- Validation Accuracy (mejor - √©poca 5): 0.811



## ITERACI√ìN 1 - CNN Plantilla modificada - BatchNormalization despu√©s de cada convoluci√≥n- Validation Accuracy final (√©poca 12): 0.715



**Fecha**: 29/10/2025  - MaxPooling para reducir dimensionalidad- Validation Loss final: 0.602

**Estado**: DEPRECATED - Movido a la carpeta MODELOS_DEPRECATED

- Capa densa 512 + Dropout 0.5

### Qu√© hice

- Salida softmax para 2 clases**M√©tricas de Evaluaci√≥n:**

Constru√≠ la CNN desde cero reutilizando la plantilla y siguiendo el patr√≥n cl√°sico que siguen los materiales de clase del tema 2. Nada especial, solo la estructura t√≠pica:

- Supplementary Data Accuracy: 0.653

- 3 bloques convolucionales con filtros que van aumentando (32, 64, 128)

- BatchNormalization despu√©s de cada convoluci√≥n (seg√∫n le√≠ acelera la convergencia)**Configuraci√≥n:**- Supplementary Data Loss: 0.918

- MaxPooling para ir reduciendo el tama√±o espacial

- Una capa densa de 512 neuronas con Dropout 0.5- √âpocas: 12

- Salida con softmax para las 2 clases (perro/gato)

- Optimizer: RMSprop (lr=0.001)**Kaggle Score:**

**Configuraci√≥n del entrenamiento:**

- 12 √©pocas- Sin data augmentation- Public Score: [Pendiente]

- Optimizer RMSprop con learning rate 0.001

- Sin data augmentation (quer√≠a ver primero qu√© pasaba sin nada)- Image size: 256x256

- Im√°genes redimensionadas a 256x256



### Por qu√© esta configuraci√≥n

### Justificaci√≥n T√©cnica### An√°lisis de Curvas de Aprendizaje

La arquitectura la saqu√© del material de U-Tad  que hab√≠amos visto en clase y 42AI computer vision. Vi que BatchNormalization ayuda bastante con la estabilidad del entrenamiento y el Dropout de 0.5 deber√≠a evitar algo de overfitting. Mi idea era empezar con algo simple y ver qu√© problemas aparec√≠an, en lugar de meterme en algo muy complejo desde el principio.



**Referencias que consult√©:**

- Material de 42AI sobre Deep Learning: https://github.com/42-AI/bootcamp_machine-learning La arquitectura se basa en el patr√≥n cl√°sico de CNNs que aprend√≠ en los tutoriales de 42AI sobre computer vision. La idea de usar BatchNormalization viene de ver que acelera convergencia y estabiliza el entrenamiento.**Evoluci√≥n por √©pocas clave:**

- Keras docs sobre BatchNormalization: https://keras.io/api/layers/normalization_layers/batch_normalization/



### Resultados**Referencias:**| √âpoca | Train Acc | Val Acc | Train Loss | Val Loss | Estado |



| M√©trica | Valor |- 42AI - Introduction to Deep Learning: https://github.com/42-AI/bootcamp_machine-learning|-------|-----------|---------|------------|----------|---------|

|---------|-------|

| Mejor Val Accuracy | 0.811 (√©poca 5) |- Keras Documentation - BatchNormalization: https://keras.io/api/layers/normalization_layers/batch_normalization/| 1     | 0.573     | 0.621   | 26.67      | 0.678    | Aprendiendo |

| Val Accuracy final | 0.715 (√©poca 12) |

| Supplementary Acc | 0.653 || 5     | 0.786     | **0.811** | 0.454    | **0.431** | MEJOR PUNTO |

| Gap Train/Val | 18.6% |

### Resultados| 6     | 0.817     | 0.667   | 0.391      | 0.863    | Colapso |

**Lo que pas√≥:**

| 12    | 0.901     | 0.715   | 0.216      | 0.602    | Overfitting severo |

El modelo empez√≥ bastante bien, lleg√≥ a 0.81 en validaci√≥n en la √©poca 5 y luego todo se fue al garete. Desde la √©poca 6 el validation loss empez√≥ a subir como loco mientras el training loss segu√≠a bajando tranquilamente. Overfitting cl√°sico de manual.

| M√©trica | Valor |

El gap de 18.6% entre training y validation es brutal. Y lo peor es que en supplementary data (datos que no hab√≠a visto nunca) solo lleg√≥ a 0.65, lo que significa que no est√° generalizando nada bien.

|---------|-------|**Observaciones:**

### Conclusi√≥n del modelo?

| Best Val Accuracy | 0.811 (√©poca 5) |- Desde la √©poca 6 se detecta overfitting severo

El modelo tiene capacidad para aprender el problema (lo del 0.81 lo demuestra) pero se est√° memorizando las im√°genes en lugar de aprender patrones generales. Me falta urgentemente data augmentation. Sin eso, el modelo ve las mismas im√°genes exactas en cada √©poca y obviamente las memoriza.

Esta primera iteraci√≥n √∫til para entender el problema, pero claramente no sirve para producci√≥n ni para Kaggle. Siguiente paso: a√±adir data augmentation para forzarlo a aprender patrones en lugar de memorizar.

**Referencias sobre el problema:**

- Paper sobre overfitting: https://arxiv.org/abs/1409.1556 (secci√≥n 3.2)| Gap Train/Val | 18.6% |- Validation loss explota desde √©poca 6 (0.431 ‚Üí 0.863 ‚Üí 1.101)



---



## ITERACI√ìN 2 - Data Augmentation**Conclusi√≥n**: Overfitting brutal desde √©poca 6. El modelo memoriza en lugar de generalizar. Necesita urgentemente data augmentation.### Conclusiones y Pr√≥ximos Pasos



**Fecha**: 02/11/2025  

**Estado**: DEPRECATED - Movido a carpeta MODELOS_DEPRECATED

---**Lo que funcion√≥ bien:**

### Qu√© hice

- ‚úÖ La arquitectura base es s√≥lida: supera el baseline en +10-24 puntos seg√∫n √©poca

Mantuve exactamente la misma arquitectura pero a√±ad√≠ data augmentation bastante agresivo:

## ITERACI√ìN 2 - Data Augmentation (DEPRECATED)- ‚úÖ BatchNormalization ayuda a la convergencia

- RandomFlip horizontal (los perros y gatos pueden mirar en cualquier direcci√≥n)

- RandomRotation hasta 36¬∞ (para simular diferentes √°ngulos de c√°mara)- ‚úÖ El modelo aprende caracter√≠sticas relevantes (llega a 0.81 en validaci√≥n)

- RandomZoom del 20% (simular diferentes distancias)

- RandomTranslation del 10% (los animales no siempre est√°n centrados)**Fecha**: 02/11/2025  

- Sub√≠ el Dropout de 0.5 a 0.6 para forzar m√°s regularizaci√≥n

- A√±ad√≠ Early Stopping y ReduceLROnPlateau como red de seguridad**Estado**: ‚ùå Movido a MODELOS_DEPRECATED**Problemas detectados:**

- Dej√© hasta 25 √©pocas pero con early stopping por si acaso

- ‚ùå **Overfitting severo** desde √©poca 6

### Por qu√© esta configuraci√≥n

### Cambios Realizados- ‚ùå Sin data augmentation, el modelo memoriza en lugar de generalizar

Despu√©s del desastre de overfitting de la Iter1, me puse a investigar sobre data augmentation. Vi varios videos de Yannic Kilcher donde explica que cuando tienes pocos datos (y 12,500 im√°genes es relativamente poco para deep learning), el augmentation es absolutamente cr√≠tico para generalizaci√≥n.

- ‚ùå Dropout 0.5 no es suficiente para prevenir overfitting

La idea es que cada √©poca el modelo vea versiones ligeramente diferentes de las mismas im√°genes (rotadas, volteadas, con zoom), forz√°ndolo a aprender caracter√≠sticas invariantes en lugar de memorizar pixeles exactos. Es como estudiar entendiendo los conceptos en lugar de memorizar las respuestas literales.

Mismo modelo pero con data augmentation agresivo:- ‚ùå Gap enorme entre validation (0.715) y supplementary data (0.653)

El Early Stopping lo a√±ad√≠ para que pare autom√°ticamente si empieza a sobreajustar, y el ReduceLROnPlateau para hacer ajustes m√°s finos cuando se acerque al √≥ptimo (reduce el learning rate cuando el validation loss deja de mejorar).

- RandomFlip horizontal

**Referencias que me ayudaron:**

- Video de Yannic Kilcher sobre Data Augmentation: https://www.youtube.com/watch?v=Zrp2b8qXHYk- RandomRotation (¬±36¬∞)**Diagn√≥stico:**

- Keras guide sobre preprocessing layers: https://keras.io/guides/preprocessing_layers/

- Paper "The Effectiveness of Data Augmentation": https://arxiv.org/abs/1712.04621- RandomZoom (¬±20%)El modelo tiene capacidad para aprender el problema (lo demuestra el 0.81 en √©poca 5), pero 

- Recomendaciones de Fast.ai sobre augmentation: https://docs.fast.ai/vision.augment.html

- RandomTranslation (¬±10%)**necesita urgentemente regularizaci√≥n adicional mediante data augmentation** para mejorar 

### Resultados

- Dropout aumentado a 0.6la generalizaci√≥n.

| M√©trica | Iter 1 | Iter 2 | Cambio |

|---------|--------|--------|--------|- Early Stopping + ReduceLROnPlateau

| Val Accuracy | 0.811 | 0.868 | +7.0% |

| Supplementary Acc | 0.653 | 0.753 | +15.3% |- √âpocas: 25**Pr√≥ximos pasos para Iteraci√≥n 2:**

| Gap Train/Val | 18.6% | 5.3% | -13.3% |

| √âpocas √∫tiles | 5 | 25 | +20 |1. **CR√çTICO**: Implementar Data Augmentation (rotaciones, flips, zoom)



**Lo que pas√≥:**### Justificaci√≥n T√©cnica2. A√±adir Early Stopping para parar cuando val_loss empiece a subir



Cambio brutal. El modelo entren√≥ las 25 √©pocas completas sin colapsar. La validation accuracy lleg√≥ a 0.868, pero lo realmente importante es que en supplementary data (datos totalmente nuevos) subi√≥ de 0.65 a 0.75. Eso significa que est√° generalizando mucho mejor.3. Aumentar Dropout de 0.5 a 0.6



El gap train/val baj√≥ de 18.6% a 5.3%, que es totalmente razonable y manejable. S√≠ hubo una peque√±a ca√≠da en la √©poca 22 pero se recuper√≥ bien.Despu√©s de ver el desastre de la Iteraci√≥n 1, investigu√© sobre data augmentation. Vi varios videos de Yannic Kilcher explicando que en problemas con pocos datos, el augmentation es cr√≠tico para generalizaci√≥n. La idea es que cada √©poca vea versiones diferentes de las mismas im√°genes, forzando al modelo a aprender patrones invariantes.4. Considerar aumentar √©pocas a 20 con early stopping



### Qu√© aprend√≠ de verdad5. Probar con ReduceLROnPlateau para ajustar learning rate din√°micamente



**Lecci√≥n clave que me llevo:** En clasificaci√≥n de im√°genes con datasets limitados, data augmentation no es algo opcional que a√±ades si te sobra tiempo. Es absolutamente obligatorio desde el primer momento. La diferencia es brutal: +15.3% en supplementary data, que son los datos que realmente importan.**Referencias:**



A partir de ahora todas las iteraciones van a llevar data augmentation por defecto. No tiene sentido no usarlo, es regalar rendimiento gratis.- Yannic Kilcher - Data Augmentation in Deep Learning: https://www.youtube.com/watch?v=Zrp2b8qXHYk**Objetivo Iteraci√≥n 2:** 



**Referencias sobre por qu√© funciona:**- Keras Guide - Image Data Augmentation: https://keras.io/guides/preprocessing_layers/- Validation Accuracy: 0.82-0.85

- Fast.ai lesson sobre augmentation: https://course.fast.ai/ (Lesson 5)

- CS231n sobre regularization: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf- Paper - "The Effectiveness of Data Augmentation in Image Classification": https://arxiv.org/abs/1712.04621- Eliminar overfitting y mantener consistencia entre train/val/test



---- Kaggle Score esperado: 0.78-0.82



## RESUMEN Iteraciones 1-2### Resultados



**Lo m√°s importante que aprend√≠ hasta aqu√≠:**### Referencias



El data augmentation cambi√≥ completamente el juego. No es una mejora incremental, es un cambio fundamental en c√≥mo el modelo aprende. La diferencia entre 0.65 y 0.75 en datos nuevos es enorme en t√©rminos pr√°cticos.| M√©trica | Iteraci√≥n 1 | Iteraci√≥n 2 | Mejora |- Documentaci√≥n de Keras Data Augmentation: https://keras.io/api/layers/preprocessing_layers/



**N√∫meros clave:**|---------|-------------|-------------|--------|- BatchNormalization: https://keras.io/api/layers/normalization_layers/batch_normalization/

- +7% en validation (bien)

- +15.3% en generalizaci√≥n (excelente)| Val Accuracy | 0.811 | **0.868** | +7.0% |- Dropout: https://keras.io/api/layers/regularization_layers/dropout/

- -13.3 puntos de overfitting (cr√≠tico)

| Supplementary Acc | 0.653 | **0.753** | +15.3% |

En este punto pens√© que esto estaba listo para Kaggle. Spoiler: no lo estaba ni de cerca.

| Gap Train/Val | 18.6% | **5.3%** | -13.3% |---

---



## ITERACI√ìN 3 - Primer Submit Kaggle

**Conclusi√≥n**: √âxito rotundo. Data augmentation resolvi√≥ el overfitting completamente. El modelo entren√≥ 25 √©pocas estables vs 5 √∫tiles en Iter1.## ITERACI√ìN 2 - Data Augmentation + Early Stopping + ReduceLROnPlateau

**Fecha**: 04/11/2025  

**Estado**: DEPRECATED - Movido a carpeta MODELOS_DEPRECATED  

**Kaggle Score**: 0.58768 (DESASTRE TOTAL)

---**Fecha**: 05/11/2025

### Qu√© hice



Re-entren√© el modelo de Iter2 con una semilla aleatoria diferente para ver si consegu√≠a alg√∫n punto extra. Mismo setup exacto, solo cambi√≥ la inicializaci√≥n aleatoria de los pesos.

## üìä RESUMEN ITERACIONES 1-2: APRENDIZAJE CLAVE### Descripci√≥n del Cambio

### Por qu√© esta configuraci√≥n



Quer√≠a probar si con diferente inicializaci√≥n consegu√≠a mejor performance. En deep learning la inicializaci√≥n aleatoria puede cambiar bastante los resultados finales, as√≠ que no est√° de m√°s probar.

**Lecci√≥n principal**: En problemas de clasificaci√≥n de im√°genes con datasets limitados, **data augmentation no es opcional, es obligatorio**. La diferencia entre 0.65 y 0.75 en supplementary data es brutal.Bas√°ndome en los resultados de la Iteraci√≥n 1 (overfitting severo), he implementado las siguientes mejoras:

**Referencias sobre inicializaci√≥n:**

- Discusiones de Kaggle sobre variabilidad: https://www.kaggle.com/c/dogs-vs-cats/discussion

- Foros de 42 sobre reproducibilidad en ML

**N√∫meros clave:****1. Data Augmentation integrado en el modelo:**

### Resultados

- Mejora validation: +7.0%- RandomFlip horizontal

| M√©trica | Valor |

|---------|-------|- Mejora generalizaci√≥n: +15.3%- RandomRotation (¬±36¬∞, factor 0.1)

| Val Accuracy | 0.858 |

| Supplementary Acc | 0.653 |- Reducci√≥n overfitting: -13.3 puntos gap- RandomZoom (¬±20%, factor 0.2)

| Kaggle Score | 0.58768 |

| Diferencia esperado/real | -29% |- RandomTranslation (¬±10% en altura y anchura)



**Lo que pas√≥:****Decisi√≥n**: Todas las iteraciones futuras incluir√°n data augmentation por defecto.



DESASTRE ABSOLUTO. Yo esperaba un score de 0.83-0.85 bas√°ndome en la validation accuracy, pero me sali√≥ 0.587. Algo estaba muy mal y no ten√≠a ni idea de qu√©.**2. Regularizaci√≥n mejorada:**



Despu√©s de analizar el submission.csv l√≠nea por l√≠nea, descubr√≠ el problema: el modelo predijo 64% perros y solo 36% gatos. En un dataset balanceado 50/50 como este, esto explica perfectamente el score de ~58%. Si predices m√°s perros de lo que deber√≠as, pierdes muchos puntos.---- Dropout aumentado: 0.5 ‚Üí 0.6



### Qu√© aprend√≠ de verdad



**Lecci√≥n brutal que me cost√≥ un submission:** La validation accuracy local puede mentir descaradamente. El modelo estaba sesgado hacia predecir m√°s perros y las m√©tricas locales no me avisaron de esto hasta que sub√≠ a Kaggle y vi el desastre.## ITERACI√ìN 3 - Primer Submit Kaggle (DEPRECATED)**3. Control inteligente del entrenamiento:**



**Problemas identificados:**- Early Stopping (monitor='val_loss', patience=5, restore_best_weights=True)

1. Usar categorical crossentropy + softmax puede dar probabilidades sesgadas entre clases

2. Usar np.argmax() sin verificar la distribuci√≥n de predicciones es peligroso**Fecha**: 04/11/2025  - ReduceLROnPlateau (factor=0.5, patience=3, min_lr=1e-7)

3. El early stopping par√≥ muy pronto (√©poca 15) y quiz√°s no dej√≥ aprender bien

**Estado**: ‚ùå Movido a MODELOS_DEPRECATED  

**Referencias que consult√© para entender el problema:**

- Stack Overflow - Binary vs Categorical: https://stackoverflow.com/questions/47034888/**Kaggle Score**: 0.58768 üíÄ**4. M√°s √©pocas permitidas:**

- Kaggle discussions sobre predicciones desbalanceadas: https://www.kaggle.com/discussions/getting-started/27270

- √âpocas: 12 ‚Üí 25 (con early stopping para no desperdiciar)

Necesito cambiar urgentemente a binary classification y verificar las predicciones antes de enviar a Kaggle.

### Cambios Realizados

---

**Configuraci√≥n mantenida:**

## ITERACI√ìN 4 - Bug Fixes

Re-entrenamiento del modelo Iter2 con diferente semilla aleatoria para probar variabilidad. Mismo setup exacto pero early stopping activado en √©poca 15.- Arquitectura: 3 bloques Conv (32, 64, 128) + BatchNorm + MaxPooling

**Fecha**: 05/11/2025  

**Estado**: DEPRECATED - Movido a carpeta MODELOS_DEPRECATED  - Optimizador: RMSprop (lr=0.001 inicial)

**Kaggle Score**: 0.54664 (A√öN PEOR)

### Resultados- Batch size: 125

### Qu√© hice

- Image size: 256x256

Correg√≠ todo lo que vi mal en Iter3:

| M√©trica | Valor |

- Cambi√© de categorical a binary classification

- Cambi√© la capa de salida de softmax(2) a sigmoid(1)|---------|-------|### Hip√≥tesis/Justificaci√≥n

- Predicciones con umbral expl√≠cito 0.5 en lugar de usar np.argmax() ciegamente

- Quit√© el early stopping, dej√© 20 √©pocas fijas para que aprenda bien| Val Accuracy | 0.858 |

- Cambi√© a optimizer Adam (m√°s estable seg√∫n le√≠)

| Supplementary Acc | 0.653 |**Problema identificado en Iteraci√≥n 1:**

### Por qu√© esta configuraci√≥n

| **Kaggle Score** | **0.58768** |El modelo sufr√≠a de overfitting severo porque memorizaba las im√°genes exactas del conjunto de entrenamiento. Sin variaciones en los datos, despu√©s de 5-6 √©pocas comenzaba a sobreajustarse.

Investigu√© bastante en los foros de 42 y en discusiones de Kaggle. B√°sicamente vi que para problemas binarios, usar binary crossentropy con sigmoid es m√°s estable matem√°ticamente que categorical con softmax. El umbral 0.5 expl√≠cito elimina cualquier ambig√ºedad en la conversi√≥n de probabilidades a clases.



**Referencias que me salvaron:**

- Material de 42AI sobre binary classification: https://github.com/42-AI/bootcamp_python**Diferencia esperado vs real**: -29% üíÄ**Hip√≥tesis de mejora:**

- Curso de Andrew Ng sobre binary vs multiclass: https://www.coursera.org/learn/machine-learning (Week 6)

- Keras best practices: https://keras.io/examples/Con data augmentation, cada √©poca ver√° versiones ligeramente diferentes de las mismas im√°genes (rotadas, volteadas, con zoom, desplazadas). Esto deber√≠a:



### Resultados### An√°lisis del Desastre1. Forzar al modelo a aprender caracter√≠sticas generales en lugar de memorizar



| M√©trica | Valor |2. Permitir entrenar m√°s √©pocas sin overfitting

|---------|-------|

| Val Accuracy | 0.742 |Despu√©s de analizar el submission.csv, descubr√≠ que el modelo predijo **64% perros y 36% gatos**. En un dataset balanceado 50/50, esto explica perfectamente el score ~58%.3. Mejorar la generalizaci√≥n en datos nuevos (supplementary y test)

| Supplementary Acc | 0.603 |

| Distribuci√≥n predicciones | 40% gatos / 60% perros |

| Kaggle Score | 0.54664 |

**Causas identificadas:**El Early Stopping evitar√° desperdiciar tiempo si el modelo converge antes de 25 √©pocas, y ReduceLROnPlateau har√° ajustes m√°s finos cuando el modelo se acerque al √≥ptimo.

**Lo que pas√≥:**

1. Categorical crossentropy + softmax puede dar probabilidades sesgadas

TODAV√çA PEOR que antes. El score baj√≥ de 0.587 a 0.546. Me qued√© en shock.

2. np.argmax() sin verificaci√≥n de distribuci√≥n**Objetivo cuantitativo:**

La distribuci√≥n de predicciones ahora s√≠ est√° m√°s balanceada (40/60 es aceptable), as√≠ que el bug de predicci√≥n est√° efectivamente arreglado. Pero el score sigue siendo horrible.

3. Early stopping par√≥ demasiado pronto (√©poca 15)- Validation accuracy: 0.82-0.85 (vs 0.811 mejor de Iter1)

**Diagn√≥stico real:** El problema no era solo el bug de predicci√≥n. El problema fundamental es que la CNN desde cero simplemente NO GENERALIZA bien:

- Validation: 74% (m√°s o menos aprende el validation set)- Supplementary accuracy: 0.78-0.82 (vs 0.653 de Iter1)

- Supplementary: 60% (empieza a fallar con datos nuevos)

- Kaggle: 54% (falla completamente con el test real)**Referencias:**- Gap train/val: <8 puntos (vs 18.6 de Iter1)



### Qu√© aprend√≠ de verdad- Stack Overflow - Binary vs Categorical Crossentropy: https://stackoverflow.com/questions/47034888/- Eliminar el colapso de validation loss



**Reality check dur√≠simo:** CNN desde cero tiene demasiada variabilidad entre entrenamientos. No es estable. Con solo 3 submissions restantes no puedo seguir apostando por esto, es tirar submissions a la basura.- Kaggle Discussion - Imbalanced Predictions: https://www.kaggle.com/discussions/getting-started/27270



La validation accuracy alta no te garantiza absolutamente nada en Kaggle. El modelo puede estar sobreajustando incluso al propio validation set.### Resultados Obtenidos



**Decisi√≥n cr√≠tica:** Es momento de dejar la CNN desde cero y pasar a Transfer Learning. Ya no es una opci√≥n para "si hace falta", es una necesidad urgente.**Conclusi√≥n**: El modelo local no es tan bueno como parec√≠a. Necesito corregir bugs de predicci√≥n y cambiar a binary classification.



**Referencias que me convencieron:****M√©tricas de Entrenamiento:**

- CS231n sobre transfer learning: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

- Fast.ai explicando cu√°ndo usar TL: https://course.fast.ai/ (spoiler: casi siempre)---- Training Accuracy final: 0.8158



---- Validation Accuracy final (√©poca 25): **0.8684** **R√âCORD ABSOLUTO**



## RESUMEN Iteraciones 3-4: Lecci√≥n de humildad## ITERACI√ìN 4 - Bug Fixes (DEPRECATED)- Validation Loss final: 0.3310



**Lo que aprend√≠ de la forma m√°s dura posible:**- Total √©pocas completadas: 25 (sin early stopping)



- Val accuracy 74% ‚Üí Kaggle 54% (p√©rdida de 20 puntos)**Fecha**: 05/11/2025  - Mejores pesos: Restaurados desde √©poca 25

- 2 submissions completamente desperdiciados

- Posici√≥n en el ranking: bottom**Estado**: ‚ùå Movido a MODELOS_DEPRECATED  



**La lecci√≥n:** No puedes confiar ciegamente en las m√©tricas locales. El modelo puede estar memorizando incluso el validation set. Y la CNN desde cero tiene demasiada variabilidad entre entrenamientos diferentes.**Kaggle Score**: 0.54664 üíÄüíÄ**Evoluci√≥n por √©pocas clave Iteraci√≥n 2:**



**Decisi√≥n cr√≠tica:** Transfer Learning. Los modelos pre-entrenados como VGG16 o ResNet ya aprendieron features generales de millones de im√°genes. Solo necesito ajustar las √∫ltimas capas para mi problema espec√≠fico (perros vs gatos).



---### Cambios Realizados| √âpoca | Train Acc | Val Acc | Train Loss | Val Loss | LR | Estado |



## ITERACI√ìN 5 - Transfer Learning VGG16|-------|-----------|---------|------------|----------|-----|---------|



**Fecha**: 05/11/2025  **Correcciones cr√≠ticas:**| 8     | 0.698     | 0.735   | 0.582      | 0.507    | 1e-3 | Mejora estable |

**Estado**: MODELO ACTIVO  

**Kaggle Score**: 0.86380  - Categorical ‚Üí **Binary classification**| 11    | 0.727     | 0.733   | 0.549      | 0.517    | 5e-4 | LR reducido (√©poca 11) |

**Ranking**: 7 de ~50 participantes (Top 15%)

- Softmax(2) ‚Üí **Sigmoid(1)**| 18    | 0.781     | 0.841   | 0.463      | 0.397    | 5e-4 | Ascenso pronunciado |

### Qu√© hice

- np.argmax() ‚Üí **Umbral expl√≠cito 0.5**| 20    | 0.787     | 0.846   | 0.447      | 0.379    | 5e-4 | Pico temporal |

Cambio radical de arquitectura, dej√© la CNN desde cero y tir√© de Transfer Learning:

- √âpocas fijas: 20 (sin early stopping)| 21    | 0.790     | 0.848   | 0.445      | 0.359    | 5e-4 | M√°ximo previo |

- Base: VGG16 pre-entrenado en ImageNet (14 millones de im√°genes)

- TODAS las capas convolucionales congeladas (no las entreno)- Optimizer: Adam (m√°s estable que RMSprop)| 22    | 0.790     | 0.717   | 0.441      | 1.034    | 5e-4 | Ca√≠da dram√°tica |

- Solo entreno una cabecera simple encima: Dense(256) + Dropout(0.5) + Dense(1)

- Binary classification con sigmoid| 23    | 0.794     | 0.820   | 0.432      | 0.400    | 5e-4 | Recuperaci√≥n |

- Im√°genes 224x224 (lo que VGG16 necesita)

### Justificaci√≥n T√©cnica| 24    | 0.799     | 0.826   | 0.429      | 0.381    | 2.5e-4 | LR reducido (√©poca 24) |

**Setup del entrenamiento:**

- 15 √©pocas (Transfer Learning converge mucho m√°s r√°pido)| **25** | **0.816** | **0.868** | **0.397** | **0.331** | **2.5e-4** | **EXPLOSI√ìN FINAL** |

- Learning rate 0.0001 (muy bajo para fine-tuning, no queremos romper los pesos pre-entrenados)

- Data augmentation m√°s conservador (Transfer Learning no necesita tanto)Despu√©s de investigar en los foros de 42 y Kaggle, vi que para problemas binarios, usar binary crossentropy con sigmoid es m√°s estable que categorical con softmax. La conversi√≥n directa con umbral 0.5 elimina ambig√ºedad.

- Tiempo de entrenamiento: 11 minutos vs 20 minutos de la CNN

**M√©tricas de Evaluaci√≥n:**

### Por qu√© esta configuraci√≥n

**Referencias:**- Supplementary Data Accuracy: **0.7533**

Transfer Learning es el est√°ndar de la industria cuando no tienes millones de im√°genes para entrenar desde cero. VGG16 ya aprendi√≥ a detectar bordes, texturas, formas, patrones complejos en 14 millones de im√°genes de ImageNet. 

- 42AI - Binary Classification Best Practices: https://github.com/42-AI/bootcamp_python- Supplementary Data Loss: 0.6736

Al congelar sus capas convolucionales, lo que hago es aprovechar todo ese conocimiento pre-aprendido y solo entrenar la decisi√≥n final "esto es un perro o un gato" sobre esas features ya extra√≠das. Es mucho m√°s eficiente y estable.

- Andrew Ng - Binary vs Multiclass: https://www.coursera.org/learn/machine-learning

Esto lo saqu√© directamente de los tutoriales de Stanford CS231n y las recomendaciones de fast.ai que b√°sicamente dicen "usa transfer learning siempre que puedas, no seas h√©roe intentando entrenar desde cero".

**Kaggle Score:**

**Referencias clave que segu√≠:**

- Stanford CS231n sobre Transfer Learning: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf (slides 15-30)### Resultados- Public Score: [EXPERIMENTAL - No enviado]

- Fast.ai Practical Deep Learning: https://course.fast.ai/ (Lesson 1 ya usa TL)

- Keras Transfer Learning Guide: https://keras.io/guides/transfer_learning/- Private Score: [EXPERIMENTAL - No enviado]

- Paper original de VGG16: https://arxiv.org/abs/1409.1556

- Video de Aladdin Persson explicando VGG16: https://www.youtube.com/watch?v=ACmuBbuXn20| M√©trica | Valor |



### Resultados|---------|-------|### An√°lisis de Resultados - Comparaci√≥n Iteraciones



| M√©trica | CNN Iter 4 | VGG16 | Mejora || Val Accuracy | 0.742 |

|---------|------------|-------|--------|

| Val Accuracy | 0.742 | 0.975 | +31.4% || Supplementary Acc | 0.603 |**COMPARATIVA ITERACI√ìN 1 vs 2:**

| Val Precision | 0.842 | 0.966 | +14.8% |

| Val Recall | 0.605 | 0.985 | +62.8% || Distribuci√≥n Submit | 40% gatos / 60% perros ‚úì |

| Val Loss | 0.586 | 0.075 | -87.2% |

| Kaggle Score | 0.547 | 0.864 | +58.0% || **Kaggle Score** | **0.54664** üíÄüíÄ || M√©trica | Iteraci√≥n 1 | Iteraci√≥n 2 | Mejora | Estado |

| Tiempo entrenamiento | 20 min | 11 min | -45% |

|---------|-------------|-------------|--------|---------|

**Lo que pas√≥:**

**¬°PEOR QUE ANTES!** Score baj√≥ de 0.58768 a 0.54664.| **Mejor Val Acc** | 0.811 (√©poca 5) | **0.868** (√©poca 25) | +**7.04%** | √âXITO EXTRAORDINARIO |

√âXITO TOTAL. En 11 minutos de entrenamiento consegu√≠ mejor resultado que 3 d√≠as iterando con CNN desde cero. Ni comparaci√≥n.

| **Supplementary Acc** | 0.653 | **0.753** | +**15.3%** | √âXITO NOTABLE |

El modelo alcanz√≥ 0.975 de validation accuracy con precision y recall perfectamente balanceados (96.6% vs 98.5%). El loss baj√≠simo (0.075) muestra que el modelo est√° muy confiado en sus predicciones, no est√° dudando.

### An√°lisis del Segundo Desastre| **Estabilidad** | Colapso √©poca 6 | Estable 25 √©pocas | +**19 √©pocas** | √âXITO TOTAL |

**Score Kaggle 0.86380:**

- Posici√≥n 7 de aproximadamente 50 participantes| **Overfitting** | Severo desde √©poca 6 | Completamente controlado | Resuelto | √âXITO TOTAL |

- Top 15% del ranking

- Gap con el primer puesto (0.968): -10.4%La distribuci√≥n est√° balanceada (40/60 es aceptable) pero el score es horrible. Esto significa que **el modelo CNN desde cero simplemente NO GENERALIZA**.| **Gap Train/Val** | 18.6 puntos | 5.26 puntos | **-13.34 puntos** | √âXITO TOTAL |

- Gap con el tercero (0.957): -9.3%

| **Mejora vs Baseline** | +42.5% | **+52.6%** | +**10.1 puntos** | EXCEPCIONAL |

### Qu√© aprend√≠ de verdad

**Diagn√≥stico:**

**Por qu√© funcion√≥ tan bien:**

- Val: 74% (aprende validaci√≥n)### Conclusiones y Pr√≥ximos Pasos

VGG16 elimina completamente toda la variabilidad y los problemas de entrenar desde cero. Las features pre-aprendidas son muy robustas y generalizan incre√≠blemente bien porque fueron entrenadas con millones de im√°genes variadas.

- Supp: 60% (empieza a fallar)

Binary classification + umbral 0.5 expl√≠cito es mucho m√°s estable que categorical para problemas de 2 clases. Y lo m√°s importante: verifiqu√© la distribuci√≥n de predicciones antes de enviar a Kaggle (est√° perfectamente balanceada).

- Kaggle: **54%** (falla completamente)**Lo que funcion√≥ EXCELENTE:**

**Conclusi√≥n:** Transfer Learning fue absolutamente la decisi√≥n correcta. Deber√≠a haberlo usado desde el principio, me habr√≠a ahorrado 3 d√≠as de iteraciones fallidas y 2 submissions desperdiciados. La lecci√≥n es clara: no intentes ser h√©roe entrenando CNNs complejas desde cero cuando tienes modelos pre-entrenados disponibles.

- **Data Augmentation es LA CLAVE**: Resolvi√≥ completamente el overfitting

**Referencias sobre por qu√© TL funciona:**

- Paper sobre feature transfer: https://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks**Conclusi√≥n brutal**: CNN desde cero tiene demasiada variabilidad y no es estable para este problema. Con solo 3 submissions restantes, no puedo seguir apostando por esto.- **Mejor resultado hist√≥rico**: 0.848 val_accuracy (+3.7% vs Iter1)  

- Blog de Jeremy Howard (fast.ai) sobre TL: https://www.fast.ai/posts/2016-10-08-teaching-philosophy.html

- **Estabilidad dram√°ticamente mejorada**: 15 √©pocas adicionales sin colapso

---

---- **Gap train/val controlado**: De 18.6 a ~5.8 puntos

## RESUMEN FINAL: De 0.54 a 0.86

- **ReduceLROnPlateau efectivo**: Ajustes finos autom√°ticos

**Timeline completo del proyecto:**

## üìä RESUMEN ITERACIONES 3-4: LECCI√ìN DE HUMILDAD- **Early Stopping como red de seguridad**: Protege contra inestabilidad tard√≠a

| Iter | Modelo | Val Acc | Kaggle | Aprendizaje clave |

|------|--------|---------|--------|-------------------|

| 1 | CNN base | 0.811 | - | Overfitting brutal sin augmentation |

| 2 | + Augmentation | 0.868 | - | Augmentation es obligatorio |**Realidad check**: Validation accuracy alta NO GARANTIZA buen score en Kaggle. El modelo sobreajust√≥ al validation set tambi√©n.**Problema detectado (√©pocas 22-23):**

| 3 | Re-train | 0.858 | 0.588 | Val accuracy puede mentir |

| 4 | Bug fixes | 0.742 | 0.547 | CNN desde cero no generaliza bien |- **Inestabilidad tard√≠a**: Ca√≠da s√∫bita de 0.848 ‚Üí 0.717 en √©poca 22

| 5 | VGG16 TL | 0.975 | 0.864 | Transfer Learning funciona |

**N√∫meros brutales:**- **Posible causa**: Learning rate muy bajo (5e-4) creando fluctuaciones

**Mejora total:** 0.547 ‚Üí 0.864 = +58% de score (de bottom a top 15%)

- Val accuracy: 74% ‚Üí Kaggle: **54%** (-20%)- **Protecci√≥n**: Early stopping recuperar√° mejores pesos (√©poca 21)

**Las 5 lecciones clave que me llevo:**

- 2 submissions desperdiciados

1. **Data augmentation no es opcional** en computer vision con datos limitados (Iter 1‚Üí2)

2. **Validation accuracy local puede mentir brutalmente**, no conf√≠es ciegamente (Iter 2‚Üí3)- Ranking: Bottom del leaderboard**Resultados finales y Predicci√≥n Ranking:**

3. **Binary classification es m√°s estable que categorical** para problemas de 2 clases (Iter 3‚Üí4)

4. **CNN desde cero tiene demasiada variabilidad** para ser confiable en producci√≥n (Iter 1-4)- **Validation accuracy final**: **0.8684** (√©poca 25)

5. **Transfer Learning es el camino correcto desde el principio**, no la √∫ltima opci√≥n (Iter 5)

**Decisi√≥n cr√≠tica**: Abandonar CNN desde cero. Es el momento de **Transfer Learning**.- **Supplementary accuracy**: **0.7533** (excelente generalizaci√≥n)

**Posici√≥n actual:** 7 de 50 (Top 15%)  

**Submissions restantes:** 2 de 5  - **Public leaderboard estimado**: **0.83-0.87** (basado en val_acc)

**Margen de mejora:** ~10 puntos hasta top 3

**Justificaci√≥n**: Los modelos pre-entrenados (VGG16, ResNet) ya aprendieron features generales de millones de im√°genes. Solo necesito ajustar las √∫ltimas capas para perros vs gatos. Esto deber√≠a ser mucho m√°s estable.- **Posici√≥n esperada**: **Top 15-20%** (muy competitivo)

---

- **Mejora vs baseline (0.569)**: +**52.6%** (extraordinario)

## Pr√≥ximos Pasos

---- **Gap val/supplementary**: 11.5 puntos (razonable para generalizaci√≥n)

Con 2 submissions restantes y estando en posici√≥n 7, tengo b√°sicamente tres opciones:



**Opci√≥n A - Fine-tuning VGG16:**

- Descongelar las √∫ltimas 2-3 capas convolucionales del VGG16## ITERACI√ìN 5 - Transfer Learning VGG16 ‚úÖ**Estrategia Iteraci√≥n 3 (si fuera necesaria):**

- Learning rate muy bajo (1e-5 o menos)

- **Riesgo:** Puede empeorar si sobreajusto los pesos pre-entrenados1. **Transfer Learning**: VGG16/ResNet50 pre-entrenado

- **Potencial:** +2-4% de score

**Fecha**: 05/11/2025  2. **Learning Rate Scheduling**: Cosine annealing 

**Opci√≥n B - Ensemble de modelos:**

- Entrenar VGG16 + ResNet50 por separado**Estado**: ‚úÖ **MODELO ACTIVO**  3. **Ensemble**: Promedio de m√∫ltiples modelos

- Promediar las predicciones de ambos

- **Riesgo:** Puede que no mejore mucho para el esfuerzo**Kaggle Score**: 0.86380 üéØ  4. **Image size**: 224x224 ‚Üí 384x384 para m√°s detalles

- **Potencial:** +1-3% de score

**Ranking**: #7 de ~50 participantes

**Opci√≥n C - Test Time Augmentation (TTA):**

- Predecir cada imagen 5 veces con transformaciones diferentes (rotaciones, flips)**VEREDICTO ITERACI√ìN 2:**

- Promediar los resultados de las 5 predicciones

- **Riesgo:** Muy bajo, casi seguro que mejora algo### Cambios Realizados**√âXITO EXCEPCIONAL** - Objetivos ampliamente superados:

- **Potencial:** +1-2% de score

- Objetivo: >0.82 ‚Üí **Conseguido: 0.8684** (+5.8% extra)

**Mi decisi√≥n:** Voy a probar primero **Test Time Augmentation (Opci√≥n C)** porque es la opci√≥n de menor riesgo y casi seguro que da alguna mejora. Si funciona bien y mejoro posici√≥n, uso el √∫ltimo submission para probar ensemble o fine-tuning dependiendo de c√≥mo est√© el ranking.

**Arquitectura completamente nueva:**- Supplementary: esperado 0.78-0.82 ‚Üí **Conseguido: 0.7533** 

**Referencias para pr√≥ximos pasos:**

- TTA tutorial: https://machinelearningmastery.com/how-to-use-test-time-augmentation-to-improve-model-performance-for-image-classification/- Base: VGG16 pre-entrenado en ImageNet- Overfitting: **Completamente eliminado**

- Fine-tuning guide: https://keras.io/guides/transfer_learning/ (secci√≥n "Fine-tuning")

- Ensemble methods: https://www.kaggle.com/getting-started/18153- Todas las capas convolucionales **CONGELADAS**- Generalizaci√≥n: **Excelente** (gap razonable val/supp)



---- Cabecera personalizada: Dense(256) + Dropout(0.5) + Dense(1)- Ranking: **Top 15-20% probable**



## Referencias Generales- Binary classification con sigmoid



### Cursos y Material Educativo- Image size: 224x224 (requerido por VGG16)**Status: EXPERIMENTAL - No enviado a Kaggle**

- **42AI Bootcamp Machine Learning**: https://github.com/42-AI (material que usamos en clase)

- **Stanford CS231n** - CNNs for Visual Recognition: http://cs231n.stanford.edu/ (EL curso de referencia)

- **Fast.ai Practical Deep Learning**: https://course.fast.ai/ (muy pr√°ctico, menos teor√≠a)

- **Andrew Ng Deep Learning Specialization**: https://www.coursera.org/specializations/deep-learning (m√°s te√≥rico)**Configuraci√≥n:****Conclusi√≥n t√©cnica:**



### Papers Importantes- √âpocas: 15 (TL converge r√°pido)La combinaci√≥n de Data Augmentation + ReduceLROnPlateau + Early Stopping ha demostrado ser **altamente efectiva** para esta arquitectura CNN desde cero. El modelo ha alcanzado un nivel de rendimiento **excepcional** que rivaliza con arquitecturas m√°s complejas.

- **VGG16 Paper** (Simonyan & Zisserman, 2014): https://arxiv.org/abs/1409.1556

- **Data Augmentation Paper** (Perez & Wang, 2017): https://arxiv.org/abs/1712.04621- Learning rate: 0.0001 (muy bajo para fine-tuning)

- **Transfer Learning Survey** (Pan & Yang, 2010): https://ieeexplore.ieee.org/document/5288526

- **Feature Transferability**: https://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks- Data augmentation: Conservador (TL no necesita tanto)---



### Documentaci√≥n T√©cnica- Tiempo entrenamiento: 11 minutos vs 20 min CNN

- **Keras Documentation**: https://keras.io/ (mi referencia principal)

- **TensorFlow Transfer Learning Tutorial**: https://www.tensorflow.org/tutorials/images/transfer_learning## ITERACI√ìN 3 - Re-entrenamiento con Diferente Inicializaci√≥n

- **Kaggle Learn Computer Vision**: https://www.kaggle.com/learn/computer-vision

### Justificaci√≥n T√©cnica

### Videos YouTube

- **Yannic Kilcher - Data Augmentation**: https://www.youtube.com/watch?v=Zrp2b8qXHYk (explica muy bien por qu√© funciona)**Fecha**: 05/11/2025

- **Aladdin Persson - VGG16 Explained**: https://www.youtube.com/watch?v=ACmuBbuXn20 (implementaci√≥n desde cero)

- **StatQuest - Neural Networks**: https://www.youtube.com/watch?v=CqOfi41LfDw (para entender conceptos base)Transfer Learning es el est√°ndar en computer vision cuando no tienes millones de im√°genes. VGG16 ya aprendi√≥ features como bordes, texturas y formas en 14M de im√°genes de ImageNet. Congelando sus capas, solo entreno la decisi√≥n final "¬øesto es perro o gato?" sobre esas features pre-aprendidas.



### Comunidad y Foros### Descripci√≥n del Cambio

- **Kaggle Dogs vs Cats Discussions**: https://www.kaggle.com/c/dogs-vs-cats/discussion

- **Stack Overflow Deep Learning**: https://stackoverflow.com/questions/tagged/deep-learningLa idea viene de los tutoriales de Stanford CS231n y las recomendaciones de fast.ai sobre "cuando usar transfer learning" (respuesta: siempre que puedas).

- **Reddit r/MachineLearning**: https://www.reddit.com/r/MachineLearning/

Re-entrenar el **mismo modelo** de la Iteraci√≥n 2 con **diferente semilla aleatoria** para explorar variabilidad en el entrenamiento y potencialmente conseguir mejor performance.

---

**Referencias:**

**√öltima actualizaci√≥n:** 06/11/2025  

**Estado:** ACTIVO - Top 15% del ranking  - Stanford CS231n - Transfer Learning: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf**Configuraci√≥n id√©ntica a Iteraci√≥n 2:**

**Pr√≥xima iteraci√≥n:** Test Time Augmentation para intentar top 10

- Fast.ai - Practical Deep Learning: https://course.fast.ai/- Arquitectura: 3 bloques Conv (32, 64, 128) + BatchNorm + MaxPooling

- Keras Tutorial - Transfer Learning Guide: https://keras.io/guides/transfer_learning/- Data Augmentation: RandomFlip, RandomRotation, RandomZoom, RandomTranslation

- Paper VGG16 - "Very Deep Convolutional Networks": https://arxiv.org/abs/1409.1556- Dropout: 0.6

- YouTube - Aladdin Persson VGG16 Explained: https://www.youtube.com/watch?v=ACmuBbuXn20- Early Stopping + ReduceLROnPlateau

- Optimizador: RMSprop (lr=0.001 inicial)

### Resultados- Batch size: 125, Image size: 256x256



| M√©trica | CNN Iter 4 | VGG16 Iter 5 | Mejora |### Hip√≥tesis/Justificaci√≥n

|---------|------------|--------------|--------|

| Val Accuracy | 0.742 | **0.975** | +31.4% |**Concepto de re-entrenamiento aleatorio:**

| Val Precision | 0.842 | **0.966** | +14.8% |En deep learning, diferentes inicializaciones de pesos pueden llevar a resultados significativamente diferentes. La Iteraci√≥n 2 consigui√≥ 0.8684, pero existe la posibilidad de que una nueva inicializaci√≥n encuentre un m√≠nimo local mejor o peor.

| Val Recall | 0.605 | **0.985** | +62.8% |

| Val Loss | 0.586 | **0.075** | -87.2% |**Ventajas esperadas:**

| **Kaggle Score** | 0.547 | **0.864** | **+58.0%** |- Posible mejora sobre 0.8684 con mejor inicializaci√≥n

| Tiempo | 20 min | **11 min** | -45% |- Early stopping m√°s efectivo (evitar inestabilidad √©pocas 22-23)

- Menor tiempo de entrenamiento si converge antes

### An√°lisis del √âxito

**Riesgos:**

**Precisi√≥n/Recall perfectamente balanceados** (96.6% vs 98.5%) indica que el modelo no tiene sesgo hacia ninguna clase. El loss baj√≠simo (0.075) muestra confianza extrema en las predicciones.- Posible rendimiento inferior por mala suerte

- Variabilidad inherente del entrenamiento

**Score Kaggle 0.86380:**

- Posici√≥n: **#7 de ~50**### Resultados Obtenidos

- Top: **15%** del ranking

- Gap con #1 (0.968): -10.4%**M√©tricas de Entrenamiento:**

- Gap con #3 (0.957): -9.3%- Training Accuracy final: 0.8434

- Validation Accuracy final (√©poca 15): **0.8582**

**Por qu√© funcion√≥:**- Validation Loss final: 0.3616

- VGG16 elimina la variabilidad de entrenar desde cero- Total √©pocas completadas: 20 (early stopping activado)

- Features pre-aprendidas son robustas y generalizan bien- Mejores pesos: Restaurados desde √©poca 15

- Binary classification + umbral 0.5 es m√°s estable

- Distribuci√≥n predicciones balanceada (verificado)**Evoluci√≥n por √©pocas clave Iteraci√≥n 3:**



**Conclusi√≥n**: Transfer Learning fue la decisi√≥n correcta. En 11 minutos consegu√≠ mejor resultado que 3 d√≠as iterando con CNN desde cero.| √âpoca | Train Acc | Val Acc | Train Loss | Val Loss | LR | Estado |

|-------|-----------|---------|------------|----------|-----|---------|

---| 15    | **0.843** | **0.858** | **0.379** | **0.362** | **2.5e-4** | **MEJOR √âPOCA** |

| 19    | 0.843     | 0.858   | 0.381      | 0.378    | 2.5e-4 | √öltimo antes ES |

## üìä RESUMEN FINAL: DE 0.54 A 0.86| 20    | -         | -       | -          | -        | -   | **Early Stopping** |



**Timeline completo:****M√©tricas de Evaluaci√≥n:**

- Supplementary Data Accuracy: **0.6533**

| Iter | Modelo | Val Acc | Kaggle | Estado | Aprendizaje |- Supplementary Data Loss: 0.8321

|------|--------|---------|--------|--------|-------------|

| 1 | CNN base | 0.811 | - | ‚ùå | Overfitting brutal |**Kaggle Score:**

| 2 | + Augmentation | 0.868 | - | ‚ùå | Mejor√≥ local pero no testeado |- Public Score: **0.58768** 

| 3 | Re-train | 0.858 | 0.588 | ‚ùå | Predicciones sesgadas |- Private Score: [Pendiente hasta cierre competici√≥n]

| 4 | Bug fixes | 0.742 | **0.547** | ‚ùå | CNN no generaliza |

| **5** | **VGG16** | **0.975** | **0.864** | ‚úÖ | **Transfer Learning funciona** |### An√°lisis de Resultados - Comparaci√≥n Iteraciones



**Mejora total**: 0.547 ‚Üí 0.864 = **+58% de score****COMPARATIVA ITERACI√ìN 2 vs 3:**



**Lecciones clave:**| M√©trica | Iteraci√≥n 2 | Iteraci√≥n 3 | Diferencia | An√°lisis |

1. Data augmentation es obligatorio (Iter 1‚Üí2)|---------|-------------|-------------|------------|----------|

2. Validation accuracy local puede mentir (Iter 2‚Üí3)| **Val Acc** | **0.8684** | 0.8582 | **-1.02%** | Iter2 mejor |

3. Binary classification m√°s estable que categorical (Iter 3‚Üí4)| **Supp Acc** | **0.7533** | 0.6533 | **-13.27%** | Iter2 significativamente mejor |

4. **CNN desde cero tiene demasiada variabilidad** (Iter 1-4)| **√âpocas** | 25 | **15** | **-10** | Iter3 m√°s eficiente |

5. **Transfer Learning es el camino correcto** (Iter 5)| **Gap Train/Val** | 5.26% | **1.48%** | **-3.78%** | Iter3 m√°s estable |

| **Kaggle Score** | [No enviado] | **0.5877** | - | **DESASTRE TOTAL** |

**Posici√≥n actual**: #7 de ~50 (Top 15%)  | **Early Stop** | No activado | **Activado** | - | Iter3 m√°s conservador |

**Submissions restantes**: 2/5  

**Objetivo siguiente**: Top 5 con fine-tuning o ensemble### An√°lisis del Desastre Kaggle



---**Score obtenido: 0.58768**

**Esperado: ~0.83-0.85**

## Pr√≥ximos Pasos**Gap: -29.4% de lo esperado**



Con 2 submissions restantes y posici√≥n #7, las opciones son:**Diagn√≥stico del problema:**



**Opci√≥n A - Fine-tuning VGG16:****1. üö® Predicci√≥n sesgada hacia una sola clase:**

- Descongelar √∫ltimas 2-3 capas conv- Sample submission: TODOS los labels = 0 (solo perros)

- Learning rate muy bajo (1e-5)- Score ~58% sugiere que el modelo predijo 100% perros

- Riesgo: Puede empeorar- En dataset balanceado (50/50), esto da accuracy ‚âà 50-60%

- Potencial: +2-4% score

**2. üêõ Bug cr√≠tico en generaci√≥n de predicciones:**

**Opci√≥n B - Ensemble:**- Posible error en conversi√≥n probabilidades ‚Üí labels

- VGG16 + ResNet50- Early stopping podr√≠a haber restaurado pesos de √©poca muy temprana

- Promedio de predicciones- Threshold incorrecto (>0.5 vs >0.9, etc.)

- Riesgo: Puede no mejorar mucho

- Potencial: +1-3% score**3. üîÑ Diferencia entorno local vs Kaggle:**

- Librer√≠as diferentes

**Opci√≥n C - Test Time Augmentation:**- Preprocessing distinto

- Predecir 5 veces con rotaciones- Orden de datos diferente

- Promediar resultados

- Riesgo: Muy bajo**4. üìä Early stopping contraproducente:**

- Potencial: +1-2% score- √âpoca 15 podr√≠a ser demasiado temprana

- Modelo no tuvo tiempo de aprender patrones complejos

**Decisi√≥n**: Voy a probar **Opci√≥n C (TTA)** primero porque es la de menor riesgo. Si funciona bien, el √∫ltimo submission lo uso para ensemble.

### Conclusiones y Lecciones Aprendidas

---

**‚ùå Lo que NO funcion√≥:**

## Referencias Generales- **Early stopping agresivo**: Par√≥ demasiado pronto (√©poca 15)

- **Re-entrenamiento**: Nueva semilla dio peor resultado

### Cursos y Tutoriales- **Generaci√≥n predicciones**: Bug cr√≠tico que caus√≥ sesgo hacia una clase

- 42AI Bootcamp - Machine Learning & Deep Learning: https://github.com/42-AI- **Validaci√≥n local**: No predijo el comportamiento en test real

- Stanford CS231n - Convolutional Neural Networks: http://cs231n.stanford.edu/

- Fast.ai - Practical Deep Learning for Coders: https://course.fast.ai/**‚úÖ Lo que S√ç funcion√≥:**

- Andrew Ng - Deep Learning Specialization: https://www.coursera.org/specializations/deep-learning- **Eficiencia**: Entrenamiento m√°s r√°pido (15 vs 25 √©pocas)

- **Estabilidad**: Gap train/val m√≠nimo (1.48%)

### Papers Clave- **Convergencia**: Early stopping funcion√≥ t√©cnicamente

- VGG16: "Very Deep Convolutional Networks for Large-Scale Image Recognition" (Simonyan & Zisserman, 2014): https://arxiv.org/abs/1409.1556

- Data Augmentation: "The Effectiveness of Data Augmentation in Image Classification" (Perez & Wang, 2017): https://arxiv.org/abs/1712.04621**üéØ Pr√≥ximos pasos cr√≠ticos:**

- Transfer Learning: "A Survey on Transfer Learning" (Pan & Yang, 2010): https://ieeexplore.ieee.org/document/52885261. **ARREGLAR BUG**: Revisar c√≥digo de generaci√≥n de predicciones

2. **ELIMINAR Early Stopping**: Entrenar √©pocas fijas (20-25)

### Documentaci√≥n T√©cnica3. **TRANSFER LEARNING**: CNN desde cero tiene demasiada variabilidad

- Keras Official Documentation: https://keras.io/4. **VALIDAR PREDICCIONES**: Verificar distribuci√≥n antes de enviar

- TensorFlow Transfer Learning Guide: https://www.tensorflow.org/tutorials/images/transfer_learning

- Kaggle Learn - Computer Vision: https://www.kaggle.com/learn/computer-vision### Plan de Recuperaci√≥n - Iteraci√≥n 4



### Videos YouTube**Objetivo**: Conseguir score >0.75 (4 submissions restantes)

- Yannic Kilcher - Data Augmentation Explained: https://www.youtube.com/watch?v=Zrp2b8qXHYk

- Aladdin Persson - VGG16 from Scratch: https://www.youtube.com/watch?v=ACmuBbuXn20**Estrategia A - Arreglar CNN desde cero:**

- StatQuest - Neural Networks Explained: https://www.youtube.com/watch?v=CqOfi41LfDw- Misma arquitectura Iter2, SIN early stopping

- √âpocas fijas: 25

### Foros y Comunidad- Revisar c√≥digo predicciones l√≠nea por l√≠nea

- Kaggle Discussions - Dogs vs Cats: https://www.kaggle.com/c/dogs-vs-cats/discussion

- Stack Overflow - Deep Learning Tag: https://stackoverflow.com/questions/tagged/deep-learning**Estrategia B - Transfer Learning:**

- Reddit r/MachineLearning: https://www.reddit.com/r/MachineLearning/- VGG16 pre-entrenado + fine-tuning

- M√°s estable y predecible

---- Menor riesgo de bugs



**√öltima actualizaci√≥n**: 06/11/2025  **Estrategia C - H√≠brida:**

**Estado del proyecto**: ACTIVO - Top 15% del ranking  - CNN corregida + verificaci√≥n predicciones

**Pr√≥xima iteraci√≥n**: Test Time Augmentation (TTA)- Backup con Transfer Learning


**Recomendaci√≥n**: **Estrategia B (Transfer Learning)** - Minimizar riesgos con solo 4 submissions restantes.

### Referencias
- Kaggle Submission Debugging: https://www.kaggle.com/learn/intro-to-machine-learning
- Early Stopping Best Practices: https://keras.io/api/callbacks/early_stopping/
- Transfer Learning Guide: https://keras.io/guides/transfer_learning/

### Referencias
- Keras Data Augmentation: https://keras.io/api/layers/preprocessing_layers/image_augmentation/
- Early Stopping: https://keras.io/api/callbacks/early_stopping/
- ReduceLROnPlateau: https://keras.io/api/callbacks/reduce_lr_on_plateau/
- Understanding Data Augmentation: https://keras.io/guides/preprocessing_layers/
