{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR                                ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        07/11/2025  -  05:00:00           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    10/11/2025  -  11:34:43           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4be98b",
   "metadata": {},
   "source": [
    "# Competición Perretes y Gatos\n",
    "\n",
    "## Iteración 9 - Migración a EfficientNetB3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf74799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix protobuf compatibility issue\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.applications import EfficientNetB3\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "seed = 42\n",
    "keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Rutas dataset\n",
    "DATASET_NAME = \"u-tad-dogs-vs-cats-2025\"\n",
    "TRAIN_PATH = f\"/kaggle/input/{DATASET_NAME}/train/train\"\n",
    "TEST_PATH = f\"/kaggle/input/{DATASET_NAME}/test/test\"\n",
    "SUPP_PATH = f\"/kaggle/input/{DATASET_NAME}/supplementary_data/supplementary_data\"\n",
    "\n",
    "print(\"Keras:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018cee3",
   "metadata": {},
   "source": [
    "## Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46884011",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 300  # EfficientNetB3 nativo (mejor que 260)\n",
    "BATCH_SIZE = 64\n",
    "N_FINE_TUNE_LAYERS = 20  # Más capas para fine-tuning\n",
    "EPOCHS_TL = 12  # Reducido con mejor early stopping\n",
    "EPOCHS_FT = 8   # Reducido con mejor early stopping\n",
    "DROPOUT_RATE = 0.3  # Menos dropout (modelo más grande)\n",
    "LABEL_SMOOTHING = 0.05  # Reducido (0.1 era demasiado)\n",
    "# MIXUP_ALPHA eliminado - demasiado agresivo\n",
    "\n",
    "print(f\"Modelo: EfficientNetB3 (optimizado)\")\n",
    "print(f\"Resolución: {IMG_SIZE}x{IMG_SIZE} (nativa B3)\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Dropout: {DROPOUT_RATE}\")\n",
    "print(f\"Label Smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"Mixup: DISABLED (mejor sin él)\")\n",
    "print(f\"Fine-tune layers: {N_FINE_TUNE_LAYERS}\")\n",
    "print(f\"Épocas TL: {EPOCHS_TL}\")\n",
    "print(f\"Épocas FT: {EPOCHS_FT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31569acc",
   "metadata": {},
   "source": [
    "## Carga de Datos\n",
    "\n",
    "Todo igual q antes, pero con batch_size=16 para que q entre B7 en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9353c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "validation_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TEST_PATH,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "supplementary_dataset = keras.utils.image_dataset_from_directory(\n",
    "    SUPP_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_dataset)}\")\n",
    "print(f\"Validation batches: {len(validation_dataset)}\")\n",
    "print(f\"Test batches: {len(test_dataset)}\")\n",
    "print(f\"Supplementary batches: {len(supplementary_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db278a00",
   "metadata": {},
   "source": [
    "## Mixup Augmentation\n",
    "\n",
    "Técnica avanzada que mezcla imágenes y labels para mejor generalización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b92ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIXUP ELIMINADO - No mejoraba resultados y añadía overhead\n",
    "# Con EfficientNetB3 + más datos, mejor enfocarse en calidad de imágenes\n",
    "print(\"Mixup disabled for better convergence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b3f64",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Mismo q antes \n",
    "- Flip horizontal + vertical\n",
    "- Rotation ±20%\n",
    "- Zoom ±20%\n",
    "- Translation ±15%\n",
    "- Contrast ±20%\n",
    "- Brightness ±20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39017ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation OPTIMIZADO - Menos agresivo\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),  # Solo horizontal (más natural)\n",
    "    keras.layers.RandomRotation(0.1),  # Reducido de 0.15\n",
    "    keras.layers.RandomZoom(0.1),  # Reducido de 0.15\n",
    "    keras.layers.RandomTranslation(height_factor=0.08, width_factor=0.08),  # Reducido\n",
    "    keras.layers.RandomContrast(0.1),  # Reducido\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "# SIN Mixup - aplicar solo augmentation estándar\n",
    "train_dataset_augmented = train_dataset.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y),\n",
    "    num_parallel_calls=tf_data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(\"Augmentation OPTIMIZADO (menos agresivo, sin Mixup)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo EfficientNetB3 pre-entrenado\n",
    "efficientnet_base = EfficientNetB3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Congelo todo inicialmente\n",
    "for layer in efficientnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construcción OPTIMIZADA - Menos capas Dense\n",
    "efficientnet_model = Sequential([\n",
    "    efficientnet_base,\n",
    "    keras.layers.BatchNormalization(),\n",
    "    Dense(256, activation='relu'),  # Reducido de 512\n",
    "    Dropout(DROPOUT_RATE),\n",
    "    Dense(128, activation='relu'),  # Reducido de 256\n",
    "    Dropout(DROPOUT_RATE / 2),  # Menos dropout en segunda capa\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar con AdamW + Cosine Decay\n",
    "initial_lr = 1e-3\n",
    "efficientnet_model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        learning_rate=initial_lr,\n",
    "        weight_decay=1e-4\n",
    "    ),\n",
    "    loss=keras.losses.BinaryCrossentropy(\n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "efficientnet_model.summary()\n",
    "print(f\"EfficientNetB3 layers: {len(efficientnet_base.layers)}\")\n",
    "print(f\"Arquitectura OPTIMIZADA: Menos capas Dense, dropout reducido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb736c",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Entreno solo las capas Dense q añadí, con EfficientNetB7 congelado.\n",
    "\n",
    "**15 épocas** con early stopping (patience=5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning OPTIMIZADO\n",
    "print(\"Starting Transfer Learning (optimized)...\")\n",
    "\n",
    "# Cosine Decay para mejor convergencia\n",
    "cosine_decay_tl = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=EPOCHS_TL * len(train_dataset_augmented),\n",
    "    alpha=0.1\n",
    ")\n",
    "\n",
    "# RECOMPILAR con Cosine Decay (no asignar después)\n",
    "efficientnet_model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        learning_rate=cosine_decay_tl,  # Pasar el schedule aquí\n",
    "        weight_decay=1e-4\n",
    "    ),\n",
    "    loss=keras.losses.BinaryCrossentropy(\n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "efficientnet_history_tl = efficientnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=EPOCHS_TL,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        # ReduceLROnPlateau ELIMINADO - incompatible con CosineDecay\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'best_model_tl.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Transfer Learning completed (with Cosine Decay)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e0e53",
   "metadata": {},
   "source": [
    "## Visualización TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ed645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos Transfer Learning (robustos a nombres de métricas)\n",
    "try:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(efficientnet_history_tl.history.get('loss', []), label='Train')\n",
    "    plt.plot(efficientnet_history_tl.history.get('val_loss', []), label='Validation')\n",
    "    plt.title('Loss - Transfer Learning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(efficientnet_history_tl.history.get('accuracy', []), label='Train')\n",
    "    plt.plot(efficientnet_history_tl.history.get('val_accuracy', []), label='Validation')\n",
    "    plt.title('Accuracy - Transfer Learning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Detect keys for precision/recall (Keras names may vary)\n",
    "    precision_key = next((k for k in ['precision_1', 'precision'] if k in efficientnet_history_tl.history), None)\n",
    "    recall_key = next((k for k in ['recall_1', 'recall'] if k in efficientnet_history_tl.history), None)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if precision_key and recall_key:\n",
    "        plt.plot(efficientnet_history_tl.history[precision_key], label='Precision')\n",
    "        plt.plot(efficientnet_history_tl.history[recall_key], label='Recall')\n",
    "    else:\n",
    "        plt.plot([], [], label='Precision (n/a)')\n",
    "        plt.plot([], [], label='Recall (n/a)')\n",
    "\n",
    "    plt.title('Precision & Recall - Transfer Learning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Plotting failed ({e}). Continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60241282",
   "metadata": {},
   "source": [
    "## Evaluación Supplementary (post-TL)\n",
    "\n",
    "Veo cómo va ANTES del fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on supplementary dataset (post-TL)...\")\n",
    "\n",
    "efficientnet_supp_results_tl = efficientnet_model.evaluate(supplementary_dataset, verbose=1)\n",
    "efficientnet_supp_accuracy_tl = efficientnet_supp_results_tl[1]\n",
    "\n",
    "print(f\"Supplementary Accuracy (TL): {efficientnet_supp_accuracy_tl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a43b7",
   "metadata": {},
   "source": [
    "## Fine-tuning - DESCONGELAR 15 CAPAS\n",
    "\n",
    "Ahora descongelo las **últimas 15 capas** de B3 (optimizado).\n",
    "\n",
    "LR MUY bajo (5e-6) para que no romper lo pre-entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67319151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelo MÁS capas de B3 (20 en vez de 15)\n",
    "print(f\"Unfreezing last {N_FINE_TUNE_LAYERS} layers...\")\n",
    "\n",
    "for layer in efficientnet_base.layers[-N_FINE_TUNE_LAYERS:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Cosine Decay para fine-tuning\n",
    "cosine_decay_ft = keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=3e-6,  # Más bajo (era 5e-6)\n",
    "    decay_steps=EPOCHS_FT * len(train_dataset_augmented),\n",
    "    alpha=0.05\n",
    ")\n",
    "\n",
    "# Recompilo con AdamW + Cosine Decay\n",
    "efficientnet_model.compile(\n",
    "    optimizer=keras.optimizers.AdamW(\n",
    "        learning_rate=cosine_decay_ft,\n",
    "        weight_decay=3e-5  # Menos weight decay\n",
    "    ),\n",
    "    loss=keras.losses.BinaryCrossentropy(\n",
    "        label_smoothing=LABEL_SMOOTHING\n",
    "    ),\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "trainable_count = sum([layer.trainable for layer in efficientnet_base.layers])\n",
    "print(f\"Trainable layers: {trainable_count}/{len(efficientnet_base.layers)}\")\n",
    "print(f\"Optimizer: AdamW (lr=3e-6 Cosine Decay, weight_decay=3e-5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4252f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning OPTIMIZADO\n",
    "print(\"Starting Fine-tuning (optimized)...\")\n",
    "\n",
    "efficientnet_history_ft = efficientnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=EPOCHS_FT,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=2,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            'best_model_ft.keras',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc30c7",
   "metadata": {},
   "source": [
    "## Visualización Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(efficientnet_history_ft.history.get('loss', []), label='Train')\n",
    "    plt.plot(efficientnet_history_ft.history.get('val_loss', []), label='Validation')\n",
    "    plt.title('Loss - Fine-tuning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(efficientnet_history_ft.history.get('accuracy', []), label='Train')\n",
    "    plt.plot(efficientnet_history_ft.history.get('val_accuracy', []), label='Validation')\n",
    "    plt.title('Accuracy - Fine-tuning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Detect keys for precision/recall (Keras names may vary)\n",
    "    precision_key_ft = next((k for k in ['precision_1', 'precision'] if k in efficientnet_history_ft.history), None)\n",
    "    recall_key_ft = next((k for k in ['recall_1', 'recall'] if k in efficientnet_history_ft.history), None)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if precision_key_ft and recall_key_ft:\n",
    "        plt.plot(efficientnet_history_ft.history[precision_key_ft], label='Precision')\n",
    "        plt.plot(efficientnet_history_ft.history[recall_key_ft], label='Recall')\n",
    "    else:\n",
    "        plt.plot([], [], label='Precision (n/a)')\n",
    "        plt.plot([], [], label='Recall (n/a)')\n",
    "\n",
    "    plt.title('Precision & Recall - Fine-tuning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Plotting failed ({e}). Continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75810c60",
   "metadata": {},
   "source": [
    "## Evaluación Final Supplementary (post-FT)\n",
    "\n",
    "Evaluación final en supplementary dataset SIN augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dbebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on supplementary dataset (post-FT, NO augmentation)...\")\n",
    "\n",
    "efficientnet_supp_results_ft = efficientnet_model.evaluate(supplementary_dataset, verbose=1)\n",
    "efficientnet_supp_accuracy_ft = efficientnet_supp_results_ft[1]\n",
    "\n",
    "print(f\"Supplementary Accuracy (FT): {efficientnet_supp_accuracy_ft:.4f}\")\n",
    "print(f\"Expected Kaggle score: ~{efficientnet_supp_accuracy_ft + 0.01:.4f} - {efficientnet_supp_accuracy_ft + 0.03:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cd907b",
   "metadata": {},
   "source": [
    "## Test-Time Augmentation (TTA)\n",
    "\n",
    "Aplicamos múltiples augmentations a cada imagen de test y promediamos las predicciones para mayor robustez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración TTA\n",
    "N_TTA = 10  # Número de augmentations por imagen (10 = buen balance precisión/tiempo)\n",
    "\n",
    "print(f\"Model: EfficientNetB3\")\n",
    "print(f\"Resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"TTA enabled: {N_TTA} augmentations per image\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Augmentation MÁS SUAVE para TTA (solo cambios ligeros)\n",
    "tta_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.05),  # Muy leve\n",
    "    keras.layers.RandomZoom(0.05),      # Muy leve\n",
    "    keras.layers.RandomTranslation(0.03, 0.03),  # Muy leve\n",
    "], name=\"tta_augmentation\")\n",
    "\n",
    "# Función para aplicar TTA\n",
    "def predict_with_tta(model, dataset, n_tta=N_TTA):\n",
    "    \"\"\"\n",
    "    Realiza predicciones con Test-Time Augmentation\n",
    "    \n",
    "    Args:\n",
    "        model: Modelo entrenado\n",
    "        dataset: Dataset de test\n",
    "        n_tta: Número de augmentations por imagen\n",
    "    \n",
    "    Returns:\n",
    "        Array de predicciones promediadas\n",
    "    \"\"\"\n",
    "    print(f\"Starting TTA predictions ({n_tta} augmentations)...\")\n",
    "    \n",
    "    # Primera predicción SIN augmentation (baseline)\n",
    "    predictions_list = [model.predict(dataset, verbose=0)]\n",
    "    print(f\"✓ Baseline prediction (1/{n_tta+1}) completed\")\n",
    "    \n",
    "    # Predicciones CON augmentation\n",
    "    for i in range(n_tta):\n",
    "        # Aplicar augmentation al dataset\n",
    "        augmented_dataset = dataset.map(\n",
    "            lambda x: tta_augmentation(x, training=True),\n",
    "            num_parallel_calls=tf_data.AUTOTUNE\n",
    "        )\n",
    "        \n",
    "        # Predecir\n",
    "        preds = model.predict(augmented_dataset, verbose=0)\n",
    "        predictions_list.append(preds)\n",
    "        \n",
    "        print(f\"✓ TTA prediction ({i+2}/{n_tta+1}) completed\")\n",
    "    \n",
    "    # Promediar todas las predicciones\n",
    "    predictions_avg = np.mean(predictions_list, axis=0)\n",
    "    \n",
    "    print(f\"TTA completed: {n_tta+1} predictions averaged\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    return predictions_avg\n",
    "\n",
    "# Ejecutar TTA\n",
    "predictions = predict_with_tta(efficientnet_model, test_dataset, n_tta=N_TTA)\n",
    "\n",
    "print(f\"Total predictions: {len(predictions)}\")\n",
    "print(f\"Prediction range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "print(f\"Prediction mean: {predictions.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero submission.csv con TTA\n",
    "test_filenames = test_dataset.file_paths\n",
    "ids = [int(os.path.splitext(os.path.basename(f))[0]) for f in test_filenames]\n",
    "\n",
    "# Threshold optimizado: 0.5 es estándar, pero puedes ajustar si ves sesgo\n",
    "predictions_binary = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'label': predictions_binary\n",
    "})\n",
    "\n",
    "submission_df = submission_df.sort_values('id')\n",
    "\n",
    "# Estadísticas finales\n",
    "print(\"=\" * 60)\n",
    "print(\"SUBMISSION STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples: {len(submission_df)}\")\n",
    "print(f\"Class 0 (Cat): {(submission_df['label'] == 0).sum()} ({(submission_df['label'] == 0).sum() / len(submission_df) * 100:.2f}%)\")\n",
    "print(f\"Class 1 (Dog): {(submission_df['label'] == 1).sum()} ({(submission_df['label'] == 1).sum() / len(submission_df) * 100:.2f}%)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"✓ Submission saved to submission.csv\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
