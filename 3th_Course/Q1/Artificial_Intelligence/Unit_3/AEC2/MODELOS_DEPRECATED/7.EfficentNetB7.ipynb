{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR                                ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        07/11/2025  -  05:21:23           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    09/11/2025  -  19:29:12           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4be98b",
   "metadata": {},
   "source": [
    "# Competición Perretes y Gatos\n",
    "\n",
    "## Iteración 7 - EfficientNetB7\n",
    "\n",
    "Modelo para mi bastante sobredimensionado, la idea es probar el top de la familia, aunque va a ser eterno el entrenamiento. \n",
    "\n",
    "**kaggle score**: 0.94962, mejora elevada pero he sacrificado casi 4h de entrenamiento y el modelo era innecesariamente grande. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf74799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras.applications import EfficientNetB7  \n",
    "\n",
    "seed = 42\n",
    "keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Rutas dataset\n",
    "DATASET_NAME = \"u-tad-dogs-vs-cats-2025\"\n",
    "TRAIN_PATH = f\"/kaggle/input/{DATASET_NAME}/train/train\"\n",
    "TEST_PATH = f\"/kaggle/input/{DATASET_NAME}/test/test\"\n",
    "SUPP_PATH = f\"/kaggle/input/{DATASET_NAME}/supplementary_data/supplementary_data\"\n",
    "\n",
    "print(\"Keras:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3018cee3",
   "metadata": {},
   "source": [
    "## Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46884011",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_HIGH_RES = True  # 384x384 \n",
    "N_TTA_AUGMENTATIONS = 10  # TTA \n",
    "DROPOUT_RATE = 0.6  # Regularización fuerte\n",
    "N_FINE_TUNE_LAYERS = 30  # REDUCIDO de 40 para que acelerar\n",
    "EPOCHS_TL = 10  # REDUCIDO de 15\n",
    "EPOCHS_FT = 10  # REDUCIDO de 15\n",
    "\n",
    "# Parámetros dinámicos\n",
    "IMG_SIZE = 384 if USE_HIGH_RES else 224\n",
    "BATCH_SIZE = 16  # Más bajo para que B7 (consume más RAM)\n",
    "\n",
    "print(f\"Modelo: EfficientNetB7\")\n",
    "print(f\"Resolución: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"TTA augmentations: {N_TTA_AUGMENTATIONS}\")\n",
    "print(f\"Dropout: {DROPOUT_RATE}\")\n",
    "print(f\"Fine-tune layers: {N_FINE_TUNE_LAYERS}\")\n",
    "print(f\"Épocas TL: {EPOCHS_TL}\")\n",
    "print(f\"Épocas FT: {EPOCHS_FT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31569acc",
   "metadata": {},
   "source": [
    "## Carga de Datos\n",
    "\n",
    "Todo igual q antes, pero con batch_size=16 pa q entre B7 en memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9353c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo datos con ALTA resolución (384x384)\n",
    "train_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "validation_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TEST_PATH,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "supplementary_dataset = keras.utils.image_dataset_from_directory(\n",
    "    SUPP_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_dataset)}\")\n",
    "print(f\"Validation batches: {len(validation_dataset)}\")\n",
    "print(f\"Test batches: {len(test_dataset)}\")\n",
    "print(f\"Supplementary batches: {len(supplementary_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660b3f64",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Mismo q antes \n",
    "- Flip horizontal + vertical\n",
    "- Rotation ±20%\n",
    "- Zoom ±20%\n",
    "- Translation ±15%\n",
    "- Contrast ±20%\n",
    "- Brightness ±20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39017ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation para que evitar overfitting\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    keras.layers.RandomRotation(0.2),\n",
    "    keras.layers.RandomZoom(0.2),\n",
    "    keras.layers.RandomTranslation(height_factor=0.15, width_factor=0.15),\n",
    "    keras.layers.RandomContrast(0.2),\n",
    "    keras.layers.RandomBrightness(0.2),\n",
    "], name=\"data_augmentation_aggressive\")\n",
    "\n",
    "# Aplico augmentation\n",
    "train_dataset_augmented = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "validation_dataset_augmented = validation_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb11abd",
   "metadata": {},
   "source": [
    "## EfficientNetB7 - Construcción\n",
    "\n",
    "**EfficientNetB7 stats:**\n",
    "- 66M parámetros (B3 tiene 12M)\n",
    "- Top-1 Accuracy ImageNet: **84.3%** (B3: 81.6%)\n",
    "- Resolución nativa: 600x600 (aguanta perfectamente 384x384)\n",
    "- MÁS LENTO pero MÁS PRECISO\n",
    "\n",
    "Arquitectura:\n",
    "- EfficientNetB7 base (congelado)\n",
    "- BatchNormalization\n",
    "- Dense(512) + Dropout(0.6)\n",
    "- BatchNormalization\n",
    "- Dense(256) + Dropout(0.6)\n",
    "- Dense(1, sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f44d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo EfficientNetB7 pre-entrenado\n",
    "efficientnet_base = EfficientNetB7(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Congelo todo inicialmente\n",
    "for layer in efficientnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construyo modelo con B7\n",
    "efficientnet_model = Sequential([\n",
    "    efficientnet_base,\n",
    "    keras.layers.BatchNormalization(),  # Regularización\n",
    "    Dense(512, activation='relu'),  # Capa densa grande\n",
    "    Dropout(DROPOUT_RATE),  # 0.6\n",
    "    keras.layers.BatchNormalization(),  # Más regularización\n",
    "    Dense(256, activation='relu'),  # Capa intermedia\n",
    "    Dropout(DROPOUT_RATE),  # 0.6\n",
    "    Dense(1, activation='sigmoid')  # Output binario\n",
    "])\n",
    "\n",
    "efficientnet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "efficientnet_model.summary()\n",
    "print(f\"EfficientNetB7 layers: {len(efficientnet_base.layers)}\")\n",
    "print(f\"Trainable layers: {sum([layer.trainable for layer in efficientnet_base.layers])}/{len(efficientnet_base.layers)}\")\n",
    "print(f\"Total parameters: {efficientnet_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fb736c",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Entreno solo las capas Dense q añadí, con EfficientNetB7 congelado.\n",
    "\n",
    "**15 épocas** con early stopping (patience=5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning con EfficientNetB7\n",
    "print(\"Starting Transfer Learning...\")\n",
    "\n",
    "efficientnet_history_tl = efficientnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=EPOCHS_TL,\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Transfer Learning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e0e53",
   "metadata": {},
   "source": [
    "## Visualización TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9ed645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráficos Transfer Learning (robustos a nombres de métricas)\n",
    "try:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(efficientnet_history_tl.history.get('loss', []), label='Train')\n",
    "    plt.plot(efficientnet_history_tl.history.get('val_loss', []), label='Validation')\n",
    "    plt.title('Loss - Transfer Learning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(efficientnet_history_tl.history.get('accuracy', []), label='Train')\n",
    "    plt.plot(efficientnet_history_tl.history.get('val_accuracy', []), label='Validation')\n",
    "    plt.title('Accuracy - Transfer Learning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Detect keys for precision/recall (Keras names may vary)\n",
    "    precision_key = next((k for k in ['precision_1', 'precision'] if k in efficientnet_history_tl.history), None)\n",
    "    recall_key = next((k for k in ['recall_1', 'recall'] if k in efficientnet_history_tl.history), None)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if precision_key and recall_key:\n",
    "        plt.plot(efficientnet_history_tl.history[precision_key], label='Precision')\n",
    "        plt.plot(efficientnet_history_tl.history[recall_key], label='Recall')\n",
    "    else:\n",
    "        plt.plot([], [], label='Precision (n/a)')\n",
    "        plt.plot([], [], label='Recall (n/a)')\n",
    "\n",
    "    plt.title('Precision & Recall - Transfer Learning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Plotting failed ({e}). Continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60241282",
   "metadata": {},
   "source": [
    "## Evaluación Supplementary (post-TL)\n",
    "\n",
    "Veo cómo va ANTES del fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ffb37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on supplementary dataset (post-TL)...\")\n",
    "\n",
    "efficientnet_supp_results_tl = efficientnet_model.evaluate(supplementary_dataset, verbose=1)\n",
    "efficientnet_supp_accuracy_tl = efficientnet_supp_results_tl[1]\n",
    "\n",
    "print(f\"Supplementary Accuracy (TL): {efficientnet_supp_accuracy_tl:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a43b7",
   "metadata": {},
   "source": [
    "## Fine-tuning - DESCONGELAR 30 CAPAS\n",
    "\n",
    "Ahora descongelo las **últimas 30 capas** de B7 (optimizado pa velocidad).\n",
    "\n",
    "LR MUY bajo (5e-6) pa no romper lo pre-entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67319151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelo últimas capas de B7\n",
    "print(f\"Unfreezing last {N_FINE_TUNE_LAYERS} layers...\")\n",
    "\n",
    "for layer in efficientnet_base.layers[-N_FINE_TUNE_LAYERS:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompilo con LR MUY bajo\n",
    "efficientnet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=5e-6),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "trainable_count = sum([layer.trainable for layer in efficientnet_base.layers])\n",
    "print(f\"Trainable layers: {trainable_count}/{len(efficientnet_base.layers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4252f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning con B7\n",
    "print(\"Starting Fine-tuning...\")\n",
    "\n",
    "efficientnet_history_ft = efficientnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=EPOCHS_FT,\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-8\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dc30c7",
   "metadata": {},
   "source": [
    "## Visualización Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f0e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(efficientnet_history_ft.history.get('loss', []), label='Train')\n",
    "    plt.plot(efficientnet_history_ft.history.get('val_loss', []), label='Validation')\n",
    "    plt.title('Loss - Fine-tuning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(efficientnet_history_ft.history.get('accuracy', []), label='Train')\n",
    "    plt.plot(efficientnet_history_ft.history.get('val_accuracy', []), label='Validation')\n",
    "    plt.title('Accuracy - Fine-tuning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Detect keys for precision/recall (Keras names may vary)\n",
    "    precision_key_ft = next((k for k in ['precision_1', 'precision'] if k in efficientnet_history_ft.history), None)\n",
    "    recall_key_ft = next((k for k in ['recall_1', 'recall'] if k in efficientnet_history_ft.history), None)\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    if precision_key_ft and recall_key_ft:\n",
    "        plt.plot(efficientnet_history_ft.history[precision_key_ft], label='Precision')\n",
    "        plt.plot(efficientnet_history_ft.history[recall_key_ft], label='Recall')\n",
    "    else:\n",
    "        plt.plot([], [], label='Precision (n/a)')\n",
    "        plt.plot([], [], label='Recall (n/a)')\n",
    "\n",
    "    plt.title('Precision & Recall - Fine-tuning B7')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Plotting failed ({e}). Continuing...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76da909",
   "metadata": {},
   "source": [
    "## TTA - Test Time Augmentation\n",
    "\n",
    "**10 augmentations**\n",
    "\n",
    "Con B7 va a tardar MÁS (cada predicción es más lenta), pero merece la pena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tta(model, img_array, n_augmentations=10):\n",
    "    \"\"\"TTA con augmentations agresivos\"\"\"\n",
    "    augmentation_layer = keras.Sequential([\n",
    "        keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        keras.layers.RandomRotation(0.2),\n",
    "        keras.layers.RandomZoom(0.2),\n",
    "        keras.layers.RandomTranslation(0.15, 0.15),\n",
    "        keras.layers.RandomContrast(0.2),\n",
    "        keras.layers.RandomBrightness(0.2),\n",
    "    ])\n",
    "    \n",
    "    predictions = []\n",
    "    for _ in range(n_augmentations):\n",
    "        augmented_img = augmentation_layer(img_array, training=True)\n",
    "        pred = model.predict(augmented_img, verbose=0)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    return np.mean(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc2e10",
   "metadata": {},
   "source": [
    "## Predicciones Finales con TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model: EfficientNetB7\")\n",
    "print(f\"TTA augmentations: {N_TTA_AUGMENTATIONS}\")\n",
    "print(f\"Resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for i, batch in enumerate(test_dataset):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processing batch {i+1}/{len(test_dataset)}...\")\n",
    "    pred = predict_with_tta(efficientnet_model, batch, N_TTA_AUGMENTATIONS)\n",
    "    predictions.extend(pred)\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "print(f\"Total predictions: {len(predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859934d",
   "metadata": {},
   "source": [
    "## Generación Submission Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7d3b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero submission.csv\n",
    "test_filenames = test_dataset.file_paths\n",
    "ids = [int(os.path.splitext(os.path.basename(f))[0]) for f in test_filenames]\n",
    "\n",
    "predictions_binary = (predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'label': predictions_binary\n",
    "})\n",
    "\n",
    "submission_df = submission_df.sort_values('id')\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved to submission.csv\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
