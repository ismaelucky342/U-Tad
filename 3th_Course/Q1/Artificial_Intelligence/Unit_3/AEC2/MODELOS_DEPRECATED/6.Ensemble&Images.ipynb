{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "#====================================================================================================#\n",
    "#                                                                                                    #\n",
    "#                                                        ██╗   ██╗   ████████╗ █████╗ ██████╗        #\n",
    "#      Competición - INAR                                ██║   ██║   ╚══██╔══╝██╔══██╗██╔══██╗       #\n",
    "#                                                        ██║   ██║█████╗██║   ███████║██║  ██║       #\n",
    "#      created:        29/10/2025  -  23:00:15           ██║   ██║╚════╝██║   ██╔══██║██║  ██║       #\n",
    "#      last change:    05/11/2025  -  02:55:40           ╚██████╔╝      ██║   ██║  ██║██████╔╝       #\n",
    "#                                                         ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═════╝        #\n",
    "#                                                                                                    #\n",
    "#      Ismael Hernandez Clemente                         ismael.hernandez@live.u-tad.com             #\n",
    "#                                                                                                    #\n",
    "#      Github:                                           https://github.com/ismaelucky342            #\n",
    "#                                                                                                    #\n",
    "#====================================================================================================#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competición Perretes y Gatos\n",
    "\n",
    "## Iteración 6 - Ensemble Learning + TTA + Imagenes mas grandes\n",
    "\n",
    "Mejoró el resultado del anterior pero los 2 modelos inferiores a B3 en tamaño arrastraron la precisión final impidiendo una mejora drastica.\n",
    "\n",
    "El siguiente paso y visto el exito de ImageNetB3 será tirar por modelos de esa familia renunciando a mi pesar Esemble learning. \n",
    "\n",
    "**kaggle score**: 0.90858, aunque mejor se esperaba una mejora mucho mas considerable y gran parte de la precisión es gracias a B3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Imports para ensemble learning\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.applications import VGG16, EfficientNetB3, ResNet50\n",
    "\n",
    "seed = 42\n",
    "keras.utils.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Rutas dataset\n",
    "DATASET_NAME = \"u-tad-dogs-vs-cats-2025\"\n",
    "TRAIN_PATH = f\"/kaggle/input/{DATASET_NAME}/train/train\"\n",
    "TEST_PATH = f\"/kaggle/input/{DATASET_NAME}/test/test\"\n",
    "SUPP_PATH = f\"/kaggle/input/{DATASET_NAME}/supplementary_data/supplementary_data\"\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ITERACION 6 - ENSEMBLE LEARNING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Keras:\", keras.__version__)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Pra pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_HIGH_RES = False  # True = 384x384 (mejor pero 3x más lento), False = 224x224\n",
    "N_TTA_AUGMENTATIONS = 5  # Augmentations por imagen en TTA (reducido para que velocidad)\n",
    "ENSEMBLE_MODELS = ['vgg16', 'efficientnet', 'resnet']  # Modelos a usar en ensemble\n",
    "\n",
    "# Parámetros q cambian según resolución\n",
    "IMG_SIZE = 384 if USE_HIGH_RES else 224\n",
    "BATCH_SIZE = 32 if USE_HIGH_RES else 125\n",
    "\n",
    "print(f\"Resolución: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"TTA augmentations: {N_TTA_AUGMENTATIONS}\")\n",
    "print(f\"Modelos ensemble: {ENSEMBLE_MODELS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de Datos\n",
    "\n",
    "Cargo las imágenes y las divido:\n",
    "- **80% train**: entrenar\n",
    "- **20% validation**:  validar q no se me sobreajuste\n",
    "\n",
    "Todo se redimensiona según la config (224x224 o 384x384)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cargo datos con la resolución configurada\n",
    "train_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "validation_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TRAIN_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=True,\n",
    "    seed=seed,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "test_dataset = keras.utils.image_dataset_from_directory(\n",
    "    TEST_PATH,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "supplementary_dataset = keras.utils.image_dataset_from_directory(\n",
    "    SUPP_PATH,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    shuffle=False,\n",
    "    seed=seed,\n",
    "    interpolation='bilinear',\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Train: {len(train_dataset)} batches\")\n",
    "print(f\"Validation: {len(validation_dataset)} batches\")\n",
    "print(f\"Test: {len(test_dataset)} batches\")\n",
    "print(f\"Supplementary: {len(supplementary_dataset)} batches\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Transformaciones random pa q el modelo no memorice:\n",
    "- **RandomFlip**: voltea horizontal\n",
    "- **RandomRotation**: rota ±10%\n",
    "- **RandomZoom**: zoom ±10%\n",
    "- **RandomTranslation**: mueve ±10%\n",
    "\n",
    "Conservador porque usamos Transfer Learning, no hace falta pasarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation conservador para Transfer Learning\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "    keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "], name=\"data_augmentation\")\n",
    "\n",
    "print(\"Data Augmentation listo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplico augmentation a los datasets\n",
    "train_dataset_augmented = train_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "validation_dataset_augmented = validation_dataset.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "\n",
    "print(\"Datasets con augmentation listos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELO 1/3: VGG16\n",
    "\n",
    "Transfer Learning clásico:\n",
    "1. Cargo VGG16 pre-entrenado en ImageNet (14M imágenes)\n",
    "2. Congelo todas las capas\n",
    "3. Añado mi cabecera:\n",
    "   - Dense(256) + Dropout(0.5) + Dense(1, sigmoid)\n",
    "\n",
    "Solo entreno ~67K parámetros en vez de 15M. Mucho más rápido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cargo VGG16 pre-entrenado\n",
    "vgg16_base = VGG16(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Congelo todo\n",
    "for layer in vgg16_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construyo modelo\n",
    "vgg16_model = Sequential([\n",
    "    vgg16_base,\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "vgg16_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODELO 1/3: VGG16\")\n",
    "print(\"=\"*70)\n",
    "vgg16_model.summary()\n",
    "print(\"=\"*70)\n",
    "print(f\"Capas entrenables: {sum([layer.trainable for layer in vgg16_base.layers])}/{len(vgg16_base.layers)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning - VGG16\n",
    "\n",
    "Entreno con VGG16 congelado:\n",
    "- Adam optimizer con LR bajo (0.0001)\n",
    "- 10 épocas (optimizado pa velocidad)\n",
    "- ReduceLROnPlateau: baja LR si se estanca\n",
    "\n",
    "Solo entreno las capas Dense q añadí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback para medir tiempo\n",
    "import time\n",
    "\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = []\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        epoch_time = time.time() - self.epoch_time_start\n",
    "        self.times.append(epoch_time)\n",
    "        print(f\"\\nÉpoca {epoch+1} completada en {epoch_time:.1f}s ({epoch_time/60:.2f} min)\")\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "# Entreno VGG16 con Transfer Learning\n",
    "vgg16_history_tl = vgg16_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=10,  # Reducido para que velocidad\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        time_callback,\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Transfer Learning VGG16 completado\")\n",
    "print(f\"Tiempo promedio por época: {sum(time_callback.times)/len(time_callback.times):.1f}s\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo resultados Transfer Learning VGG16\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(vgg16_history_tl.history['loss'], label='Train')\n",
    "plt.plot(vgg16_history_tl.history['val_loss'], label='Validation')\n",
    "plt.title('Loss - VGG16 TL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(vgg16_history_tl.history['accuracy'], label='Train')\n",
    "plt.plot(vgg16_history_tl.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy - VGG16 TL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúo VGG16 en datos suplementarios\n",
    "vgg16_supp_results = vgg16_model.evaluate(supplementary_dataset, verbose=0)\n",
    "vgg16_supp_accuracy = vgg16_supp_results[1]  # accuracy es el segundo elemento\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"VGG16 Accuracy en datos suplementarios: {vgg16_supp_accuracy:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelo solo el último bloque (block5) de VGG16\n",
    "vgg16_base.trainable = True\n",
    "\n",
    "# Congelo todo excepto las últimas 4 capas (block5)\n",
    "for layer in vgg16_base.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Verifico\n",
    "print(\"Capas VGG16 después de descongelar block5:\")\n",
    "trainable_count = sum([1 for layer in vgg16_base.layers if layer.trainable])\n",
    "frozen_count = sum([1 for layer in vgg16_base.layers if not layer.trainable])\n",
    "\n",
    "print(f\"Congeladas: {frozen_count}\")\n",
    "print(f\"Entrenables: {trainable_count}\")\n",
    "print(f\"Total params: {vgg16_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning VGG16\n",
    "\n",
    "Ahora descongelo las últimas capas de VGG16 (block5) pa ajustar fino:\n",
    "- LR MUY bajo (1e-5) pa no romper lo pre-entrenado\n",
    "- 10 épocas más\n",
    "- Objetivo: mejorar accuracy\n",
    "\n",
    "Riesgo: puede empeorar si sobreajusto, por eso LR bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompilo con LR MUY bajo para fine-tuning\n",
    "vgg16_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),  # 10x más bajo\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Entreno con fine-tuning\n",
    "vgg16_history_ft = vgg16_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=10,  # 10 épocas para todos los modelos\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,  # Menos patience\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Fine-tuning VGG16 completado\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo resultados Fine-tuning VGG16\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(vgg16_history_ft.history['loss'], label='Train')\n",
    "plt.plot(vgg16_history_ft.history['val_loss'], label='Validation')\n",
    "plt.title('Loss - VGG16 FT')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(vgg16_history_ft.history['accuracy'], label='Train')\n",
    "plt.plot(vgg16_history_ft.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy - VGG16 FT')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# MODELO 2/3: EfficientNetB3\n",
    "\n",
    "EfficientNetB3 es un modelo más moderno y eficiente que VGG16, diseñado específicamente para maximizar accuracy con menos parámetros.\n",
    "\n",
    "**Ventajas**:\n",
    "- Arquitectura balanceada (depth, width, resolution)\n",
    "- Mejor relación accuracy/parámetros\n",
    "- Entrenado en ImageNet (mismo dataset que VGG16)\n",
    "\n",
    "Vamos a seguir el mismo proceso: **Transfer Learning** → **Fine-tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo EfficientNetB3 pre-entrenado\n",
    "efficientnet_base = EfficientNetB3(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Congelo todo\n",
    "for layer in efficientnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construyo modelo\n",
    "efficientnet_model = Sequential([\n",
    "    efficientnet_base,\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "efficientnet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODELO 2/3: EfficientNetB3\")\n",
    "print(\"=\"*70)\n",
    "efficientnet_model.summary()\n",
    "print(\"=\"*70)\n",
    "print(f\"Capas entrenables: {sum([layer.trainable for layer in efficientnet_base.layers])}/{len(efficientnet_base.layers)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning - EfficientNetB3\n",
    "\n",
    "Entreno con EfficientNetB3 congelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno EfficientNetB3 con Transfer Learning\n",
    "efficientnet_history_tl = efficientnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=10,  # Reducido para que velocidad\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Transfer Learning EfficientNetB3 completado\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo resultados Transfer Learning EfficientNetB3\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(efficientnet_history_tl.history['loss'], label='Train')\n",
    "plt.plot(efficientnet_history_tl.history['val_loss'], label='Validation')\n",
    "plt.title('Loss - EfficientNetB3 TL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(efficientnet_history_tl.history['accuracy'], label='Train')\n",
    "plt.plot(efficientnet_history_tl.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy - EfficientNetB3 TL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning - EfficientNetB3\n",
    "\n",
    "Descongelo últimas 20 capas pa ajustar fino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelo últimas 20 capas\n",
    "for layer in efficientnet_base.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompilo con LR bajo\n",
    "efficientnet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Capas entrenables: {sum([layer.trainable for layer in efficientnet_base.layers])}/{len(efficientnet_base.layers)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno con fine-tuning EfficientNetB3\n",
    "efficientnet_history_ft = efficientnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=10,  # 10 épocas para todos los modelos\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,  # Menos patience\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Fine-tuning EfficientNetB3 completado\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo resultados Fine-tuning EfficientNetB3\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(efficientnet_history_ft.history['loss'], label='Train')\n",
    "plt.plot(efficientnet_history_ft.history['val_loss'], label='Validation')\n",
    "plt.title('Loss - EfficientNetB3 Fine-tuning')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(efficientnet_history_ft.history['accuracy'], label='Train')\n",
    "plt.plot(efficientnet_history_ft.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy - EfficientNetB3 Fine-tuning')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúo EfficientNetB3 en datos suplementarios\n",
    "print(\"Evaluando EfficientNetB3 en datos suplementarios...\")\n",
    "efficientnet_supp_results = efficientnet_model.evaluate(supplementary_dataset)\n",
    "efficientnet_supp_accuracy = efficientnet_supp_results[1]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"EfficientNetB3 Supplementary Accuracy: {efficientnet_supp_accuracy:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# MODELO 3/3: ResNet50\n",
    "\n",
    "ResNet50 introduce skip connections (conexiones residuales) que permiten entrenar redes muy profundas sin degradación.\n",
    "\n",
    "**Ventajas**:\n",
    "- Arquitectura profunda (50 capas)\n",
    "- Skip connections resuelven el problema del gradient vanishing\n",
    "- Gran capacidad de aprendizaje de features complejas\n",
    "\n",
    "Mismo proceso: **Transfer Learning** → **Fine-tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo ResNet50 pre-entrenado\n",
    "resnet_base = ResNet50(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "\n",
    "# Congelo todo\n",
    "for layer in resnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Construyo modelo\n",
    "resnet_model = Sequential([\n",
    "    resnet_base,\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "resnet_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODELO 3/3: ResNet50\")\n",
    "print(\"=\"*70)\n",
    "resnet_model.summary()\n",
    "print(\"=\"*70)\n",
    "print(f\"Capas entrenables: {sum([layer.trainable for layer in resnet_base.layers])}/{len(resnet_base.layers)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning - ResNet50\n",
    "\n",
    "Entreno con ResNet50 congelado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno ResNet50 con Transfer Learning\n",
    "resnet_history_tl = resnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=10,  # Reducido para que velocidad\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Transfer Learning ResNet50 completado\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo resultados Transfer Learning ResNet50\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(resnet_history_tl.history['loss'], label='Train')\n",
    "plt.plot(resnet_history_tl.history['val_loss'], label='Validation')\n",
    "plt.title('Loss - ResNet50 TL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(resnet_history_tl.history['accuracy'], label='Train')\n",
    "plt.plot(resnet_history_tl.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy - ResNet50 TL')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning - ResNet50\n",
    "\n",
    "Descongelo últimas 15 capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descongelo últimas 15 capas\n",
    "for layer in resnet_base.layers[-15:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompilo con LR bajo\n",
    "resnet_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Capas entrenables: {sum([layer.trainable for layer in resnet_base.layers])}/{len(resnet_base.layers)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno con fine-tuning ResNet50\n",
    "resnet_history_ft = resnet_model.fit(\n",
    "    train_dataset_augmented,\n",
    "    epochs=10,  # 10 épocas para todos los modelos\n",
    "    validation_data=validation_dataset_augmented,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,  # Menos patience\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-7\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Fine-tuning ResNet50 completado\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizo resultados Fine-tuning ResNet50\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(resnet_history_ft.history['loss'], label='Train')\n",
    "plt.plot(resnet_history_ft.history['val_loss'], label='Validation')\n",
    "plt.title('Loss - ResNet50 Fine-tuning')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(resnet_history_ft.history['accuracy'], label='Train')\n",
    "plt.plot(resnet_history_ft.history['val_accuracy'], label='Validation')\n",
    "plt.title('Accuracy - ResNet50 Fine-tuning')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalúo ResNet50 en datos suplementarios\n",
    "print(\"Evaluando ResNet50 en datos suplementarios...\")\n",
    "resnet_supp_results = resnet_model.evaluate(supplementary_dataset)\n",
    "resnet_supp_accuracy = resnet_supp_results[1]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"ResNet50 Supplementary Accuracy: {resnet_supp_accuracy:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comparación Modelos\n",
    "\n",
    "Veo cómo van los 3 en supplementary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparo los 3\n",
    "models_comparison = {\n",
    "    'VGG16': vgg16_supp_accuracy,\n",
    "    'EfficientNetB3': efficientnet_supp_accuracy,\n",
    "    'ResNet50': resnet_supp_accuracy\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(models_comparison.keys(), models_comparison.values(), color=['blue', 'green', 'red'])\n",
    "plt.title('Comparación Supplementary - 3 Modelos')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.6, 1.0)\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "for i, (model, acc) in enumerate(models_comparison.items()):\n",
    "    plt.text(i, acc + 0.01, f'{acc:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARACIÓN\")\n",
    "print(\"=\"*70)\n",
    "for model, acc in models_comparison.items():\n",
    "    print(f\"{model:20s}: {acc:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Test Time Augmentation (TTA)\n",
    "\n",
    "Predigo cada imagen varias veces con augmentations random y promedio.\n",
    "\n",
    "**Por qué mola**:\n",
    "- Reduce overfitting\n",
    "- Más robusto\n",
    "- +1-2% gratis sin reentrenar\n",
    "\n",
    "**Cómo va**:\n",
    "1. Cojo 1 imagen\n",
    "2. Le hago N augmentations random\n",
    "3. Predigo cada una → N predicciones\n",
    "4. Promedio → predicción final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función TTA\n",
    "def predict_with_tta(model, img_array, n_augmentations=10):\n",
    "    \"\"\"Predice con TTA - hace N augmentations y promedia\"\"\"\n",
    "    # Layer de augmentation\n",
    "    augmentation_layer = keras.Sequential([\n",
    "        keras.layers.RandomFlip(\"horizontal\"),\n",
    "        keras.layers.RandomRotation(0.1),\n",
    "        keras.layers.RandomZoom(0.1),\n",
    "        keras.layers.RandomTranslation(0.1, 0.1),\n",
    "    ])\n",
    "    \n",
    "    predictions = []\n",
    "    for _ in range(n_augmentations):\n",
    "        # Augmentation random\n",
    "        augmented_img = augmentation_layer(img_array, training=True)\n",
    "        # Predigo\n",
    "        pred = model.predict(augmented_img, verbose=0)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Promedio\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "print(\"Función TTA lista\")\n",
    "print(f\"Se usarán {N_TTA_AUGMENTATIONS} augmentations por imagen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Ensemble + TTA - Predicciones\n",
    "\n",
    "Combino **Ensemble** (3 modelos) + **TTA** (10 augmentations).\n",
    "\n",
    "**Total por imagen**: 3 × 10 = **30 predicciones**\n",
    "\n",
    "Promedio todo pa la predicción final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero predicciones con Ensemble + TTA\n",
    "print(\"=\"*70)\n",
    "print(\"GENERANDO PREDICCIONES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Modelos: {ENSEMBLE_MODELS}\")\n",
    "print(f\"TTA por modelo: {N_TTA_AUGMENTATIONS}\")\n",
    "print(f\"Total por imagen: {len(ENSEMBLE_MODELS) * N_TTA_AUGMENTATIONS}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ensemble_predictions = []\n",
    "\n",
    "for batch in test_dataset:\n",
    "    batch_predictions = []\n",
    "    \n",
    "    # Predigo con cada modelo + TTA\n",
    "    if 'vgg16' in ENSEMBLE_MODELS:\n",
    "        vgg16_pred = predict_with_tta(vgg16_model, batch, N_TTA_AUGMENTATIONS)\n",
    "        batch_predictions.append(vgg16_pred)\n",
    "    \n",
    "    if 'efficientnet' in ENSEMBLE_MODELS:\n",
    "        efficientnet_pred = predict_with_tta(efficientnet_model, batch, N_TTA_AUGMENTATIONS)\n",
    "        batch_predictions.append(efficientnet_pred)\n",
    "    \n",
    "    if 'resnet' in ENSEMBLE_MODELS:\n",
    "        resnet_pred = predict_with_tta(resnet_model, batch, N_TTA_AUGMENTATIONS)\n",
    "        batch_predictions.append(resnet_pred)\n",
    "    \n",
    "    # Promedio todo\n",
    "    ensemble_pred = np.mean(batch_predictions, axis=0)\n",
    "    ensemble_predictions.extend(ensemble_pred)\n",
    "\n",
    "ensemble_predictions = np.array(ensemble_predictions)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Predicciones listas: {len(ensemble_predictions)}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genero Submission\n",
    "\n",
    "Creo el csv con las predicciones del ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero submission.csv\n",
    "test_filenames = test_dataset.file_paths\n",
    "ids = [int(os.path.splitext(os.path.basename(f))[0]) for f in test_filenames]\n",
    "\n",
    "# Paso probabilidades a clases con threshold 0.5\n",
    "predictions_binary = (ensemble_predictions > 0.5).astype(int).flatten()\n",
    "\n",
    "# Creo DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'label': predictions_binary\n",
    "})\n",
    "\n",
    "# Ordeno por id\n",
    "submission_df = submission_df.sort_values('id')\n",
    "\n",
    "# Guardo\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"SUBMISSION.CSV GENERADO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total predicciones: {len(submission_df)}\")\n",
    "print(f\"Cats (0): {(predictions_binary == 0).sum()}\")\n",
    "print(f\"Dogs (1): {(predictions_binary == 1).sum()}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPrimeras predicciones:\")\n",
    "print(submission_df.head(10))\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLISTO para que KAGGLE\")\n",
    "print(f\"Técnicas usadas:\")\n",
    "print(f\"  - Ensemble: {ENSEMBLE_MODELS}\")\n",
    "print(f\"  - TTA: {N_TTA_AUGMENTATIONS} augmentations\")\n",
    "print(f\"  - Resolución: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  - Total predicciones por imagen: {len(ENSEMBLE_MODELS) * N_TTA_AUGMENTATIONS}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9809442,
     "sourceId": 86515,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
