# GUI√ìN DE AUDIO - PRESENTACI√ìN AEC4
## Configuraci√≥n Avanzada 2: NFS (10 PUNTOS)

---

## üé¨ PARTE 1: INTRODUCCI√ìN (1-2 min)

Hola, soy Ismael y voy a presentar la AEC4 de Sistemas Distribuidos.

He implementado un sistema de gesti√≥n de archivos distribuido usando Docker, Kubernetes y AWS EC2. La aplicaci√≥n tiene tres componentes: un BROKER en el puerto 32002 que coordina las conexiones, varios SERVIDORES en el puerto 32001 que gestionan los archivos, y CLIENTES que interact√∫an con el sistema.

He implementado la Configuraci√≥n Avanzada 2, la m√°s compleja, que vale 10 puntos. Esta configuraci√≥n incluye un cluster con 1 master y 3 workers, 3 r√©plicas del servidor distribuidas en diferentes nodos f√≠sicos, y almacenamiento compartido NFS con PersistentVolume y PersistentVolumeClaim. Esto proporciona alta disponibilidad, tolerancia a fallos y balanceo de carga autom√°tico.

---

## üñ•Ô∏è PARTE 2: MOSTRAR NODOS Y SERVICIOS (2-3 min)

### Al ejecutar: `kubectl get nodes -o wide`

Aqu√≠ pod√©is ver mi cluster completo. Tengo 4 nodos: 1 master que ejecuta el control-plane de Kubernetes y 3 workers donde corren las aplicaciones. Todos est√°n en estado Ready, operativos y listos para ejecutar pods.

Las 3 r√©plicas del servidor est√°n repartidas entre estos 3 workers. Esto me da alta disponibilidad real: si un nodo falla, los otros dos siguen funcionando.

### Al ejecutar: `kubectl get deployments`

Aqu√≠ vemos los deployments. El broker-deployment tiene 1/1 ready, y el server-deployment tiene 3/3 ready, las tres r√©plicas funcionando correctamente.

### Al ejecutar: `kubectl get pods -o wide`

En los pods con detalle, vemos que el pod del broker est√° corriendo en uno de los workers, y lo m√°s importante: las 3 r√©plicas del servidor est√°n repartidas en los 3 workers diferentes. Cada r√©plica est√° en un nodo distinto, lo cual maximiza la disponibilidad. Si cualquier nodo falla, siempre tengo al menos 2 r√©plicas funcionando.

### Al ejecutar: `kubectl get services`

Los servicios expuestos son el broker-service en el puerto 32002 y el server-service en el puerto 32001. El server-service autom√°ticamente balancea las peticiones entre las 3 r√©plicas.

### Al ejecutar: `kubectl get pv,pvc`

El PersistentVolume nfs-pv est√° en estado Bound, vinculado al PVC. Tiene 5Gi disponibles y el modo ReadWriteMany activo, lo que permite que m√∫ltiples pods lean y escriban simult√°neamente. El PersistentVolumeClaim nfs-pvc tambi√©n est√° Bound y est√° montado por los 3 pods del servidor. Todos acceden al mismo almacenamiento compartido.

---

## üì¶ PARTE 3: EXPLICAR DOCKERFILES Y YAMLS (3-4 min)

### Al mostrar: `cat docker/broker/Dockerfile`

El Dockerfile del broker es sencillo. He usado Ubuntu 20.04 como imagen base por compatibilidad con los binarios. Instalo libstdc++6 como dependencia necesaria, copio el binario del broker, expongo el puerto 32002 y ejecuto el broker. Es una imagen ligera con solo lo necesario.

### Al mostrar: `cat docker/server/Dockerfile`

El Dockerfile del servidor es similar, pero aqu√≠ creo el directorio /app/FileManagerDir para almacenar archivos. El puerto es el 32001. Uso ENTRYPOINT porque el servidor necesita recibir la IP del broker como par√°metro al arrancar.

Estas im√°genes est√°n publicadas en Docker Hub para que Kubernetes pueda descargarlas desde cualquier nodo del cluster.

### Al mostrar: `cat kubernetes/broker/deployment.yaml`

El deployment del broker es simple. Solo necesito 1 r√©plica. Uso mi imagen de Docker Hub, expongo el puerto 32002, y le pongo l√≠mites de recursos para el scheduler de Kubernetes. El service es de tipo NodePort, accesible desde fuera del cluster.

### Al mostrar: `cat kubernetes/nfs/nfs-pv-pvc.yaml`

Aqu√≠ est√° toda la configuraci√≥n del almacenamiento distribuido. El PersistentVolume nfs-pv conecta con el servidor NFS usando su IP. Tiene 5Gi de capacidad y ReadWriteMany, que permite que m√∫ltiples pods lean y escriban simult√°neamente. Esto es lo que permite tener las r√©plicas compartiendo datos.

El PersistentVolumeClaim nfs-pvc es la solicitud de acceso a ese volumen. Los pods montan el PVC, lo cual abstrae los detalles de implementaci√≥n.

### Al mostrar: `cat kubernetes/server/deployment-advanced2.yaml`

Este es el deployment avanzado 2. Tengo 3 r√©plicas del servidor, y Kubernetes las distribuye autom√°ticamente en diferentes nodos. Cada r√©plica monta el PVC nfs-pvc en /app/FileManagerDir, entonces las 3 r√©plicas acceden al mismo almacenamiento NFS. Los archivos son accesibles desde cualquier pod en cualquier nodo.

Las ventajas son enormes: alta disponibilidad porque si un nodo falla los otros contin√∫an, persistencia real porque los datos est√°n en red y sobreviven a reinicios, y escalabilidad porque puedo a√±adir m√°s r√©plicas f√°cilmente.

---

## üß™ PARTE 4: DEMOSTRACI√ìN PR√ÅCTICA (5-7 min)

### Al conectar el primer cliente

Voy a ejecutar el primer cliente. El cliente se ha conectado correctamente.

### Al ejecutar: `ls`

Aqu√≠ tengo los archivos de prueba locales: test1.txt, test2.txt y demo.txt.

### Al ejecutar: `lls`

El directorio remoto est√° vac√≠o porque es la primera ejecuci√≥n. [O: Ya hay algunos archivos de ejecuciones anteriores]

### Al ejecutar: `upload test1.txt`

Archivo subido correctamente. Voy a verificar.

### Al ejecutar: `lls`

Perfecto, ahora aparece test1.txt en el listado remoto. El archivo se ha transferido correctamente al servidor.

### Al ejecutar: `upload test2.txt` y `upload demo.txt`

Voy a subir un par de archivos m√°s.

### Al ejecutar: `lls`

Ahora tengo 3 archivos en el servidor. Lo importante es que estos archivos est√°n en el almacenamiento NFS compartido. Los 3 pods del servidor, en 3 nodos diferentes, pueden acceder a estos mismos archivos. Si un cliente se conecta a cualquier r√©plica, ver√° exactamente estos archivos.

### Al verificar en el servidor NFS

Voy a conectarme al servidor NFS para verificar que los archivos est√°n ah√≠ f√≠sicamente. Aqu√≠ est√°n los archivos, almacenados en /mnt/nfs-share/FileManagerDir del servidor NFS. Desde aqu√≠ son accesibles por todos los pods en todos los nodos del cluster. Si un nodo falla, los datos siguen disponibles. Esto es persistencia real en un sistema distribuido.

### Al ejecutar: `exit` y conectar segundo cliente

Cierro este cliente y ahora inicio un segundo cliente para demostrar la persistencia de datos. Conectado. Este segundo cliente puede haberse conectado a una r√©plica diferente gracias al balanceo de carga de Kubernetes.

### Al ejecutar: `lls` en el segundo cliente

Perfecto! Este segundo cliente ve todos los archivos que subi√≥ el primer cliente: test1.txt, test2.txt y demo.txt.

Esto demuestra varias cosas: el almacenamiento NFS funciona correctamente, los datos est√°n disponibles desde cualquier pod en cualquier nodo, hay persistencia real entre diferentes conexiones, el balanceo de carga distribuye clientes entre r√©plicas, y todas las r√©plicas acceden al mismo almacenamiento compartido. Es un sistema distribuido real con alta disponibilidad.

### Al ejecutar: `download test1.txt`

Voy a descargar uno de los archivos. Archivo descargado correctamente.

### Al ejecutar: `exit` y verificar el archivo descargado

Cierro el cliente y verifico que el archivo se descarg√≥ bien. Aqu√≠ est√° el archivo con su contenido correcto. Todo el ciclo funciona perfectamente.

---

## ‚ö° PARTE 5: DEMOSTRACI√ìN EXTRA (Opcional, 2-3 min)

### Al ver logs de r√©plicas

Voy a ver los logs de las 3 r√©plicas para demostrar que las peticiones se distribuyen. Como pod√©is ver, las peticiones se han distribuido entre los diferentes pods. Cada r√©plica ha procesado conexiones de clientes. El balanceo de carga de Kubernetes funciona perfectamente.

### Al eliminar un pod

Ahora voy a eliminar un pod para simular que un nodo falla. Kubernetes detecta inmediatamente que falta un pod y empieza a crear uno nuevo autom√°ticamente para mantener las 3 r√©plicas especificadas.

Mientras esto pasa, las otras 2 r√©plicas siguen funcionando con normalidad. Los clientes pueden seguir conect√°ndose sin problema. El servicio nunca se interrumpe.

Cuando el nuevo pod termina de arrancar, se monta autom√°ticamente al volumen NFS, tiene acceso inmediato a todos los archivos existentes, y empieza a recibir peticiones del balanceador. Esto es alta disponibilidad real, tolerancia a fallos de hardware en un sistema distribuido de producci√≥n.

---

## üéØ PARTE 6: CONCLUSI√ìN (1 min)

Para terminar, he completado la Configuraci√≥n Avanzada 2 con NFS, la m√°s compleja de la pr√°ctica, que vale 10 puntos.

He creado Dockerfiles optimizados para broker y servidor, publicados en Docker Hub. He montado un cluster de Kubernetes en AWS EC2 con 4 nodos: 1 master y 3 workers, con red overlay gestionada por Flannel.

En orquestaci√≥n tengo deployments con r√©plicas distribuidas, services con NodePort y balanceo de carga, y las 3 r√©plicas repartidas en nodos f√≠sicos diferentes.

El almacenamiento distribuido incluye un servidor NFS independiente, un PersistentVolume de 5Gi con ReadWriteMany, y un PersistentVolumeClaim montado por las 3 r√©plicas. Los datos est√°n compartidos entre todos los nodos con persistencia real en red.

Esto proporciona: alta disponibilidad con tolerancia a fallos de nodos, balanceo de carga con peticiones distribuidas, persistencia distribuida en almacenamiento compartido en red, escalabilidad f√°cil, y b√°sicamente un sistema distribuido real con m√∫ltiples puntos de fallo cubiertos.

La demostraci√≥n ha mostrado que puedo subir y descargar archivos, hay persistencia entre conexiones, los datos son accesibles desde cualquier r√©plica, hay tolerancia a fallos con recreaci√≥n autom√°tica de pods, y el balanceo de carga distribuye correctamente.

Todo funciona como un sistema distribuido de producci√≥n real.

Gracias por la atenci√≥n.

---

## üìù NOTAS PARA LA GRABACI√ìN

### Tono y Ritmo
- Habla **despacio y claro** - hay mucho contenido t√©cnico
- Enfatiza las palabras clave: "alta disponibilidad", "tolerancia a fallos", "NFS", "ReadWriteMany"
- Haz pausas cortas entre secciones para que sea m√°s digerible
- No corras, tienes 15-20 minutos

### Puntos a Enfatizar
- ‚ú® "Configuraci√≥n Avanzada 2 - 10 puntos"
- ‚ú® "3 r√©plicas en 3 nodos F√çSICOS diferentes"
- ‚ú® "Almacenamiento compartido en RED, no local"
- ‚ú® "Sistema distribuido de PRODUCCI√ìN real"
- ‚ú® "Alta disponibilidad y tolerancia a fallos"

### Durante la Grabaci√≥n
- Si te equivocas, **pausa, respira, y contin√∫a** desde el inicio de esa frase
- Si un comando tarda, **llena el silencio** explicando qu√© esperas que pase
- **Muestra confianza** - conoces el sistema que has montado
- Si algo no funciona como esperabas, **explica el problema** y c√≥mo lo resolver√≠as

---

## ‚è±Ô∏è TIMING APROXIMADO

```
00:00 - 02:00  Introducci√≥n
02:00 - 05:00  Mostrar nodos y servicios
05:00 - 09:00  Explicar Dockerfiles y YAMLs
09:00 - 16:00  Demostraci√≥n pr√°ctica completa
16:00 - 18:00  Demostraci√≥n extra (opcional)
18:00 - 20:00  Conclusi√≥n
```

Total: 18-20 minutos

---

**¬°Buena suerte con la grabaci√≥n!**
